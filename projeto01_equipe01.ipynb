{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "projeto01_equipe01.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7I8N6Aj9oknZ",
        "51lTgeW1oYR0",
        "Cm4A3iMJoji_",
        "D0ZdGEizokw2",
        "VcbinyrLp0W5",
        "sRbt_9lnourv",
        "lCN3m1b7ougd",
        "UEfgrogNxi4Z"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardodut/Mineracao_dados_textos_web/blob/master/projeto01_equipe01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDkhltJeg5ag",
        "colab_type": "text"
      },
      "source": [
        "<b> EQUIPE: </b>\n",
        "  - Eduardo Façanha\n",
        "  - Giovanni Brígido\n",
        "  - Maurício Brito\n",
        "\n",
        "<b> ATIVIDADE 01 </b> - Pré-processamento dos textos (Prazo: 11/05/2020 - 30%)\n",
        "\n",
        "- Tokenização\n",
        "- Lematização\n",
        "- POS Tagging\n",
        "- Normalização (hashtags, menções, emojis e símbolos especiais)\n",
        "- Chunking\n",
        "- NER (entidades nomeadas)\n",
        "- Remoção stop-words\n",
        "\n",
        "<b> ATIVIDADE 02 </b> - Representação Semântica (Prazo: 30/06/2020 - 30%)\n",
        "\n",
        "- Uso de bases de conhecimento externas\n",
        "- Identificação de tópicos\n",
        "- Representação vetorial das palavras e textos\n",
        "\n",
        "<b> ATIVIDADE 03 </b> - Analise da Linguagem Ofensiva - Subtarefas A e B (Prazo: 30/07/2020 - 40%)\n",
        "\n",
        "- Resultado da subtarefa A para um conjunto de teste a ser fornecido\n",
        "- Resultado da subtarefa B para um conjunto de teste a ser fornecido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kcaie5jI2md",
        "colab_type": "text"
      },
      "source": [
        "## Atividade 03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIEThqy6XFPG",
        "colab_type": "text"
      },
      "source": [
        "### Bibliotecas utilizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZr2fyJboEcp",
        "colab_type": "text"
      },
      "source": [
        "Tecnologias utilizadas\n",
        "figura do scikit learn\n",
        "spacy\n",
        "nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZazwvFs-JE-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83ebb58e-47b5-40ec-e78a-154ce8bd0649"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "!pip install Transformers\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg\n",
        "import en_core_web_md\n",
        "import en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "!python -m nltk.downloader wordnet\n",
        "!python -m nltk.downloader omw\n",
        "!pip install spacy-wordnet\n",
        "!pip install empath\n",
        "from empath import Empath "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from Transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 15.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 20.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from Transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from Transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from Transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->Transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=851b556e758b87ec3b3d0f81000cfd9f4006d91368cee57efef223ef9aa67660\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, Transformers\n",
            "Successfully installed Transformers-3.0.2 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=e4062316e09f053e0536d74929ef6b59368f0f75384fb284fdd0378364ece37b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-upkfoksv/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=bbda54c2ba3278164032c281d3c770a52116cf612ca406edd7664d4937e24a44\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9zjle9gd/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw.zip.\n",
            "Collecting spacy-wordnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f2/4d8070df0f7a7a9eeed74eb7e9ce3cf41349eb5e06b1e088de9eeca630e2/spacy-wordnet-0.0.4.tar.gz (648kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 3.4MB/s \n",
            "\u001b[?25hCollecting nltk<3.4,>=3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.15.0)\n",
            "Building wheels for collected packages: spacy-wordnet, nltk\n",
            "  Building wheel for spacy-wordnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-wordnet: filename=spacy_wordnet-0.0.4-py2.py3-none-any.whl size=650292 sha256=805c934662c6fc3f4cd6d8fd743c45174204c3af436b89c40a84809e99ccecea\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/93/1d/c86db913cd146fc9ddb26d10f56579c5d58a3e00bc8f96a3a6\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394470 sha256=e933657ab8cd7f53ba2ba410d84bf953a6457d6e43c6bcf41fb659879f82e4c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built spacy-wordnet nltk\n",
            "Installing collected packages: nltk, spacy-wordnet\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3 spacy-wordnet-0.0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting empath\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/84/a5de61a99252f60d705d7982b3648db517a704c89fa7629d3d3637a6e604/empath-0.89.tar.gz (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from empath) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (2.10)\n",
            "Building wheels for collected packages: empath\n",
            "  Building wheel for empath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empath: filename=empath-0.89-cp36-none-any.whl size=57824 sha256=7dd93637c98ce15a78aba87f05413165db96e2afe70a81e3a267dc537edf4c5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ea/2f/2bc54d4f9985ce61753ebc5b00cb2df51d855589267c667308\n",
            "Successfully built empath\n",
            "Installing collected packages: empath\n",
            "Successfully installed empath-0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6aUco5gIM9bU"
      },
      "source": [
        "#### Carregamento do arquivo de dados e transformação em DataFrame\n",
        "\n",
        "É realizado o download do arquivo e instanciado um DataFrame com os dados. A variável do DataFrame é chamada 'tweets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyxNSs8QOzMR",
        "colab_type": "text"
      },
      "source": [
        "#### Dataset_treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nqucHlrtM9bX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "bd98daca-8fd3-4ca7-cc59-293e87283728"
      },
      "source": [
        "#download o arquivo localizado no reposítório do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/olid-training-v1.0.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "tweets = pd.read_csv('/content/olid-training-v1.0.tsv', sep='\\t',encoding= 'utf-8', index_col = 'id')\n",
        "\n",
        "#verificação e remoção de duplicatas\n",
        "\n",
        "if tweets.duplicated(['tweet']).sum()>0:\n",
        "  tweets.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "print('TWEETS DUPLICADOS: ',tweets.duplicated(['tweet']).sum())\n",
        "\n",
        "\n",
        "#visualização dos primeiros registros\n",
        "tweets.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1915k  100 1915k    0     0  7122k      0 --:--:-- --:--:-- --:--:-- 7122k\n",
            "TWEETS DUPLICADOS:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>86426</th>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90194</th>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16820</th>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62688</th>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43605</th>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet  ... subtask_c\n",
              "id                                                        ...          \n",
              "86426  @USER She should ask a few native Americans wh...  ...       NaN\n",
              "90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       IND\n",
              "16820  Amazon is investigating Chinese employees who ...  ...       NaN\n",
              "62688  @USER Someone should'veTaken\" this piece of sh...  ...       NaN\n",
              "43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnPFaVz5M9bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37ee4de5-29ee-4bcd-96fd-b610cec6f585"
      },
      "source": [
        "#verificação e remoção de duplicatas\n",
        "\n",
        "if tweets.duplicated(['tweet']).sum()>0:\n",
        "  tweets.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "print('TWEETS DUPLICADOS: ',tweets.duplicated(['tweet']).sum())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TWEETS DUPLICADOS:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3N2sunzJO8_I"
      },
      "source": [
        "#### testset_a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TEBtdccDO8_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "0ea9800e-44ba-4363-afab-890abc288618"
      },
      "source": [
        "#download o arquivo localizado no reposítório do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/testset-levela.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "testset_a = pd.read_csv('/content/testset-levela.tsv', sep='\\t',encoding= 'utf-8', index_col = 'id')\n",
        "\n",
        "#conversão da coluna 'id' de inteiro para string\n",
        "# testset_a['id'] = testset_a['id'].astype('str')\n",
        "#verificação e remoção de duplicatas\n",
        "# print(testset_a.duplicated(['tweet']).sum())\n",
        "# if testset_a.duplicated(['tweet']).sum()>0:\n",
        "#   testset_a.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "# print('TWEETS DUPLICADOS: ',testset_a.duplicated(['tweet']).sum())\n",
        "# testset_a = testset_a[['subtask_c','subtask_b','subtask_a','id','tweet']]\n",
        "testset_a.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  130k  100  130k    0     0   509k      0 --:--:-- --:--:-- --:--:--  509k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15923</th>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27014</th>\n",
              "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30530</th>\n",
              "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13876</th>\n",
              "      <td>#Watching #Boomer getting the news that she is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60133</th>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet\n",
              "id                                                      \n",
              "15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
              "27014  #ConstitutionDay is revered by Conservatives, ...\n",
              "30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...\n",
              "13876  #Watching #Boomer getting the news that she is...\n",
              "60133  #NoPasaran: Unity demo to oppose the far-right..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ab-NJFKRO91-"
      },
      "source": [
        "#### testset_b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KYagkzX3O91_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "ca5e24d5-1326-4a31-f223-35203a24b748"
      },
      "source": [
        "#download o arquivo localizado no reposítório do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/testset-levelb.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "testset_b = pd.read_csv('/content/testset-levelb.tsv', sep='\\t',encoding= 'utf-8', index_col = 'id')\n",
        "# print(testset_b.duplicated(['tweet']).sum())\n",
        "# if testset_b.duplicated(['tweet']).sum()>0:\n",
        "#   testset_b.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "# print('TWEETS DUPLICADOS: ',testset_b.duplicated(['tweet']).sum())\n",
        "testset_b.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 35447  100 35447    0     0   183k      0 --:--:-- --:--:-- --:--:--  183k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15923</th>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60133</th>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83681</th>\n",
              "      <td>. . . What the fuck did he do this time?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65507</th>\n",
              "      <td>@USER Do you get the feeling he is kissing @US...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12588</th>\n",
              "      <td>@USER Nigga ware da hits at</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet\n",
              "id                                                      \n",
              "15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
              "60133  #NoPasaran: Unity demo to oppose the far-right...\n",
              "83681           . . . What the fuck did he do this time?\n",
              "65507  @USER Do you get the feeling he is kissing @US...\n",
              "12588                        @USER Nigga ware da hits at"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOCN6gf8r6-I",
        "colab_type": "text"
      },
      "source": [
        "### Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKjvjKEJFo6",
        "colab_type": "text"
      },
      "source": [
        "#### Funções de preprocessamento e Limpeza\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcNhWJl3u24d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tratamento básico\n",
        "def tratamento_texto(text):\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  #remove as menções a usuários de cada text\n",
        "  #remove as palavras url\n",
        "  text = re.sub('url$', '', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^n$', 'and', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^u$', 'you', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^r$', 'are', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^sh*t$', 'shit', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'&amp;', '', text, flags=re.MULTILINE)\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  tokens = []\n",
        "  for token in doc:\n",
        "      if token.lemma_ != \"-PRON-\" :\n",
        "        if not token.is_stop:\n",
        "          if not token.is_punct :\n",
        "            tokens.append(token.lemma_)\n",
        "  \n",
        "  text =  \" \".join([token for token in tokens]).strip()  \n",
        "\n",
        "  \n",
        "  return text"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I30PdXZfoKlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0cd835b5-6bd4-47ee-c18d-0d51fa7ded6d"
      },
      "source": [
        "tratamento_texto(tweets.tweet.iloc[0])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'@user ask native americans'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHPEqgHXi5hd",
        "colab_type": "text"
      },
      "source": [
        "#### Funções para criação de features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wirMv6NLwgtc",
        "colab_type": "text"
      },
      "source": [
        "##### Features intrínsicas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-qnjF7ZizUX",
        "colab_type": "text"
      },
      "source": [
        "##### Comprimento do tweet % (contagem de caracteres/comprimento máximo de um tweet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGYzZ_eBixeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "984847c2-bef3-434e-b35c-c9fca2b8d56a"
      },
      "source": [
        "def get_tweet_length(text):\n",
        "    return len(text)/240.0\n",
        "tweets.tweet.apply(get_tweet_length).head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.295833\n",
              "90194    0.279167\n",
              "16820    0.758333\n",
              "62688    0.270833\n",
              "43605    0.300000\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7I8N6Aj9oknZ"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brr6sOHPQCAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['comprimento_tweet'] = tweets.tweet.apply(get_tweet_length)\n",
        "tweets.hist(column = 'comprimento_tweet', by= \"subtask_a\")\n",
        "tweets.hist(column = 'comprimento_tweet', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rSOG6JlKA4JB"
      },
      "source": [
        "##### Contagem de palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5eO9xKUwA4JH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "232f8800-05f6-4573-ed77-a430311657a9"
      },
      "source": [
        "def get_word_count(text):\n",
        "  return len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_word_count).head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    14\n",
              "90194    11\n",
              "16820    27\n",
              "62688    11\n",
              "43605    12\n",
              "Name: tweet, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51lTgeW1oYR0",
        "colab_type": "text"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ2sS3sIQwjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['contagem_palavras'] = tweets.tweet.apply(get_word_count)\n",
        "tweets.hist(column = 'contagem_palavras', by= \"subtask_a\")\n",
        "tweets.hist(column = 'contagem_palavras', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1NCQy-xBBLSq"
      },
      "source": [
        "##### Comprimento médio das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JoF8F4kfBLSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "be2b0305-8f3e-491a-8003-67650f2fa047"
      },
      "source": [
        "def get_avg_word_len(text):\n",
        "  words = text.split()\n",
        "  word_len = 0\n",
        "  for word in words:\n",
        "    word_len = word_len + len(word)\n",
        "  return word_len/len(words)\n",
        "\n",
        "tweets.tweet.apply(get_avg_word_len).head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    4.142857\n",
              "90194    5.181818\n",
              "16820    5.777778\n",
              "62688    5.000000\n",
              "43605    5.083333\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cm4A3iMJoji_"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VStr92wgRNtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['avg_len'] = tweets.tweet.apply(get_avg_word_len)\n",
        "tweets.hist(column = 'avg_len', by= \"subtask_a\")\n",
        "tweets.hist(column = 'avg_len', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8WBEOKo0Cfla"
      },
      "source": [
        "##### Contagem de stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K92ZzPj6Cflc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0b981b5a-7125-41e9-b622-c127307bf6e8"
      },
      "source": [
        "def get_stop_words_percent(text):\n",
        "  return len([t for t in text.split() if t in STOP_WORDS])/len(text.split())\n",
        "\n",
        "\n",
        "tweets.tweet.apply(get_stop_words_percent).head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.571429\n",
              "90194    0.000000\n",
              "16820    0.296296\n",
              "62688    0.363636\n",
              "43605    0.250000\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0ZdGEizokw2"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THQ3PvhSRTr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['sw_count'] = tweets.tweet.apply(get_stop_words_percent)\n",
        "tweets.hist(column = 'sw_count', by= \"subtask_a\")\n",
        "tweets.hist(column = 'sw_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UHDd9bMuFQz6"
      },
      "source": [
        "##### Percentual de #hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wPkRCVrMFQz8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8deb0ffe-1c6f-449c-afb3-d80efe34a216"
      },
      "source": [
        "def get_hashtag_percent(text):\n",
        "  return len([t for t in text.split() if t.startswith(\"#\")])/len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_hashtag_percent).head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.000000\n",
              "90194    0.181818\n",
              "16820    0.185185\n",
              "62688    0.000000\n",
              "43605    0.000000\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcbinyrLp0W5",
        "colab_type": "text"
      },
      "source": [
        "###### Histogramas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE88MrhNRY1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['hashtag_count'] = tweets.tweet.apply(get_hashtag_percent)\n",
        "tweets[tweets['hashtag_count'] > 0].hist(column = 'hashtag_count', by= \"subtask_a\")\n",
        "tweets[tweets['hashtag_count'] > 0].hist(column = 'hashtag_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kw7Tchg4Filo"
      },
      "source": [
        "##### Contagem de @menções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uXzj_UkrFilp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2a980bf1-a565-43dd-bc1a-1cb2e7db7da5"
      },
      "source": [
        "def get_mention_percent(text):\n",
        "  return len([t for t in text.split() if t.startswith(\"@\")])/len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_mention_percent).head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.071429\n",
              "90194    0.272727\n",
              "16820    0.000000\n",
              "62688    0.090909\n",
              "43605    0.166667\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRbt_9lnourv"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M--KengRRmH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['mention_count'] = tweets.tweet.apply(get_mention_percent)\n",
        "tweets[tweets['mention_count'] > 0].hist(column = 'mention_count', by= \"subtask_a\")\n",
        "tweets[tweets['mention_count'] > 0].hist(column = 'mention_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vN1XcdmAJEAS"
      },
      "source": [
        "##### Contagem de palavras em MAIÚSCULO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8O3HOLHLJEAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0f4bf056-4c0d-42d4-f88b-f1cc3be71b15"
      },
      "source": [
        "def get_uppercase_percent(text):\n",
        "  return len([t for t in text.split() if t.isupper()])/len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_uppercase_percent).head()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.071429\n",
              "90194    0.454545\n",
              "16820    0.185185\n",
              "62688    0.090909\n",
              "43605    0.166667\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCN3m1b7ougd"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FU5WTp0Ryi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['upper_count'] = tweets.tweet.apply(get_uppercase_percent)\n",
        "tweets.hist(column = 'upper_count', by= \"subtask_a\")\n",
        "tweets.hist(column = 'upper_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pbgQzWVYJzK8"
      },
      "source": [
        "##### Verificar se o tweet contém URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L-ZWmeH3JzK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "da9b39c3-01b5-4137-ae4f-7cf66d82ed04"
      },
      "source": [
        "def get_contain_url(text):\n",
        "\n",
        "  return int('URL' in text )\n",
        "\n",
        "tweets.tweet.apply(get_contain_url).head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0\n",
              "90194    1\n",
              "16820    1\n",
              "62688    0\n",
              "43605    0\n",
              "Name: tweet, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3erhQ6fYv3oC",
        "colab_type": "text"
      },
      "source": [
        "##### Contagem de sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i5LQJapv3H3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cec6d3c8-3e6e-4876-e9a1-e52db7a923df"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "def sent_count(text):\n",
        "  return len(sent_tokenize(text))\n",
        "tweets.tweet.apply(sent_count).head()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    1\n",
              "90194    2\n",
              "16820    2\n",
              "62688    2\n",
              "43605    1\n",
              "Name: tweet, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEfgrogNxi4Z",
        "colab_type": "text"
      },
      "source": [
        "###### Histogramas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsK8qpmWwvra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['sent_count'] = tweets.tweet.apply(sent_count)\n",
        "tweets.hist(column = 'sent_count', by= \"subtask_a\")\n",
        "tweets.hist(column = 'sent_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-yecoLvukCoc"
      },
      "source": [
        "##### Lista de tipos de entidades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64IvzaLzMvtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_entities(text):\n",
        "  doc = nlp(text)\n",
        "  lista_entities = \"\"\n",
        "  if len(doc.ents) > 0:\n",
        "    for token in doc:\n",
        "      if len(token.ent_type_) > 0:\n",
        "        lista_entities = lista_entities  + \" \" + token.ent_type_\n",
        "  \n",
        "  return lista_entities\n",
        "\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_es-Vl-9w0D7"
      },
      "source": [
        "##### Features de bases de conhecimento externas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R8-XrfBqX8ZH"
      },
      "source": [
        "###### Classificar como positivo ou negativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLMBCWaXBACA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classificar_positivo_negativo(text):\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    return doc.cats\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nvZIIzAjF_R4"
      },
      "source": [
        "###### Anexar embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2CaCbnGC-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embeddings(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  return doc.vector\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJkSowVecxR",
        "colab_type": "text"
      },
      "source": [
        "###### Empath"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a2ZmTRjsv0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from empath import Empath\n",
        "lexicon = Empath()\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjNRT4FOm6Qg",
        "colab_type": "text"
      },
      "source": [
        "###### Criação das features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VettZGZKm4fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class Criar_Features(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \n",
        "\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "\n",
        "        features = pd.DataFrame(X)\n",
        "        #Comprimento do tweet/240.0\n",
        "        features['length'] = features['tweet'].apply(get_tweet_length)\n",
        "        #Hashtag por tweet (%)\n",
        "        features['hashtag_%'] = features['tweet'].apply(get_hashtag_percent)\n",
        "        #Menções por tweet (%)\n",
        "        features['mentions_%'] = features['tweet'].apply(get_mention_percent)\n",
        "        #Stop words (%)\n",
        "        features['stop_words_%'] = features['tweet'].apply(get_stop_words_percent)\n",
        "        #upper case (%)\n",
        "        features['uppercase_%'] =  features['tweet'].apply(get_uppercase_percent)\n",
        "        #contagem de sentenças\n",
        "        features['sent_count'] = features['tweet'].apply(sent_count)\n",
        "        #contém url\n",
        "        features['has_url'] = features['tweet'].apply(get_contain_url)\n",
        "        \n",
        "        features['tweet'] = features['tweet'].apply(tratamento_texto)\n",
        "               \n",
        "\n",
        "        return features\n",
        "\n",
        "# criar_features = Criar_Features()\n",
        "# print(criar_features.fit_transform(tweets.tweet))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4y1CEwxm9cya",
        "colab": {}
      },
      "source": [
        "#@title Aumento das features de treino e teste { display-mode: \"form\" }\n",
        "\n",
        "criar_trainset = True #@param {type:\"boolean\"}\n",
        "criar_testset_a = True #@param {type:\"boolean\"}\n",
        "criar_testset_b = True #@param {type:\"boolean\"}\n",
        "\n",
        "if criar_trainset:\n",
        "  trainset_aumentado = Criar_Features().fit_transform(tweets)\n",
        "  trainset_aumentado.to_csv('/content/trainset_aumentado.txt', sep=' ')\n",
        "\n",
        "if criar_testset_a:\n",
        "  testset_a_aumentado = Criar_Features().fit_transform(testset_a)\n",
        "  testset_a_aumentado.to_csv('/content/testset_a_aumentado.txt', sep=' ')\n",
        "\n",
        "\n",
        "if criar_testset_b:\n",
        "  testset_b_aumentado = Criar_Features().fit_transform(testset_b) \n",
        "  testset_b_aumentado.to_csv('/content/testset_b_aumentado.txt', sep=' ')\n",
        "\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9EF1TAWDKRph",
        "colab": {}
      },
      "source": [
        "#@title Busca dos vetores de entidades, positivo e negativo, vetores GloVe e base Empath { display-mode: \"form\" }\n",
        "criar_trainset = True #@param {type:\"boolean\"}\n",
        "criar_testset_a = True #@param {type:\"boolean\"}\n",
        "criar_testset_b = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "vetores_trainset = pd.DataFrame()\n",
        "vetores_testset_a = pd.DataFrame()\n",
        "vetores_testset_b = pd.DataFrame()\n",
        "def buscar_vetores(dataset_aumentado):\n",
        "  \n",
        "  #Extrai entidades\n",
        "  entidades = dataset_aumentado['tweet'].apply(extract_entities)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #classifica como positivo ou negativo\n",
        "  pos_neg = dataset_aumentado['tweet'].apply(classificar_positivo_negativo).apply(pd.Series)\n",
        "  #busca vetores glove          \n",
        "  vetor_glove = dataset_aumentado['tweet'].apply(get_embeddings).apply(pd.Series)\n",
        "  \n",
        "  #empath\n",
        "  emp = dataset_aumentado['tweet'].apply(lexicon.analyze).apply(pd.Series)\n",
        "  print(emp)\n",
        "  df = pd.concat([entidades, pos_neg, vetor_glove, emp],axis = 1)\n",
        "      \n",
        "  return df\n",
        "\n",
        "for tamanho in [\"pequeno\", \"medio\", \"grande\"]:\n",
        "  \n",
        "  if tamanho == \"grande\":\n",
        "    nlp = en_core_web_lg.load()\n",
        "\n",
        "  if tamanho == \"medio\":\n",
        "    nlp = en_core_web_md.load()\n",
        "\n",
        "  if tamanho == \"pequeno\":\n",
        "    nlp = en_core_web_sm.load()\n",
        "\n",
        "  textcat = nlp.create_pipe(\"textcat\")\n",
        "  textcat.add_label(\"POSITIVE\")\n",
        "  textcat.add_label(\"NEGATIVE\")\n",
        "  \n",
        "  nlp.add_pipe(textcat, last=True)\n",
        "  \n",
        "  nlp.begin_training()\n",
        "\n",
        "  if criar_trainset:\n",
        "    vetores_trainset = buscar_vetores(trainset_aumentado)\n",
        "    vetores_trainset['entities'] = vetores_trainset['tweet']\n",
        "    vetores_trainset.drop(\"tweet\", inplace= True, axis=1)\n",
        "\n",
        "    vetores_trainset.to_csv('/content/vetores_trainset_aumentado_'+ tamanho + '.txt', sep=' ')\n",
        "\n",
        "  if criar_testset_a:\n",
        "    vetores_testset_a = buscar_vetores(testset_a_aumentado)\n",
        "    vetores_testset_a['entities'] = vetores_testset_a['tweet']\n",
        "    vetores_testset_a.drop(\"tweet\", inplace= True, axis=1)\n",
        "    vetores_testset_a.to_csv('/content/vetores_testset_a_aumentado_'+ tamanho + '.txt', sep=' ')\n",
        "\n",
        "\n",
        "  if criar_testset_b:\n",
        "    vetores_testset_b = buscar_vetores(testset_b_aumentado)\n",
        "    vetores_testset_b['entities'] = vetores_testset_b['tweet']\n",
        "    vetores_testset_b.drop(\"tweet\", inplace= True, axis=1)\n",
        "    vetores_testset_b.to_csv('/content/vetores_testset_b_aumentado_'+ tamanho + '.txt', sep=' ')\n",
        "\n",
        "  nlp.remove_pipe('textcat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "An5iJWSbkzBq"
      },
      "source": [
        "### Pipelines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76i1-Vp5fS-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "cellView": "form",
        "outputId": "73b9a911-95d3-4999-8bf9-32863a8a0d3c"
      },
      "source": [
        "#@title Download dos datasets e vetores pré-configurados\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/trainset_aumentado.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/testset_a_aumentado.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/testset_b_aumentado.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_trainset_aumentado_grande.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_trainset_aumentado_medio.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_trainset_aumentado_pequeno.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_a_aumentado_grande.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_a_aumentado_medio.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_a_aumentado_pequeno.txt\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_b_aumentado_grande.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_b_aumentado_medio.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_b_aumentado_pequeno.txt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1814k  100 1814k    0     0  3091k      0 --:--:-- --:--:-- --:--:-- 3086k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  143k  100  143k    0     0   767k      0 --:--:-- --:--:-- --:--:--  767k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 39213  100 39213    0     0   177k      0 --:--:-- --:--:-- --:--:--  177k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 53.9M  100 53.9M    0     0  15.2M      0  0:00:03  0:00:03 --:--:-- 15.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 54.1M  100 54.1M    0     0  30.2M      0  0:00:01  0:00:01 --:--:-- 30.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 23.6M  100 23.6M    0     0  20.2M      0  0:00:01  0:00:01 --:--:-- 20.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3646k  100 3646k    0     0  4681k      0 --:--:-- --:--:-- --:--:-- 4681k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3652k  100 3652k    0     0  3746k      0 --:--:-- --:--:-- --:--:-- 3742k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1580k  100 1580k    0     0  2937k      0 --:--:-- --:--:-- --:--:-- 2932k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1016k  100 1016k    0     0  2062k      0 --:--:-- --:--:-- --:--:-- 2062k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1018k  100 1018k    0     0  2418k      0 --:--:-- --:--:-- --:--:-- 2413k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  441k  100  441k    0     0  1350k      0 --:--:-- --:--:-- --:--:-- 1346k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w787-LvAbd7D",
        "colab": {}
      },
      "source": [
        "#@title Leitura dos datasets aumentados e respectivos vetores { display-mode: \"form\" }\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#pasta raiz dos arquivos\n",
        "pasta_raiz = '/content'\n",
        "\n",
        "trainset_aumentado = pd.read_csv(pasta_raiz+'/trainset_aumentado.txt', sep=' ', index_col='id')\n",
        "\n",
        "testset_a_aumentado = pd.read_csv(pasta_raiz+'/testset_a_aumentado.txt', sep=' ', index_col='id')\n",
        "\n",
        "testset_b_aumentado = pd.read_csv(pasta_raiz+'/testset_b_aumentado.txt', sep=' ', index_col='id')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vetores_trainset_aumentado_grande = pd.read_csv(pasta_raiz+'/vetores_trainset_aumentado_grande.txt', sep=' ', index_col = 'id')\n",
        "vetores_trainset_aumentado_grande.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_trainset_aumentado_medio = pd.read_csv(pasta_raiz+'/vetores_trainset_aumentado_medio.txt', sep=' ', index_col = 'id')\n",
        "vetores_trainset_aumentado_medio.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_trainset_aumentado_pequeno = pd.read_csv(pasta_raiz+'/vetores_trainset_aumentado_pequeno.txt', sep=' ', index_col = 'id')\n",
        "vetores_trainset_aumentado_pequeno.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_a_aumentado_grande = pd.read_csv(pasta_raiz+'/vetores_testset_a_aumentado_grande.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_a_aumentado_grande.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_a_aumentado_medio = pd.read_csv('vetores_testset_a_aumentado_medio.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_a_aumentado_medio.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_a_aumentado_pequeno = pd.read_csv('vetores_testset_a_aumentado_pequeno.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_a_aumentado_pequeno.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_b_aumentado_grande = pd.read_csv('vetores_testset_b_aumentado_grande.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_b_aumentado_grande.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_b_aumentado_medio = pd.read_csv('vetores_testset_b_aumentado_medio.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_b_aumentado_medio.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_b_aumentado_pequeno = pd.read_csv('vetores_testset_b_aumentado_pequeno.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_b_aumentado_pequeno.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "dict_vetores = {'pequeno':[vetores_trainset_aumentado_pequeno, vetores_testset_a_aumentado_pequeno, vetores_testset_b_aumentado_pequeno],\n",
        "                'medio':[vetores_trainset_aumentado_medio, vetores_testset_a_aumentado_medio, vetores_testset_b_aumentado_medio],\n",
        "                'grande':[vetores_trainset_aumentado_grande, vetores_testset_a_aumentado_grande, vetores_testset_b_aumentado_grande]\n",
        "}\n",
        "\n",
        "VOCABULARIO_TWEETS = TfidfVectorizer().fit(pd.concat([testset_a_aumentado['tweet'], \n",
        "                                                      testset_b_aumentado['tweet'], \n",
        "                                                      trainset_aumentado['tweet']])).vocabulary_\n",
        "VOCABULARIO_ENTIDADES = TfidfVectorizer().fit(pd.concat([vetores_testset_a_aumentado_grande['entities'], \n",
        "                                                         vetores_testset_b_aumentado_grande['entities'], \n",
        "                                                         vetores_trainset_aumentado_grande['entities']])).vocabulary_"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKZNUasoBtQf",
        "colab_type": "text"
      },
      "source": [
        "#### Funções auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgUx-uoDk8Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names= []):\n",
        "        self.attribute_names = attribute_names\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        if len(self.attribute_names) == 1:\n",
        "          print(\"nome do atributo: \", self.attribute_names)\n",
        "          print(X.head())          \n",
        "          return X.loc[:,self.attribute_names[0]]\n",
        "        return X[self.attribute_names]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XM3ClBxIGOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class Vetorizador_tfidf(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,               \n",
        "                 use_idf = False, ngram_range = (1,1), \n",
        "                 tokenizer = TweetTokenizer().tokenize):\n",
        "      \n",
        "        self.use_idf\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(use_idf = use_idf, ngram_range=ngram_range, tokenizer=tokenizer)\n",
        "    \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        return self.tfidf_vectorizer.fit(pd.concat([testset_a_aumentado['tweet'], testset_b_aumentado['tweet'], trainset_aumentado['tweet']]))\n",
        "    def transform(self, X):\n",
        "        \n",
        "        return self.tfidf_vectorizer.transform(X)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ub6dRZ_Pe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "def pipelinizar(function, active=True):\n",
        "    def list_comprehend_a_function(list_or_series, active=True):\n",
        "        if active:\n",
        "            return [function(i) for i in list_or_series]\n",
        "        else: # if it's not active, just pass it right back\n",
        "            return list_or_series\n",
        "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ttAsDinw4oY6"
      },
      "source": [
        "#### Pipeline vetorização do texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqZ9z68G4rHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "167aba05-313d-4cc5-ad05-4059aaf79139"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "vetorizar_tweets = ColumnTransformer(\n",
        "        [('vetorizar' ,  TfidfVectorizer(vocabulary=VOCABULARIO_TWEETS),\"tweet\")])\n",
        "\n",
        "\n",
        "vetorizar_tweets.fit_transform(trainset_aumentado.join(dict_vetores['grande'][0])).toarray().shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13207, 16678)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDTo2hNPk3vp",
        "colab_type": "text"
      },
      "source": [
        "#### Pipeline vetor de entidades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JJW9j-llUOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5986c7c1-8afc-4c6b-ce7f-e6669f733e35"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "anexar_entidades = ColumnTransformer(\n",
        "        [('vetorizar' ,  TfidfVectorizer(vocabulary=VOCABULARIO_ENTIDADES),\"entities\")])\n",
        "\n",
        "anexar_entidades.fit_transform(trainset_aumentado.join(dict_vetores['grande'][0])).toarray().shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13207, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3JUiqit-wlIM"
      },
      "source": [
        "#### Pipeline features numéricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkubnGDtwrZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Selecionar_Features_Numericas(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names= []):\n",
        "        self.attribute_names = attribute_names\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "\n",
        "        return X[X.dtypes[X.dtypes == 'float64'].index].values\n",
        "\n",
        "features_numericas = Pipeline(steps= [('features_numericas', Selecionar_Features_Numericas())])\n",
        "# features_numericas.fit_transform(X_train)        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cNwAicP-117Q"
      },
      "source": [
        "### União das features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6lGPKT514op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "uniao_features = FeatureUnion([(\"tratamento_entidades\", ColumnTransformer([\n",
        "                                                                        ('vetorizar',\n",
        "                                                                         TfidfVectorizer(vocabulary= VOCABULARIO_ENTIDADES),\n",
        "                                                                         \"entities\")])), #vetorizar a coluna entidades\n",
        "                               (\"features_numericas\", Selecionar_Features_Numericas()),   #concatenar todas as features numéricas sem alteração\n",
        "                               (\"tratamento_tweets\", ColumnTransformer([\n",
        "                                                                        ('vetorizar',\n",
        "                                                                         TfidfVectorizer(vocabulary= VOCABULARIO_TWEETS, tokenizer= TweetTokenizer().tokenize),\n",
        "                                                                        \"tweet\")]))])   #vetorizar os tweets\n",
        "\n",
        "#  ('vetorizar_reduzir' ,  \n",
        "#                                                                         Pipeline([('vetorizar',TfidfVectorizer(vocabulary= VOCABULARIO_TWEETS, tokenizer= TweetTokenizer().tokenize)), ('reduzir_dim', NMF())]),\n",
        "#                                                                         \"tweet\")]))])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2WlCftBGa5l",
        "colab_type": "text"
      },
      "source": [
        "### Subtask A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFiF8D1J0UOq",
        "colab_type": "text"
      },
      "source": [
        "##### Histograma dos targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j61p6Ll8BVE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d884a8f5-9817-4a20-9144-5693e10b634d"
      },
      "source": [
        "trainset_aumentado.subtask_a.fillna(\"NaN\").hist()\n",
        "# tweets.subtask_a.hist()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7b01ae6ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOO0lEQVR4nO3df6zddX3H8edrdKh0E1CWG9OytYnNFhxxYzfIplvuYIGKZsVFDQubxTXp/mDqCG5W9wdGJYNFhmjUpBtINWTImFubYeYYeJMtCygFIwNk3OEP2uDPAu6q4C5574/7KTtAL/fc3nPPbfd5PpKbfr+f7+f765/nOf32nNtUFZKkPvzEal+AJGl8jL4kdcToS1JHjL4kdcToS1JH1qz2BTyfk046qTZs2HDY+//gBz9g7dq1o7sgSRqT5fRr7969362qnznUtiM6+hs2bODOO+887P2np6eZmpoa3QVJ0pgsp19Jvr7QNh/vSFJHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjuhv5ErSatqw4+ZVO/d1m1fmV8j4Tl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOjJU9JNcnOTeJP+R5G+SvDDJxiR3JJlJ8ukkx7a5L2jrM237hoHjvLuNP5DknJW5JUnSQhaNfpJ1wNuByar6ReAY4HzgCuCqqno58Ciwre2yDXi0jV/V5pHklLbfK4DNwMeSHDPa25EkPZ9hH++sAV6UZA1wHPAIcCZwU9u+CzivLW9p67TtZyVJG7+hqp6sqq8CM8Dpy78FSdKwFv3vEqtqf5IPAt8AfgT8M7AXeKyq5tq0fcC6trwOeLjtO5fkceClbfz2gUMP7vO0JNuB7QATExNMT08v/a6a2dnZZe0vqW+XnDq3+KQVslL9WjT6SU5k/l36RuAx4G+ZfzyzIqpqJ7ATYHJysqampg77WNPT0yxnf0l9u3CV/4/clejXMI93fgv4alV9p6r+B/gM8GrghPa4B2A9sL8t7wdOBmjbjwe+Nzh+iH0kSWMwTPS/AZyR5Lj2bP4s4D7g88Ab25ytwO62vKet07bfVlXVxs9vn+7ZCGwCvjCa25AkDWOYZ/p3JLkJuAuYA+5m/vHLzcANST7Qxq5pu1wDfCrJDHCA+U/sUFX3JrmR+ReMOeCiqnpqxPcjSXoei0YfoKouBS591vBDHOLTN1X1BPCmBY5zGXDZEq9RkjQifiNXkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0Zfkjpi9CWpI0NFP8kJSW5K8pUk9yf51SQvSXJLkgfbnye2uUny4SQzSb6c5LSB42xt8x9MsnWlbkqSdGjDvtO/GvinqvoF4JXA/cAO4Naq2gTc2tYBXgtsaj/bgY8DJHkJcCnwKuB04NKDLxSSpPFYNPpJjgd+A7gGoKp+XFWPAVuAXW3aLuC8trwF+GTNux04IcnLgHOAW6rqQFU9CtwCbB7p3UiSnteaIeZsBL4DfCLJK4G9wDuAiap6pM35JjDRltcBDw/sv6+NLTT+DEm2M/83BCYmJpienh72Xp5jdnZ2WftL6tslp86t2rlXql/DRH8NcBrwtqq6I8nV/N+jHACqqpLUKC6oqnYCOwEmJydramrqsI81PT3NcvaX1LcLd9y8aue+bvPaFenXMM/09wH7quqOtn4T8y8C32qPbWh/frtt3w+cPLD/+ja20LgkaUwWjX5VfRN4OMnPt6GzgPuAPcDBT+BsBXa35T3AW9qneM4AHm+PgT4HnJ3kxPYPuGe3MUnSmAzzeAfgbcD1SY4FHgLeyvwLxo1JtgFfB97c5n4WOBeYAX7Y5lJVB5K8H/him/e+qjowkruQJA1lqOhX1ZeAyUNsOusQcwu4aIHjXAtcu5QLlCSNjt/IlaSOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6sia1b6AlXTP/se5cMfNYz/v1y5/3djPKUnD8J2+JHXE6EtSR4y+JHXE6EtSR4y+JHVk6OgnOSbJ3Un+sa1vTHJHkpkkn05ybBt/QVufads3DBzj3W38gSTnjPpmJEnPbynv9N8B3D+wfgVwVVW9HHgU2NbGtwGPtvGr2jySnAKcD7wC2Ax8LMkxy7t8SdJSDBX9JOuB1wF/3dYDnAnc1KbsAs5ry1vaOm37WW3+FuCGqnqyqr4KzACnj+ImJEnDGfbLWR8C/hT46bb+UuCxqppr6/uAdW15HfAwQFXNJXm8zV8H3D5wzMF9npZkO7AdYGJigunp6WHv5TkmXgSXnDq3+MQRW841SzpyrEY/DpqdnV2Rliwa/SSvB75dVXuTTI38Cp6lqnYCOwEmJydraurwT/mR63dz5T3j/9Lx1y6YGvs5JY3eanyj/6DrNq9lOf1byDBFfDXw20nOBV4IvBi4GjghyZr2bn89sL/N3w+cDOxLsgY4HvjewPhBg/tIksZg0Wf6VfXuqlpfVRuY/4fY26rqAuDzwBvbtK3A7ra8p63Ttt9WVdXGz2+f7tkIbAK+MLI7kSQtajnPPt4F3JDkA8DdwDVt/BrgU0lmgAPMv1BQVfcmuRG4D5gDLqqqp5ZxfknSEi0p+lU1DUy35Yc4xKdvquoJ4E0L7H8ZcNlSL1KSNBp+I1eSOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0Jakji0Y/yclJPp/kviT3JnlHG39JkluSPNj+PLGNJ8mHk8wk+XKS0waOtbXNfzDJ1pW7LUnSoQzzTn8OuKSqTgHOAC5KcgqwA7i1qjYBt7Z1gNcCm9rPduDjMP8iAVwKvAo4Hbj04AuFJGk8Fo1+VT1SVXe15f8G7gfWAVuAXW3aLuC8trwF+GTNux04IcnLgHOAW6rqQFU9CtwCbB7p3UiSnteapUxOsgH4ZeAOYKKqHmmbvglMtOV1wMMDu+1rYwuNP/sc25n/GwITExNMT08v5RKfYeJFcMmpc4e9/+FazjVLOnKsRj8Omp2dXZGWDB39JD8F/B3wx1X1/SRPb6uqSlKjuKCq2gnsBJicnKypqanDPtZHrt/Nlfcs6XVtJL52wdTYzylp9C7ccfOqnfu6zWtZTv8WMtSnd5L8JPPBv76qPtOGv9Ue29D+/HYb3w+cPLD7+ja20LgkaUyG+fROgGuA+6vqLwc27QEOfgJnK7B7YPwt7VM8ZwCPt8dAnwPOTnJi+wfcs9uYJGlMhnn28Wrg94F7knypjb0HuBy4Mck24OvAm9u2zwLnAjPAD4G3AlTVgSTvB77Y5r2vqg6M5C4kSUNZNPpV9W9AFth81iHmF3DRAse6Frh2KRcoSRodv5ErSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUkbFHP8nmJA8kmUmyY9znl6SejTX6SY4BPgq8FjgF+N0kp4zzGiSpZ+N+p386MFNVD1XVj4EbgC1jvgZJ6taaMZ9vHfDwwPo+4FWDE5JsB7a31dkkDyzjfCcB313G/oclV4z7jJL+v/nNK5bVr59baMO4o7+oqtoJ7BzFsZLcWVWToziWJI3TSvVr3I939gMnD6yvb2OSpDEYd/S/CGxKsjHJscD5wJ4xX4MkdWusj3eqai7JHwGfA44Brq2qe1fwlCN5TCRJq2BF+pWqWonjSpKOQH4jV5I6YvQlqSNHbfSTrE+yO8mDSf4rydVJjk0yleTxJF9qP//S5r83yf6B8ctX+x4k9SdJJblyYP2dSd47sL49yVfazxeSvKaN/31r18yzGvdrSzn/Efc5/WEkCfAZ4ONVtaX9eoedwGXAzcC/VtXrD7HrVVX1wTFeqiQ925PA7yT586p6xpevkrwe+EPgNVX13SSnAf+Q5PSqekObMwW8c4HGLepofad/JvBEVX0CoKqeAi4G/gA4bjUvTJIWMcf8m9SLD7HtXcCfHHwxqKq7gF3ARaM6+dEa/VcAewcHqur7wDeAlwO/PvBXnz8bmHbxwPg5Y7xeSRr0UeCCJMc/a/w5bQPubOMjcVQ+3hmCj3ckHbGq6vtJPgm8HfjROM99tL7Tvw/4lcGBJC8GfhaYWZUrkqSl+RCwDVg7MPactrX1kX2J9WiN/q3AcUneAk//nv4rgeuAH67idUnSUKrqAHAj8+E/6C+AK5K8FCDJLwEXAh8b1XmPyujX/NeI3wC8KcmDwH8CTwDvWdULk6SluZL5XwEPQFXtAa4F/j3JV4C/An6vqh4Z1Qn9NQyS1JGj8p2+JOnwGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SO/C9b5WuioUKIOwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVFlc7BuA1ji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "aa7a1f7d-3232-43b1-9618-8512b7f6274f"
      },
      "source": [
        "trainset_aumentado.subtask_b.fillna(\"NaN\").hist()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7b01601240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He8JVu2ZKbgz",
        "colab_type": "text"
      },
      "source": [
        "#### Segregação de dados de treino e de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QU1Vnsk9rqa",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2f6ee486-c211-4857-b7ce-2f21c6eec825"
      },
      "source": [
        "#@title Separação de dados em conjutos de teste e treino\n",
        "test_size = 0.25 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "target = \"subtask_b\" #@param [\"subtask_a\", \"subtask_b\", \"subtask_c\"]\n",
        "estratificar_alvo = True #@param {type:\"boolean\"}\n",
        "remover_nan = True #@param {type:\"boolean\"}\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = trainset_aumentado.drop(trainset_aumentado.loc[:,trainset_aumentado.columns.str.startswith('subtask')], axis=1)\n",
        "if remover_nan:\n",
        "  y = trainset_aumentado[target].dropna()\n",
        "  X = X[trainset_aumentado[target] == trainset_aumentado[target]]\n",
        "else:\n",
        "  y = trainset_aumentado[target].fillna(\"NOT\")\n",
        "\n",
        "y = binarizador_label.fit_transform(y)\n",
        "stratify = None \n",
        "if estratificar_alvo == True:\n",
        "  stratify = y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state= 42, stratify= stratify)\n",
        "\n",
        "print(\"Quantidade de orbservações de teste: \", pd.Series(binarizador_label.inverse_transform(y_test)).count())\n",
        "print(\"Percentual por categoria\")\n",
        "print(pd.Series(binarizador_label.inverse_transform(y_test)).value_counts()*100/pd.Series(binarizador_label.inverse_transform(y_test)).count())\n",
        "print('-------------')\n",
        "print(\"Quantidade de orbservações de treino: \", pd.Series(binarizador_label.inverse_transform(y_train)).count())\n",
        "print(\"Percentual por categoria\")\n",
        "print(pd.Series(binarizador_label.inverse_transform(y_train)).value_counts()*100/pd.Series(binarizador_label.inverse_transform(y_train)).count())\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quantidade de orbservações de teste:  1098\n",
            "Percentual por categoria\n",
            "TIN    88.069217\n",
            "UNT    11.930783\n",
            "dtype: float64\n",
            "-------------\n",
            "Quantidade de orbservações de treino:  3294\n",
            "Percentual por categoria\n",
            "TIN    88.099575\n",
            "UNT    11.900425\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omD5r8ov9MFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "spacy_tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_grid_anexar_bases_externas = {'anexar_bases_externas__tamanho': ['pequeno', 'medio', 'grande']}\n",
        "\n",
        "param_grid_transformar_colunas = {'transformar_colunas__tweet__vetorizar__ngram_range': [(1,1),(2,2),(1,2)],\n",
        "                                  'transformar_colunas__tweet__vetorizar__tokenizer'  : [TweetTokenizer().tokenize],\n",
        "                                  'transformar_colunas__tweet__vetorizar__use_idf'    : [True,False],\n",
        "                                  'transformar_colunas__tweet__reduzir__n_components' : randint(10,10000),\n",
        "                                  'transformar_colunas__empath__n_components'         : [10,20,50,100],\n",
        "                                  'transformar_colunas__entities__vetorizar__use_idf' : [True,False],\n",
        "                            }\n",
        "\n",
        "param_grid_reducao_dimensionalidade = {'reducao_dimensionalidade__n_components' : randint(100,1000)}\n",
        "\n",
        "dict_algoritmos = {'logistic_regression_1': LogisticRegression(),\n",
        "                   'logistic_regression_2': LogisticRegression(),\n",
        "                   'knn'                  : KNeighborsClassifier(), \n",
        "                   'svc'                  : SVC(),\n",
        "                   'sgdc'                 : SGDClassifier(),\n",
        "                   'random_forest'        : RandomForestClassifier(),\n",
        "                   'ada_boost'            : AdaBoostClassifier(),\n",
        "                   'extra_trees'          : ExtraTreesClassifier(),\n",
        "                   'grad_boost'           : GradientBoostingClassifier(), \n",
        "                   'xgboost'              : xgb.XGBClassifier()}\n",
        "\n",
        "dict_param_grid_algoritmos = {'logistic_regression_1': {'classificador__penalty'        : 'l1', \n",
        "                                                       'classificador__C'               : [100, 10, 1.0, 0.1, 0.01], \n",
        "                                                       'classificador__fit_intercept'   : [False, True], \n",
        "                                                       'classificador__solver'          : ['liblinear', 'saga'],\n",
        "                                                       'classificador__n_jobs'          : [-1]},\n",
        "                             'logistic_regression_2': {'classificador__penalty'         : 'l2', \n",
        "                                                       'classificador__C'               : [100, 10, 1.0, 0.1, 0.01], \n",
        "                                                       'classificador__fit_intercept'   : [False, True], \n",
        "                                                       'classificador__solver'          : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
        "                                                       'classificador__n_jobs'          : [-1]},\n",
        "                             'knn'                  : {}, \n",
        "                             'svc'                  : {'classificador__C'               : [1.0, 10.0, 100.0],\n",
        "                                                       'classificador__gamma'           : ['scale', 'auto'], \n",
        "                                                       'classificador__kernel'          : ['linear', 'poly', 'rbf']},\n",
        "                             'sgdc'                 : {},\n",
        "                             'random_forest'        : {'classificador__n_estimators'    : randint(100,1000), \n",
        "                                                       'classificador__max_depth'       : [None, 2, 3, 4, 5, 7, 10,100,200],\n",
        "                                                       'classificador__min_samples_leaf': randint(1,4),\n",
        "                                                       'classificador__bootstrap'       : [True,False],\n",
        "                                                       'classificador__criterion'       : ['gini', 'entropy']},\n",
        "                             'ada_boost'            : {},\n",
        "                             'extra_trees'          : {},\n",
        "                             'grad_boost'           : {}, \n",
        "                             'xgboost'              : {'classificador__learning_rate'   : [0.05, 0.10, 0.15, 0.2, 0.25, 0.3],\n",
        "                                                       'classificador__max_depth'       : [3,4,5,6,8,10,12,15] ,\n",
        "                                                       'classificador__min_child_weight': [1,3,5,7],\n",
        "                                                       'classificador__gamma'           : [0.0,0.1,0.2,0.3,0.4],\n",
        "                                                       'classificador__colsample_bytree': [0.3, 0.4, 0.5, 0.7]}}\n",
        "\n",
        "\n",
        "param_grid_linear_svm = {'penalty' : ['l1','l2'] , 'loss' : ['hinge', 'squared_hinge'], 'C' : [0.01, 1, 10, 100], 'random_state' : [0, 42]}\n",
        "\n",
        "param_grid_nb_multi = {'alpha' : [0.1, 1.0, 2] , 'fit_prior' : [False, True]}\n",
        "\n",
        "param_grid_nb_gauss = {'var_smoothing' : [0.1, 1.0, 2]}"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a32Ac04vuTZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Anexar_Bases_Externas(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, tamanho = \"\"):\n",
        "        self.tamanho = tamanho\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "      \n",
        "      for vetor in dict_vetores[self.tamanho]:\n",
        "\n",
        "        \n",
        "\n",
        "        if X.index.isin(vetor.index).all():\n",
        "          return X.join(vetor.loc[X.index.values])\n",
        "  \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coYlVbxV77cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "# !pip install empath\n",
        "from empath import Empath \n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "spacy_tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "\n",
        "\n",
        "transformar_colunas = ColumnTransformer([('tweet',    Pipeline([('vetorizar',  TfidfVectorizer(vocabulary= VOCABULARIO_TWEETS, \n",
        "                                                                                               tokenizer = TweetTokenizer().tokenize)),\n",
        "                                                                ('reduzir',    TruncatedSVD())]), 'tweet'),\n",
        "                                         ('empath',   TruncatedSVD(),          list(Empath().cats.keys())),\n",
        "                                         ('entities', Pipeline([('vetorizar',  TfidfVectorizer(vocabulary= VOCABULARIO_ENTIDADES)),\n",
        "                                                                ('reduzir',    TruncatedSVD())]), 'entities')],\n",
        "                                          remainder=  'passthrough') #colunas remanescentes\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2JCyctn_jcM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "cellView": "both",
        "outputId": "90557bca-6f6b-4144-e7b1-e440535f040f"
      },
      "source": [
        "#@title Otimização de hiperparâmetros\n",
        "algoritmo = \"random_forest\" #@param ['logistic_regression_1', 'logistic_regression_2', 'knn', 'svc', 'sgdc', 'random_forest', 'ada_boost', 'extra_trees', 'grad_boost', 'xgboost']\n",
        "num_folds = 6 #@param {type:\"integer\"}\n",
        "\n",
        "# busca_randomizada = True #@param {type:\"boolean\"}\n",
        "num_inter = 10 #@param {type:\"integer\"}\n",
        "usar_todo_dataset = True #@param {type:\"boolean\"}\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "\n",
        "param_grid = {**param_grid_anexar_bases_externas, \n",
        "              **param_grid_transformar_colunas, \n",
        "              **param_grid_reducao_dimensionalidade, \n",
        "              **dict_param_grid_algoritmos[algoritmo]}\n",
        "\n",
        "\n",
        "canal_dados = Pipeline([(\"anexar_bases_externas\",\n",
        "                              Anexar_Bases_Externas()),\n",
        "                        ('transformar_colunas',\n",
        "                              transformar_colunas),\n",
        "                        ('reducao_dimensionalidade',  \n",
        "                              PCA()), \n",
        "                        ('classificador', \n",
        "                              dict_algoritmos[algoritmo])\n",
        "    ])\n",
        "cv = RepeatedStratifiedKFold(n_splits= num_folds, n_repeats=3)\n",
        "args_search_cv = {'estimator':  canal_dados, \n",
        "                  'cv'                 : cv, \n",
        "                  'scoring'            :'f1_macro', \n",
        "                  'n_jobs'             : -1,\n",
        "                  'verbose'            : 10,\n",
        "                  'n_iter'             : num_inter,\n",
        "                  'param_distributions': param_grid,\n",
        "                  'random_state'       : 42 }\n",
        "\n",
        "random_search = RandomizedSearchCV(**args_search_cv) \n",
        "\n",
        "if usar_todo_dataset:\n",
        "  random_search.fit(X, y)\n",
        "else:\n",
        "  random_search.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 18 folds for each of 10 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   19.4s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   37.6s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  9.0min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed: 13.5min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed: 15.6min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed: 24.1min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1syq5EyTPEzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "pca_test = PCA(n_components=30)\n",
        "pca_test.fit(X_train_scaled)\n",
        "sns.set(style='whitegrid')\n",
        "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.axvline(linewidth=4, color='r', linestyle = '--', x=10, ymin=0, ymax=1)\n",
        "display(plt.show())\n",
        "evr = pca_test.explained_variance_ratio_\n",
        "cvr = np.cumsum(pca_test.explained_variance_ratio_)\n",
        "pca_df = pd.DataFrame()\n",
        "pca_df['Cumulative Variance Ratio'] = cvr\n",
        "pca_df['Explained Variance Ratio'] = evr\n",
        "display(pca_df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YLXWty7IYL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQ3TqKle-a9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import recall_score\n",
        "scoring = {'prec_macro': 'precision_macro',\n",
        "            'rec_macro': make_scorer(recall_score, average='macro')}\n",
        "scores = cross_validate(random_search, X, y, scoring=scoring,\n",
        "                         cv=5, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnfZiorW3wLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "3cb72897-c521-4dfe-c29b-017201f24db7"
      },
      "source": [
        "#gridsearchcv.best_params_\n",
        "random_search.best_params_"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anexar_bases_externas__tamanho': 'grande',\n",
              " 'classificador__colsample_bytree': 0.5,\n",
              " 'classificador__gamma': 0.4,\n",
              " 'classificador__learning_rate': 0.15,\n",
              " 'classificador__max_depth': 3,\n",
              " 'classificador__min_child_weight': 3,\n",
              " 'transformar_colunas__empath__n_components': 50,\n",
              " 'transformar_colunas__entities__vetorizar__use_idf': False,\n",
              " 'transformar_colunas__tweet__reduzir__n_components': 100,\n",
              " 'transformar_colunas__tweet__vetorizar__ngram_range': (1, 1),\n",
              " 'transformar_colunas__tweet__vetorizar__tokenizer': <bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fb5cd70c6d8>>,\n",
              " 'transformar_colunas__tweet__vetorizar__use_idf': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PUvf4RXCdIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a819f10-8cfb-4a35-d585-6904d3396eca"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypFQpczQZSo3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "777614d9-0923-47ff-94a0-447ccaee1070"
      },
      "source": [
        "y_pred = random_search.predict(X_test)\n",
        "print(\"F1 Score\")\n",
        "print(f1_score(y_test,binarizador_label.inverse_transform(y_pred), average=None))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score\n",
            "[0.9370155  0.01515152]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfODskRULZwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "074148f5-2228-45f0-c300-66fc133e6430"
      },
      "source": [
        "pd.Series(y_pred).value_counts()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1097\n",
              "1       1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CyaC2v4LPGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1f3cb13f-2988-4d8b-a13b-fbd1d9c11228"
      },
      "source": [
        "t = gridsearchcv.predict(testset_a_aumentado)\n",
        "pd.Series(t).value_counts()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NOT    674\n",
              "OFF    186\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi7kuRmW42Yh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "fb25cda3-a8f3-4416-a1a2-39dda94d090b"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Melhores parâmetros: {}\".format(random_search.best_params_)) \n",
        "print(\"melhor score {}\".format(random_search.best_score_))\n",
        "# print(rand_forest_cv.score(X_test, y_test))\n",
        "plot_confusion_matrix(random_search, X, y, display_labels=binarizador_label.classes_)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Melhores parâmetros: {'anexar_bases_externas__tamanho': 'medio', 'classificador__bootstrap': False, 'classificador__criterion': 'gini', 'classificador__max_depth': 5, 'classificador__min_samples_leaf': 1, 'classificador__n_estimators': 660, 'transformar_colunas__empath__n_components': 50, 'transformar_colunas__entities__vetorizar__use_idf': True, 'transformar_colunas__tweet__reduzir__n_components': 50, 'transformar_colunas__tweet__vetorizar__ngram_range': (2, 2), 'transformar_colunas__tweet__vetorizar__tokenizer': <bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fb5cd70c6d8>>, 'transformar_colunas__tweet__vetorizar__use_idf': False}\n",
            "melhor score 0.88091985428051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb5cd7d2f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEqtCs-uGhmE",
        "colab_type": "text"
      },
      "source": [
        "#### Algoritmos utilizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB8DWNizFko7",
        "colab_type": "text"
      },
      "source": [
        "#### Ajuste de hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgoyyk0Kn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZr3AvppA6e1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f92a3a01-6945-4337-a3cc-7a8e35bd6728"
      },
      "source": [
        "def spacy_tokenizer_lemmatizer(text):\n",
        "    \n",
        "    nlp = English()\n",
        "    tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "    tokens = tokenizer(text)\n",
        "    \n",
        "    lemma_list = []\n",
        "    for token in tokens:\n",
        "        if (token.is_stop and token.is_punct) is False :\n",
        "            lemma_list.append(token.lemma_)\n",
        "    \n",
        "    return(lemma_list)\n",
        "\n",
        "print(tweets.tweet[0])\n",
        "spacy_tokenizer_lemmatizer(tweets.tweet[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@USER She should ask a few native Americans what their take on this is.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@USER', 'ask', 'native', 'Americans']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFitjJ_eLcZ",
        "colab_type": "text"
      },
      "source": [
        "## rascunhos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2jJxhY0l-wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "94e884f8-b9f1-4337-dd80-3931a75b5e7a"
      },
      "source": [
        "for sent in tweets['tokens_sem_stopwords'][0]:\n",
        "  for palavra in sent:\n",
        "    print(palavra)\n",
        "    if len(wn.synsets(palavra))>0:\n",
        "      print(wn.synsets(palavra)[0].hypernyms()[0].name())\n",
        "    \n",
        "\n",
        "def busca_hiperonimos(lista_sentencas_tokenizadas):\n",
        "  dicionario_sinonimos = dict()\n",
        "  dicionario_antonimos = dict()\n",
        "\n",
        "  for sent in lista_sentencas_tokenizadas:\n",
        "    for palavra in sent:\n",
        "      sinonimos = []\n",
        "      antonimos = []\n",
        "      for syn  in wn.synsets(palavra):\n",
        "        for l in syn.lemmas():\n",
        "          if l.name() not in sinonimos:\n",
        "            sinonimos.append(l.name()) \n",
        "          if l.antonyms():\n",
        "              antonimos.append(l.antonyms()[0].name())\n",
        "      if len(sinonimos) > 0:\n",
        "        dicionario_sinonimos[palavra] = sinonimos\n",
        "      if len(antonimos) > 0:\n",
        "        dicionario_antonimos[palavra] = antonimos\n",
        "    \n",
        "  return dicionario_sinonimos, dicionario_antonimos\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@user\n",
            "ask\n",
            "communicate.v.02\n",
            "native\n",
            "person.n.01\n",
            "americans\n",
            "inhabitant.n.01\n",
            "take\n",
            "income.n.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zocVWj8Sm0du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, language='english'):\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  def is_punct(self, token):\n",
        "    return all(\n",
        "    unicodedata.category(char).startswith('P') for char in token)\n",
        "  def is_stopword(self, token):\n",
        "    return token.lower() in self.stopwords\n",
        "\n",
        "  def normalize(self, document):\n",
        "\n",
        "    return [\n",
        "    self.lemmatize(token, tag).lower()\n",
        "    for paragraph in document\n",
        "    for sentence in paragraph\n",
        "    for (token, tag) in sentence\n",
        "    if not self.is_punct(token) and not self.is_stopword(token)\n",
        "    ]\n",
        "\n",
        "\n",
        "def lemmatize(self, token, pos_tag):\n",
        "  tag = {\n",
        "  'N': wn.NOUN,\n",
        "  'V': wn.VERB,\n",
        "  'R': wn.ADV,\n",
        "  'J': wn.ADJ\n",
        "  }.get(pos_tag[0], wn.NOUN)\n",
        "  return self.lemmatizer.lemmatize(token, tag)\n",
        "\n",
        "def fit(self, X, y=None):\n",
        "  return self\n",
        "def transform(self, documents):\n",
        "  for document in documents:\n",
        "    yield self.normalize(document)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLfeL62XdHqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, language='english'):\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "  def is_punct(self, token):\n",
        "    return all(unicodedata.category(char).startswith('P') for char in token)\n",
        " \n",
        "  def is_stopword(self, token):\n",
        "    return token.lower() in self.stopwords\n",
        " \n",
        "  def normalize(self, document):\n",
        "    return [\n",
        "      self.lemmatize(token, tag).lower()\n",
        "      for paragraph in document\n",
        "        for sentence in paragraph\n",
        "          for (token, tag) in sentence\n",
        "            if not self.is_punct(token) and not self.is_stopword(token)\n",
        "    ]\n",
        "  \n",
        "  def lemmatize(self, token, pos_tag):\n",
        "    tag = {\n",
        "    'N': wn.NOUN,\n",
        "    'V': wn.VERB,\n",
        "    'R': wn.ADV,\n",
        "    'J': wn.ADJ\n",
        "    }.get(pos_tag[0], wn.NOUN)\n",
        "    return self.lemmatizer.lemmatize(token, tag)\n",
        "  \n",
        "  \n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def transform(self, documents):\n",
        "    for document in documents:\n",
        "      yield self.normalize(document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWqFAvhSVnH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "def pipelinize(function, active=True):\n",
        "    def list_comprehend_a_function(list_or_series, active=True):\n",
        "        if active:\n",
        "            return [function(i) for i in list_or_series]\n",
        "        else: # if it's not active, just pass it right back\n",
        "            return list_or_series\n",
        "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}