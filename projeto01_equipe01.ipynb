{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "projeto01_equipe01.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7I8N6Aj9oknZ",
        "51lTgeW1oYR0",
        "Cm4A3iMJoji_",
        "D0ZdGEizokw2",
        "VcbinyrLp0W5",
        "sRbt_9lnourv",
        "lCN3m1b7ougd",
        "UEfgrogNxi4Z"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardodut/Mineracao_dados_textos_web/blob/master/projeto01_equipe01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDkhltJeg5ag",
        "colab_type": "text"
      },
      "source": [
        "<b> EQUIPE: </b>\n",
        "  - Eduardo Façanha\n",
        "  - Giovanni Brígido\n",
        "  - Maurício Brito\n",
        "\n",
        "<b> ATIVIDADE 01 </b> - Pré-processamento dos textos (Prazo: 11/05/2020 - 30%)\n",
        "\n",
        "- Tokenização\n",
        "- Lematização\n",
        "- POS Tagging\n",
        "- Normalização (hashtags, menções, emojis e símbolos especiais)\n",
        "- Chunking\n",
        "- NER (entidades nomeadas)\n",
        "- Remoção stop-words\n",
        "\n",
        "<b> ATIVIDADE 02 </b> - Representação Semântica (Prazo: 30/06/2020 - 30%)\n",
        "\n",
        "- Uso de bases de conhecimento externas\n",
        "- Identificação de tópicos\n",
        "- Representação vetorial das palavras e textos\n",
        "\n",
        "<b> ATIVIDADE 03 </b> - Analise da Linguagem Ofensiva - Subtarefas A e B (Prazo: 30/07/2020 - 40%)\n",
        "\n",
        "- Resultado da subtarefa A para um conjunto de teste a ser fornecido\n",
        "- Resultado da subtarefa B para um conjunto de teste a ser fornecido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kcaie5jI2md",
        "colab_type": "text"
      },
      "source": [
        "## Atividade 03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIEThqy6XFPG",
        "colab_type": "text"
      },
      "source": [
        "### Bibliotecas utilizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZr2fyJboEcp",
        "colab_type": "text"
      },
      "source": [
        "Tecnologias utilizadas\n",
        "figura do scikit learn\n",
        "spacy\n",
        "nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZazwvFs-JE-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92aa7b68-f0f5-4f48-b4ec-cb44ffa1b868"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "!pip install Transformers\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg\n",
        "import en_core_web_md\n",
        "import en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "!python -m nltk.downloader wordnet\n",
        "!python -m nltk.downloader omw\n",
        "!pip install spacy-wordnet\n",
        "!pip install empath\n",
        "from empath import Empath "
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from Transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from Transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from Transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from Transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from Transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Transformers) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->Transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->Transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->Transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Transformers) (2.10)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n",
            "Requirement already satisfied: spacy-wordnet in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: nltk<3.4,>=3.3 in /usr/local/lib/python3.6/dist-packages (from spacy-wordnet) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.12.0)\n",
            "Requirement already satisfied: empath in /usr/local/lib/python3.6/dist-packages (0.89)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from empath) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6aUco5gIM9bU"
      },
      "source": [
        "#### Carregamento do arquivo de dados e transformação em DataFrame\n",
        "\n",
        "É realizado o download do arquivo e instanciado um DataFrame com os dados. A variável do DataFrame é chamada 'tweets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyxNSs8QOzMR",
        "colab_type": "text"
      },
      "source": [
        "#### Dataset_treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nqucHlrtM9bX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "9c547756-8b34-48a9-c24c-0e9314391a0e"
      },
      "source": [
        "#download o arquivo localizado no reposítório do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/olid-training-v1.0.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "tweets = pd.read_csv('/content/olid-training-v1.0.tsv', sep='\\t',encoding= 'utf-8', index_col = 'id')\n",
        "\n",
        "#verificação e remoção de duplicatas\n",
        "\n",
        "if tweets.duplicated(['tweet']).sum()>0:\n",
        "  tweets.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "print('TWEETS DUPLICADOS: ',tweets.duplicated(['tweet']).sum())\n",
        "\n",
        "\n",
        "#visualização dos primeiros registros\n",
        "tweets.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1915k  100 1915k    0     0  4102k      0 --:--:-- --:--:-- --:--:-- 4111k\n",
            "TWEETS DUPLICADOS:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>86426</th>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90194</th>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16820</th>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62688</th>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43605</th>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet  ... subtask_c\n",
              "id                                                        ...          \n",
              "86426  @USER She should ask a few native Americans wh...  ...       NaN\n",
              "90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       IND\n",
              "16820  Amazon is investigating Chinese employees who ...  ...       NaN\n",
              "62688  @USER Someone should'veTaken\" this piece of sh...  ...       NaN\n",
              "43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnPFaVz5M9bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0b47cf4-f84d-4281-b22f-47db2aba6884"
      },
      "source": [
        "#verificação e remoção de duplicatas\n",
        "\n",
        "if tweets.duplicated(['tweet']).sum()>0:\n",
        "  tweets.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "print('TWEETS DUPLICADOS: ',tweets.duplicated(['tweet']).sum())"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TWEETS DUPLICADOS:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3N2sunzJO8_I"
      },
      "source": [
        "#### testset_a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TEBtdccDO8_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "114abc20-a3f4-4562-98d5-18a40c6f7959"
      },
      "source": [
        "#download o arquivo localizado no reposítório do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/testset-levela.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "testset_a = pd.read_csv('/content/testset-levela.tsv', sep='\\t',encoding= 'utf-8', index_col = 'id')\n",
        "\n",
        "#conversão da coluna 'id' de inteiro para string\n",
        "# testset_a['id'] = testset_a['id'].astype('str')\n",
        "#verificação e remoção de duplicatas\n",
        "# print(testset_a.duplicated(['tweet']).sum())\n",
        "# if testset_a.duplicated(['tweet']).sum()>0:\n",
        "#   testset_a.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "# print('TWEETS DUPLICADOS: ',testset_a.duplicated(['tweet']).sum())\n",
        "# testset_a = testset_a[['subtask_c','subtask_b','subtask_a','id','tweet']]\n",
        "testset_a.head()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  130k  100  130k    0     0   655k      0 --:--:-- --:--:-- --:--:--  655k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15923</th>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27014</th>\n",
              "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30530</th>\n",
              "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13876</th>\n",
              "      <td>#Watching #Boomer getting the news that she is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60133</th>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet\n",
              "id                                                      \n",
              "15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
              "27014  #ConstitutionDay is revered by Conservatives, ...\n",
              "30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...\n",
              "13876  #Watching #Boomer getting the news that she is...\n",
              "60133  #NoPasaran: Unity demo to oppose the far-right..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ab-NJFKRO91-"
      },
      "source": [
        "#### testset_b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KYagkzX3O91_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "a7b74e9a-149b-4576-986a-b0f2e15bb329"
      },
      "source": [
        "#download o arquivo localizado no reposítório do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/testset-levelb.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "testset_b = pd.read_csv('/content/testset-levelb.tsv', sep='\\t',encoding= 'utf-8', index_col = 'id')\n",
        "# print(testset_b.duplicated(['tweet']).sum())\n",
        "# if testset_b.duplicated(['tweet']).sum()>0:\n",
        "#   testset_b.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "# print('TWEETS DUPLICADOS: ',testset_b.duplicated(['tweet']).sum())\n",
        "testset_b.head()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 35447  100 35447    0     0   103k      0 --:--:-- --:--:-- --:--:--  104k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15923</th>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60133</th>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83681</th>\n",
              "      <td>. . . What the fuck did he do this time?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65507</th>\n",
              "      <td>@USER Do you get the feeling he is kissing @US...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12588</th>\n",
              "      <td>@USER Nigga ware da hits at</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet\n",
              "id                                                      \n",
              "15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
              "60133  #NoPasaran: Unity demo to oppose the far-right...\n",
              "83681           . . . What the fuck did he do this time?\n",
              "65507  @USER Do you get the feeling he is kissing @US...\n",
              "12588                        @USER Nigga ware da hits at"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOCN6gf8r6-I",
        "colab_type": "text"
      },
      "source": [
        "### Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKjvjKEJFo6",
        "colab_type": "text"
      },
      "source": [
        "#### Funções de preprocessamento e Limpeza\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcNhWJl3u24d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tratamento básico\n",
        "def tratamento_texto(text):\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  #remove as menções a usuários de cada text\n",
        "  #remove as palavras url\n",
        "  text = re.sub('url$', '', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^n$', 'and', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^u$', 'you', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^r$', 'are', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'^sh*t$', 'shit', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'&amp;', '', text, flags=re.MULTILINE)\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  tokens = []\n",
        "  for token in doc:\n",
        "      if token.lemma_ != \"-PRON-\" :\n",
        "        if not token.is_stop:\n",
        "          if not token.is_punct :\n",
        "            tokens.append(token.lemma_)\n",
        "  \n",
        "  text =  \" \".join([token for token in tokens]).strip()  \n",
        "\n",
        "  \n",
        "  return text"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I30PdXZfoKlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0cd835b5-6bd4-47ee-c18d-0d51fa7ded6d"
      },
      "source": [
        "tratamento_texto(tweets.tweet.iloc[0])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'@user ask native americans'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHPEqgHXi5hd",
        "colab_type": "text"
      },
      "source": [
        "#### Funções para criação de features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wirMv6NLwgtc",
        "colab_type": "text"
      },
      "source": [
        "##### Features intrínsicas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-qnjF7ZizUX",
        "colab_type": "text"
      },
      "source": [
        "##### Comprimento do tweet % (contagem de caracteres/comprimento máximo de um tweet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGYzZ_eBixeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9fcd71a9-ca81-4a44-af9a-76d21326463f"
      },
      "source": [
        "def get_tweet_length(text):\n",
        "    return len(text)/240.0\n",
        "tweets.tweet.apply(get_tweet_length).head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.295833\n",
              "90194    0.279167\n",
              "16820    0.758333\n",
              "62688    0.270833\n",
              "43605    0.300000\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7I8N6Aj9oknZ"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brr6sOHPQCAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['comprimento_tweet'] = tweets.tweet.apply(get_tweet_length)\n",
        "tweets.hist(column = 'comprimento_tweet', by= \"subtask_a\")\n",
        "tweets.hist(column = 'comprimento_tweet', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rSOG6JlKA4JB"
      },
      "source": [
        "##### Contagem de palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5eO9xKUwA4JH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "000986dc-8ef6-4e87-bdf3-a7492a90ea98"
      },
      "source": [
        "def get_word_count(text):\n",
        "  return len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_word_count).head()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    14\n",
              "90194    11\n",
              "16820    27\n",
              "62688    11\n",
              "43605    12\n",
              "Name: tweet, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51lTgeW1oYR0",
        "colab_type": "text"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ2sS3sIQwjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['contagem_palavras'] = tweets.tweet.apply(get_word_count)\n",
        "tweets.hist(column = 'contagem_palavras', by= \"subtask_a\")\n",
        "tweets.hist(column = 'contagem_palavras', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1NCQy-xBBLSq"
      },
      "source": [
        "##### Comprimento médio das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JoF8F4kfBLSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c602bd48-bea4-453c-f27a-408c18e6ee26"
      },
      "source": [
        "def get_avg_word_len(text):\n",
        "  words = text.split()\n",
        "  word_len = 0\n",
        "  for word in words:\n",
        "    word_len = word_len + len(word)\n",
        "  return word_len/len(words)\n",
        "\n",
        "tweets.tweet.apply(get_avg_word_len).head()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    4.142857\n",
              "90194    5.181818\n",
              "16820    5.777778\n",
              "62688    5.000000\n",
              "43605    5.083333\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cm4A3iMJoji_"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VStr92wgRNtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['avg_len'] = tweets.tweet.apply(get_avg_word_len)\n",
        "tweets.hist(column = 'avg_len', by= \"subtask_a\")\n",
        "tweets.hist(column = 'avg_len', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8WBEOKo0Cfla"
      },
      "source": [
        "##### Contagem de stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K92ZzPj6Cflc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0c00239e-04f1-4f72-f5fd-f3a00e3a2403"
      },
      "source": [
        "def get_stop_words_percent(text):\n",
        "  return len([t for t in text.split() if t in STOP_WORDS])/len(text.split())\n",
        "\n",
        "\n",
        "tweets.tweet.apply(get_stop_words_percent).head()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.571429\n",
              "90194    0.000000\n",
              "16820    0.296296\n",
              "62688    0.363636\n",
              "43605    0.250000\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0ZdGEizokw2"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THQ3PvhSRTr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['sw_count'] = tweets.tweet.apply(get_stop_words_percent)\n",
        "tweets.hist(column = 'sw_count', by= \"subtask_a\")\n",
        "tweets.hist(column = 'sw_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UHDd9bMuFQz6"
      },
      "source": [
        "##### Percentual de #hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wPkRCVrMFQz8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "10d442b1-ed9c-4147-db85-dca59f206071"
      },
      "source": [
        "def get_hashtag_percent(text):\n",
        "  return len([t for t in text.split() if t.startswith(\"#\")])/len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_hashtag_percent).head()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.000000\n",
              "90194    0.181818\n",
              "16820    0.185185\n",
              "62688    0.000000\n",
              "43605    0.000000\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcbinyrLp0W5",
        "colab_type": "text"
      },
      "source": [
        "###### Histogramas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE88MrhNRY1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['hashtag_count'] = tweets.tweet.apply(get_hashtag_percent)\n",
        "tweets[tweets['hashtag_count'] > 0].hist(column = 'hashtag_count', by= \"subtask_a\")\n",
        "tweets[tweets['hashtag_count'] > 0].hist(column = 'hashtag_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kw7Tchg4Filo"
      },
      "source": [
        "##### Contagem de @menções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uXzj_UkrFilp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "23b4a9c4-bb58-49da-ef8f-6191c21aef7e"
      },
      "source": [
        "def get_mention_percent(text):\n",
        "  return len([t for t in text.split() if t.startswith(\"@\")])/len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_mention_percent).head()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.071429\n",
              "90194    0.272727\n",
              "16820    0.000000\n",
              "62688    0.090909\n",
              "43605    0.166667\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRbt_9lnourv"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M--KengRRmH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['mention_count'] = tweets.tweet.apply(get_mention_percent)\n",
        "tweets[tweets['mention_count'] > 0].hist(column = 'mention_count', by= \"subtask_a\")\n",
        "tweets[tweets['mention_count'] > 0].hist(column = 'mention_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vN1XcdmAJEAS"
      },
      "source": [
        "##### Contagem de palavras em MAIÚSCULO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8O3HOLHLJEAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7121ce7f-835d-4dd5-8d24-cbba1b739d3b"
      },
      "source": [
        "def get_uppercase_percent(text):\n",
        "  return len([t for t in text.split() if t.isupper()])/len(text.split())\n",
        "\n",
        "tweets.tweet.apply(get_uppercase_percent).head()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0.071429\n",
              "90194    0.454545\n",
              "16820    0.185185\n",
              "62688    0.090909\n",
              "43605    0.166667\n",
              "Name: tweet, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCN3m1b7ougd"
      },
      "source": [
        "###### Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FU5WTp0Ryi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['upper_count'] = tweets.tweet.apply(get_uppercase_percent)\n",
        "tweets.hist(column = 'upper_count', by= \"subtask_a\")\n",
        "tweets.hist(column = 'upper_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pbgQzWVYJzK8"
      },
      "source": [
        "##### Verificar se o tweet contém URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L-ZWmeH3JzK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "bd319fcc-8a7a-4044-da7c-9ef2d977f834"
      },
      "source": [
        "def get_contain_url(text):\n",
        "\n",
        "  return int('URL' in text )\n",
        "\n",
        "tweets.tweet.apply(get_contain_url).head()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    0\n",
              "90194    1\n",
              "16820    1\n",
              "62688    0\n",
              "43605    0\n",
              "Name: tweet, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3erhQ6fYv3oC",
        "colab_type": "text"
      },
      "source": [
        "##### Contagem de sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i5LQJapv3H3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d76ba91c-c571-4432-f8de-3bcabfa1cb25"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "def sent_count(text):\n",
        "  return len(sent_tokenize(text))\n",
        "tweets.tweet.apply(sent_count).head()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "86426    1\n",
              "90194    2\n",
              "16820    2\n",
              "62688    2\n",
              "43605    1\n",
              "Name: tweet, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEfgrogNxi4Z",
        "colab_type": "text"
      },
      "source": [
        "###### Histogramas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsK8qpmWwvra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['sent_count'] = tweets.tweet.apply(sent_count)\n",
        "tweets.hist(column = 'sent_count', by= \"subtask_a\")\n",
        "tweets.hist(column = 'sent_count', by= \"subtask_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-yecoLvukCoc"
      },
      "source": [
        "##### Lista de tipos de entidades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64IvzaLzMvtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_entities(text):\n",
        "  doc = nlp(text)\n",
        "  lista_entities = \"\"\n",
        "  if len(doc.ents) > 0:\n",
        "    for token in doc:\n",
        "      if len(token.ent_type_) > 0:\n",
        "        lista_entities = lista_entities  + \" \" + token.ent_type_\n",
        "  \n",
        "  return lista_entities\n",
        "\n",
        "\n"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_es-Vl-9w0D7"
      },
      "source": [
        "##### Features de bases de conhecimento externas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R8-XrfBqX8ZH"
      },
      "source": [
        "###### Classificar como positivo ou negativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLMBCWaXBACA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classificar_positivo_negativo(text):\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    return doc.cats\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nvZIIzAjF_R4"
      },
      "source": [
        "###### Anexar embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2CaCbnGC-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embeddings(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  return doc.vector\n"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJkSowVecxR",
        "colab_type": "text"
      },
      "source": [
        "###### Empath"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a2ZmTRjsv0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from empath import Empath\n",
        "lexicon = Empath()\n",
        "\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjNRT4FOm6Qg",
        "colab_type": "text"
      },
      "source": [
        "###### Criação das features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VettZGZKm4fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class Criar_Features(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \n",
        "\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "\n",
        "        features = pd.DataFrame(X)\n",
        "        #Comprimento do tweet/240.0\n",
        "        features['length'] = features['tweet'].apply(get_tweet_length)\n",
        "        #Hashtag por tweet (%)\n",
        "        features['hashtag_%'] = features['tweet'].apply(get_hashtag_percent)\n",
        "        #Menções por tweet (%)\n",
        "        features['mentions_%'] = features['tweet'].apply(get_mention_percent)\n",
        "        #Stop words (%)\n",
        "        features['stop_words_%'] = features['tweet'].apply(get_stop_words_percent)\n",
        "        #upper case (%)\n",
        "        features['uppercase_%'] =  features['tweet'].apply(get_uppercase_percent)\n",
        "        #contagem de sentenças\n",
        "        features['sent_count'] = features['tweet'].apply(sent_count)\n",
        "        #contém url\n",
        "        features['has_url'] = features['tweet'].apply(get_contain_url)\n",
        "        \n",
        "        features['tweet'] = features['tweet'].apply(tratamento_texto)\n",
        "               \n",
        "\n",
        "        return features\n",
        "\n",
        "# criar_features = Criar_Features()\n",
        "# print(criar_features.fit_transform(tweets.tweet))"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4y1CEwxm9cya",
        "colab": {}
      },
      "source": [
        "#@title Aumento das features de treino e teste { display-mode: \"form\" }\n",
        "\n",
        "criar_trainset = True #@param {type:\"boolean\"}\n",
        "criar_testset_a = True #@param {type:\"boolean\"}\n",
        "criar_testset_b = True #@param {type:\"boolean\"}\n",
        "\n",
        "if criar_trainset:\n",
        "  trainset_aumentado = Criar_Features().fit_transform(tweets)\n",
        "  trainset_aumentado.to_csv('/content/trainset_aumentado.txt', sep=' ')\n",
        "\n",
        "if criar_testset_a:\n",
        "  testset_a_aumentado = Criar_Features().fit_transform(testset_a)\n",
        "  testset_a_aumentado.to_csv('/content/testset_a_aumentado.txt', sep=' ')\n",
        "\n",
        "\n",
        "if criar_testset_b:\n",
        "  testset_b_aumentado = Criar_Features().fit_transform(testset_b) \n",
        "  testset_b_aumentado.to_csv('/content/testset_b_aumentado.txt', sep=' ')\n",
        "\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9EF1TAWDKRph",
        "colab": {}
      },
      "source": [
        "#@title Busca dos vetores de entidades, positivo e negativo, e vetores GloVe { display-mode: \"form\" }\n",
        "criar_trainset = True #@param {type:\"boolean\"}\n",
        "criar_testset_a = True #@param {type:\"boolean\"}\n",
        "criar_testset_b = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "vetores_trainset = pd.DataFrame()\n",
        "vetores_testset_a = pd.DataFrame()\n",
        "vetores_testset_b = pd.DataFrame()\n",
        "def buscar_vetores(dataset_aumentado):\n",
        "  \n",
        "  #Extrai entidades\n",
        "  entidades = dataset_aumentado['tweet'].apply(extract_entities)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #classifica como positivo ou negativo\n",
        "  pos_neg = dataset_aumentado['tweet'].apply(classificar_positivo_negativo).apply(pd.Series)\n",
        "  #busca vetores glove          \n",
        "  vetor_glove = dataset_aumentado['tweet'].apply(get_embeddings).apply(pd.Series)\n",
        "  df = pd.concat([entidades, pos_neg, vetor_glove],axis = 1)\n",
        "      \n",
        "  return df\n",
        "\n",
        "for tamanho in [\"pequeno\", \"medio\", \"grande\"]:\n",
        "  \n",
        "  if tamanho == \"grande\":\n",
        "    nlp = en_core_web_lg.load()\n",
        "\n",
        "  if tamanho == \"medio\":\n",
        "    nlp = en_core_web_md.load()\n",
        "\n",
        "  if tamanho == \"pequeno\":\n",
        "    nlp = en_core_web_sm.load()\n",
        "\n",
        "  textcat = nlp.create_pipe(\"textcat\")\n",
        "  textcat.add_label(\"POSITIVE\")\n",
        "  textcat.add_label(\"NEGATIVE\")\n",
        "  \n",
        "  nlp.add_pipe(textcat, last=True)\n",
        "  \n",
        "  nlp.begin_training()\n",
        "\n",
        "  if criar_trainset:\n",
        "    vetores_trainset = buscar_vetores(trainset_aumentado)\n",
        "    vetores_trainset['entities'] = vetores_trainset['tweet']\n",
        "    vetores_trainset.drop(\"tweet\", inplace= True, axis=1)\n",
        "\n",
        "    vetores_trainset.to_csv('/content/vetores_trainset_aumentado_'+ tamanho + '.txt', sep=' ')\n",
        "\n",
        "  if criar_testset_a:\n",
        "    vetores_testset_a = buscar_vetores(testset_a_aumentado)\n",
        "    vetores_testset_a['entities'] = vetores_testset_a['tweet']\n",
        "    vetores_testset_a.drop(\"tweet\", inplace= True, axis=1)\n",
        "    vetores_testset_a.to_csv('/content/vetores_testset_a_aumentado_'+ tamanho + '.txt', sep=' ')\n",
        "\n",
        "\n",
        "  if criar_testset_b:\n",
        "    vetores_testset_b = buscar_vetores(testset_b_aumentado)\n",
        "    vetores_testset_b['entities'] = vetores_testset_b['tweet']\n",
        "    vetores_testset_b.drop(\"tweet\", inplace= True, axis=1)\n",
        "    vetores_testset_b.to_csv('/content/vetores_testset_b_aumentado_'+ tamanho + '.txt', sep=' ')\n",
        "\n",
        "  nlp.remove_pipe('textcat')"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "An5iJWSbkzBq"
      },
      "source": [
        "### Pipelines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76i1-Vp5fS-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "cellView": "form",
        "outputId": "d8b89c42-fc67-4532-99b0-ec4aa4b73b43"
      },
      "source": [
        "#@title Download dos datasets e vetores pré-configurados\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/trainset_aumentado.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/testset_a_aumentado.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/testset_b_aumentado.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_trainset_aumentado_grande.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_trainset_aumentado_medio.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_trainset_aumentado_pequeno.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_a_aumentado_grande.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_a_aumentado_medio.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_a_aumentado_pequeno.txt\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_b_aumentado_grande.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_b_aumentado_medio.txt\n",
        "\n",
        "\n",
        "\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/datasets_aumentados/vetores_testset_b_aumentado_pequeno.txt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1814k  100 1814k    0     0  4986k      0 --:--:-- --:--:-- --:--:-- 4986k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  143k  100  143k    0     0   751k      0 --:--:-- --:--:-- --:--:--  751k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 39213  100 39213    0     0   181k      0 --:--:-- --:--:-- --:--:--  181k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 44.1M  100 44.1M    0     0  35.2M      0  0:00:01  0:00:01 --:--:-- 35.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 44.4M  100 44.4M    0     0  38.4M      0  0:00:01  0:00:01 --:--:-- 38.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13.8M  100 13.8M    0     0  16.6M      0 --:--:-- --:--:-- --:--:-- 16.6M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2994k  100 2994k    0     0  6147k      0 --:--:-- --:--:-- --:--:-- 6135k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2999k  100 2999k    0     0  6491k      0 --:--:-- --:--:-- --:--:-- 6491k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  927k  100  927k    0     0  1679k      0 --:--:-- --:--:-- --:--:-- 1676k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  833k  100  833k    0     0  2479k      0 --:--:-- --:--:-- --:--:-- 2479k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  834k  100  834k    0     0  2878k      0 --:--:-- --:--:-- --:--:-- 2878k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  258k  100  258k    0     0  1324k      0 --:--:-- --:--:-- --:--:-- 1324k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w787-LvAbd7D",
        "colab": {}
      },
      "source": [
        "#@title Leitura dos datasets aumentados e respectivos vetores { display-mode: \"form\" }\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#pasta raiz dos arquivos\n",
        "pasta_raiz = '/content'\n",
        "\n",
        "trainset_aumentado = pd.read_csv(pasta_raiz+'/trainset_aumentado.txt', sep=' ', index_col='id')\n",
        "\n",
        "testset_a_aumentado = pd.read_csv(pasta_raiz+'/testset_a_aumentado.txt', sep=' ', index_col='id')\n",
        "\n",
        "testset_b_aumentado = pd.read_csv(pasta_raiz+'/testset_b_aumentado.txt', sep=' ', index_col='id')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vetores_trainset_aumentado_grande = pd.read_csv('/content/vetores_trainset_aumentado_grande.txt', sep=' ', index_col = 'id')\n",
        "vetores_trainset_aumentado_grande.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_trainset_aumentado_medio = pd.read_csv('/content/vetores_trainset_aumentado_medio.txt', sep=' ', index_col = 'id')\n",
        "vetores_trainset_aumentado_medio.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_trainset_aumentado_pequeno = pd.read_csv('/content/vetores_trainset_aumentado_pequeno.txt', sep=' ', index_col = 'id')\n",
        "vetores_trainset_aumentado_pequeno.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_a_aumentado_grande = pd.read_csv('vetores_testset_a_aumentado_grande.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_a_aumentado_grande.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_a_aumentado_medio = pd.read_csv('vetores_testset_a_aumentado_medio.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_a_aumentado_medio.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_a_aumentado_pequeno = pd.read_csv('vetores_testset_a_aumentado_pequeno.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_a_aumentado_pequeno.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_b_aumentado_grande = pd.read_csv('vetores_testset_b_aumentado_grande.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_b_aumentado_grande.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_b_aumentado_medio = pd.read_csv('vetores_testset_b_aumentado_medio.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_b_aumentado_medio.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "vetores_testset_b_aumentado_pequeno = pd.read_csv('vetores_testset_b_aumentado_pequeno.txt', sep=' ', index_col = 'id')\n",
        "vetores_testset_b_aumentado_pequeno.entities.fillna(\"\",inplace=True)\n",
        "\n",
        "dict_vetores = {'pequeno':[vetores_trainset_aumentado_pequeno, vetores_testset_a_aumentado_pequeno, vetores_testset_b_aumentado_pequeno],\n",
        "                'medio':[vetores_trainset_aumentado_medio, vetores_testset_a_aumentado_medio, vetores_testset_b_aumentado_medio],\n",
        "                'grande':[vetores_trainset_aumentado_grande, vetores_testset_a_aumentado_grande, vetores_testset_b_aumentado_grande]\n",
        "}\n",
        "\n",
        "VOCABULARIO_TWEETS = TfidfVectorizer().fit(pd.concat([testset_a_aumentado['tweet'], \n",
        "                                                      testset_b_aumentado['tweet'], \n",
        "                                                      trainset_aumentado['tweet']])).vocabulary_\n",
        "VOCABULARIO_ENTIDADES = TfidfVectorizer().fit(pd.concat([vetores_testset_a_aumentado_grande['entities'], \n",
        "                                                         vetores_testset_b_aumentado_grande['entities'], \n",
        "                                                         vetores_trainset_aumentado_grande['entities']])).vocabulary_"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKZNUasoBtQf",
        "colab_type": "text"
      },
      "source": [
        "#### Funções auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgUx-uoDk8Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names= []):\n",
        "        self.attribute_names = attribute_names\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        if len(self.attribute_names) == 1:\n",
        "          print(\"nome do atributo: \", self.attribute_names)\n",
        "          print(X.head())          \n",
        "          return X.loc[:,self.attribute_names[0]]\n",
        "        return X[self.attribute_names]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XM3ClBxIGOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class Vetorizador_tfidf(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,               \n",
        "                 use_idf = False, ngram_range = (1,1), \n",
        "                 tokenizer = TweetTokenizer().tokenize):\n",
        "      \n",
        "        self.use_idf\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(use_idf = use_idf, ngram_range=ngram_range, tokenizer=tokenizer)\n",
        "    \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        return self.tfidf_vectorizer.fit(pd.concat([testset_a_aumentado['tweet'], testset_b_aumentado['tweet'], trainset_aumentado['tweet']]))\n",
        "    def transform(self, X):\n",
        "        \n",
        "        return self.tfidf_vectorizer.transform(X)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a32Ac04vuTZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Anexar_Vetores(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, tamanho = \"\"):\n",
        "        self.tamanho = tamanho\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "      \n",
        "      for vetor in dict_vetores[self.tamanho]:\n",
        "\n",
        "        \n",
        "\n",
        "        if X.index.isin(vetor.index).all():\n",
        "          return X.join(vetor.loc[X.index.values])\n",
        "  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ub6dRZ_Pe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "def pipelinizar(function, active=True):\n",
        "    def list_comprehend_a_function(list_or_series, active=True):\n",
        "        if active:\n",
        "            return [function(i) for i in list_or_series]\n",
        "        else: # if it's not active, just pass it right back\n",
        "            return list_or_series\n",
        "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ttAsDinw4oY6"
      },
      "source": [
        "#### Pipeline vetorização do texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqZ9z68G4rHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9db5379-1a51-4b2b-d992-8b2e2c4752c3"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "vetorizar_tweets = ColumnTransformer(\n",
        "        [('vetorizar' ,  TfidfVectorizer(vocabulary=VOCABULARIO_TWEETS),\"tweet\")])\n",
        "\n",
        "\n",
        "vetorizar_tweets.fit_transform(trainset_aumentado.join(dict_vetores['grande'][0])).toarray().shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13207, 16678)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDTo2hNPk3vp",
        "colab_type": "text"
      },
      "source": [
        "#### Pipeline vetor de entidades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JJW9j-llUOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "066a181b-546a-4516-f8c6-e7f88b0c1250"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "anexar_entidades = ColumnTransformer(\n",
        "        [('vetorizar' ,  TfidfVectorizer(vocabulary=VOCABULARIO_ENTIDADES),\"entities\")])\n",
        "\n",
        "anexar_entidades.fit_transform(trainset_aumentado.join(dict_vetores['grande'][0])).toarray().shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13207, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3JUiqit-wlIM"
      },
      "source": [
        "#### Pipeline features numéricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkubnGDtwrZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Selecionar_Features_Numericas(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names= []):\n",
        "        self.attribute_names = attribute_names\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "\n",
        "        return X[X.dtypes[X.dtypes == 'float64'].index].values\n",
        "\n",
        "features_numericas = Pipeline(steps= [('features_numericas', Selecionar_Features_Numericas())])\n",
        "# features_numericas.fit_transform(X_train)        "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cNwAicP-117Q"
      },
      "source": [
        "### União das features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6lGPKT514op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "\n",
        "uniao_features = FeatureUnion([(\"vetores_entidades\", ColumnTransformer([\n",
        "                                                                        ('vetorizar',\n",
        "                                                                         TfidfVectorizer(vocabulary= VOCABULARIO_ENTIDADES),\n",
        "                                                                         \"entities\")])), #vetorizar a coluna entidades\n",
        "                               (\"features_numericas\", Selecionar_Features_Numericas()),   #concatenar todas as features numéricas sem alteração\n",
        "                               (\"vetorizar_tweets\", ColumnTransformer([\n",
        "                                                                       ('vetorizar' ,  \n",
        "                                                                        TfidfVectorizer(vocabulary= VOCABULARIO_TWEETS, tokenizer= TweetTokenizer().tokenize),\n",
        "                                                                        \"tweet\")]))])   #vetorizar os tweets\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2WlCftBGa5l",
        "colab_type": "text"
      },
      "source": [
        "### Subtask A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFiF8D1J0UOq",
        "colab_type": "text"
      },
      "source": [
        "##### Histograma dos targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j61p6Ll8BVE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "86df222e-09f3-4fe2-c228-53731644a949"
      },
      "source": [
        "trainset_aumentado.subtask_a.fillna(\"NaN\").hist()\n",
        "# tweets.subtask_a.hist()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0d47556c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVFlc7BuA1ji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9156632b-61d1-4b74-9905-df5485d1ae5d"
      },
      "source": [
        "trainset_aumentado.subtask_b.fillna(\"NaN\").hist()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0d474a2320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He8JVu2ZKbgz",
        "colab_type": "text"
      },
      "source": [
        "#### Segregação de dados de treino e de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QU1Vnsk9rqa",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "36a51872-7244-4249-c4bf-f2e883223602"
      },
      "source": [
        "#@title Seleção de parâmetros da separação de dados de teste e treino\n",
        "test_size = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "target = \"subtask_b\" #@param [\"subtask_a\", \"subtask_b\"]\n",
        "estratificar_alvo = True #@param {type:\"boolean\"}\n",
        "remover_nan = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = trainset_aumentado\n",
        "if remover_nan:\n",
        "  y = trainset_aumentado[target].dropna()\n",
        "  X = trainset_aumentado[trainset_aumentado[target] == trainset_aumentado[target]]\n",
        "else:\n",
        "  y = trainset_aumentado[target].fillna(\"NOT\")\n",
        "\n",
        "\n",
        "stratify = None \n",
        "if estratificar_alvo == True:\n",
        "  stratify = y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.drop(X.loc[:,X.columns.str.startswith('subtask')], axis=1), y, test_size=test_size, random_state= 42, stratify= stratify)\n",
        "\n",
        "print(\"Quantidade de orbservações de teste: \", y_test.count())\n",
        "print(\"Percentual por categoria\")\n",
        "print(y_test.value_counts()*100/y_test.count())\n",
        "print('-------------')\n",
        "print(\"Quantidade de orbservações de treino: \", y_train.count())\n",
        "print(\"Percentual por categoria\")\n",
        "print(y_train.value_counts()*100/y_train.count())\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quantidade de orbservações de teste:  879\n",
            "Percentual por categoria\n",
            "TIN    88.054608\n",
            "UNT    11.945392\n",
            "Name: subtask_b, dtype: float64\n",
            "-------------\n",
            "Quantidade de orbservações de treino:  3513\n",
            "Percentual por categoria\n",
            "TIN    88.101338\n",
            "UNT    11.898662\n",
            "Name: subtask_b, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHqmI7WrAAys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid_anexar_vetores = {'anexar_vetores__tamanho': ['pequeno', 'medio', 'grande']}\n",
        "\n",
        "param_grid_feature_union = {'uniao_features__vetorizar_tweets__vetorizar__ngram_range': [(1,1),(2,2),(1,2)],\n",
        "                            'uniao_features__vetorizar_tweets__vetorizar__use_idf': [True,False],\n",
        "                            'uniao_features__vetores_entidades__vetorizar__use_idf' : [True,False]                          \n",
        "                            }\n",
        "\n",
        "param_grid_reduc_dim = {}\n",
        "\n",
        "\n",
        "# xgb.XGBClassifier\n",
        "# param_grid_rand_for = {'classifier__n_estimators' : np.arange(60,600,10), \n",
        "#                      'classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "838eYGsxPvo5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "54a4fb5e-9c5a-4795-d833-ed007c55ec9b"
      },
      "source": [
        "RandomForestClassifier(n_estimators=[10,30,1000])."
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0,\n",
              "                       n_estimators=[10, 30, 1000], n_jobs=None,\n",
              "                       oob_score=False, random_state=None, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omD5r8ov9MFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "\n",
        "# dict_algoritmos = {'logistic_regression': LogisticRegression(),\n",
        "#                    'random_forest' : RandomForestClassifier(),\n",
        "#                    'svm': ,\n",
        "#                    'naive_bayes' : ,\n",
        "#                    'ada_boost': ,\n",
        "#                    'extra_trees': ,\n",
        "#                    'knn': , \n",
        "#                    'xgboost': xgb.,\n",
        "#                    'extra_trees' : \n",
        "#                    }"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Py3f1qVCUB",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Treinamento\n",
        "num_cv = 3 #@param {type:\"integer\"}\n",
        "algoritmo = \"logistic_regression\" #@param [\"logistic_regression\", \"random_forest\", \"svm\", \"naive_bayes\", \"ada_boost\", \"knn\", \"xgboost\", \"extra_trees\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2JCyctn_jcM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b6240953-ff33-4d51-a62d-c13dc54c867d"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "classifier = xgb.XGBClassifier(n_jobs= -1)\n",
        "param_grid_algoritmo = {}\n",
        "\n",
        "param_grid = {**param_grid_anexar_vetores, \n",
        "              **param_grid_feature_union, \n",
        "              **param_grid_reduc_dim, \n",
        "              **param_grid_algoritmo}\n",
        "\n",
        "# classifier.fit(features, y_train)\n",
        "classificador = Pipeline([(\"anexar_vetores\", Anexar_Vetores()),\n",
        "                          ('uniao_features', uniao_features),\n",
        "                          # ('reducao_dimensionalidade', TruncatedSVD(n_components = 10000)), PCA\n",
        "                          ('classifier',classifier)\n",
        "    ])\n",
        "\n",
        "\n",
        "gridsearchcv = GridSearchCV(classificador, param_grid, cv= 5, scoring='f1_weighted', n_jobs= -1,verbose= 10)\n",
        "gridsearchcv.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    9.4s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   17.8s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   40.0s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   59.3s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  7.7min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  9.5min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 11.2min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 13.1min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQ3TqKle-a9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "994c3a10-8d32-40a7-922c-79463fbd6d38"
      },
      "source": [
        "gridsearchcv.cv_results_ "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([19.08187881, 19.15802116, 14.10071087, 14.10986867, 19.33555369,\n",
              "        19.40805321, 20.26961889, 19.33995414, 14.02592287, 14.05102425,\n",
              "        19.29879088, 19.15421562, 20.49251266, 20.40827279, 16.70202713,\n",
              "        16.77006884, 20.38153324, 20.5152523 , 20.85512176, 20.83251057,\n",
              "        16.65829701, 16.74979086, 20.58227515, 20.40543151, 20.30255904,\n",
              "        20.22435145, 16.66097946, 16.65547729, 20.45203829, 20.23058534,\n",
              "        20.92165728, 20.89556479, 16.55530543, 16.43878946, 20.26512799,\n",
              "        19.91777444]),\n",
              " 'mean_score_time': array([0.40689406, 0.42711048, 0.4538209 , 0.42818785, 0.43517394,\n",
              "        0.45862193, 0.44558492, 0.45349646, 0.42578731, 0.41555767,\n",
              "        0.45988293, 0.54993567, 0.60236435, 0.56246505, 0.52924461,\n",
              "        0.57009015, 0.6145226 , 0.61640835, 0.55731316, 0.53884397,\n",
              "        0.54460969, 0.56521158, 0.62782531, 0.58430123, 0.51107039,\n",
              "        0.63865538, 0.53000069, 0.5739377 , 0.60000453, 0.63626823,\n",
              "        0.60286522, 0.55907426, 0.51510725, 0.59217763, 0.59759145,\n",
              "        0.48553171]),\n",
              " 'mean_test_score': array([0.5904363 , 0.58893883, 0.55080323, 0.54970222, 0.58856572,\n",
              "        0.58933721, 0.586567  , 0.59024017, 0.55003744, 0.5499543 ,\n",
              "        0.58729593, 0.5860241 , 0.68566845, 0.68565549, 0.68023062,\n",
              "        0.67940113, 0.68882973, 0.68993555, 0.69176115, 0.68927878,\n",
              "        0.68200173, 0.68649754, 0.68656971, 0.69049474, 0.68704811,\n",
              "        0.68570928, 0.67549655, 0.67659002, 0.68044946, 0.68011061,\n",
              "        0.68399252, 0.68183334, 0.67855612, 0.67742418, 0.68549291,\n",
              "        0.68034267]),\n",
              " 'param_anexar_vetores__tamanho': masked_array(data=['pequeno', 'pequeno', 'pequeno', 'pequeno', 'pequeno',\n",
              "                    'pequeno', 'pequeno', 'pequeno', 'pequeno', 'pequeno',\n",
              "                    'pequeno', 'pequeno', 'medio', 'medio', 'medio',\n",
              "                    'medio', 'medio', 'medio', 'medio', 'medio', 'medio',\n",
              "                    'medio', 'medio', 'medio', 'grande', 'grande',\n",
              "                    'grande', 'grande', 'grande', 'grande', 'grande',\n",
              "                    'grande', 'grande', 'grande', 'grande', 'grande'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_uniao_features__vetores_entidades__vetorizar__use_idf': masked_array(data=[True, True, True, True, True, True, False, False,\n",
              "                    False, False, False, False, True, True, True, True,\n",
              "                    True, True, False, False, False, False, False, False,\n",
              "                    True, True, True, True, True, True, False, False,\n",
              "                    False, False, False, False],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_uniao_features__vetorizar_tweets__vetorizar__ngram_range': masked_array(data=[(1, 1), (1, 1), (2, 2), (2, 2), (1, 2), (1, 2), (1, 1),\n",
              "                    (1, 1), (2, 2), (2, 2), (1, 2), (1, 2), (1, 1), (1, 1),\n",
              "                    (2, 2), (2, 2), (1, 2), (1, 2), (1, 1), (1, 1), (2, 2),\n",
              "                    (2, 2), (1, 2), (1, 2), (1, 1), (1, 1), (2, 2), (2, 2),\n",
              "                    (1, 2), (1, 2), (1, 1), (1, 1), (2, 2), (2, 2), (1, 2),\n",
              "                    (1, 2)],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_uniao_features__vetorizar_tweets__vetorizar__use_idf': masked_array(data=[True, False, True, False, True, False, True, False,\n",
              "                    True, False, True, False, True, False, True, False,\n",
              "                    True, False, True, False, True, False, True, False,\n",
              "                    True, False, True, False, True, False, True, False,\n",
              "                    True, False, True, False],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'pequeno',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'medio',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': True,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (2, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': True},\n",
              "  {'anexar_vetores__tamanho': 'grande',\n",
              "   'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 2),\n",
              "   'uniao_features__vetorizar_tweets__vetorizar__use_idf': False}],\n",
              " 'rank_test_score': array([25, 28, 33, 36, 29, 27, 31, 26, 34, 35, 30, 32, 10, 11, 18, 20,  5,\n",
              "         3,  1,  4, 14,  8,  7,  2,  6,  9, 24, 23, 16, 19, 13, 15, 21, 22,\n",
              "        12, 17], dtype=int32),\n",
              " 'split0_test_score': array([0.58805626, 0.58575969, 0.54396698, 0.54655808, 0.58354064,\n",
              "        0.58009487, 0.57965758, 0.58103185, 0.54581816, 0.54711028,\n",
              "        0.58285965, 0.5761626 , 0.6791827 , 0.67470958, 0.6728473 ,\n",
              "        0.68059628, 0.67508424, 0.68981276, 0.68548951, 0.68319272,\n",
              "        0.68248337, 0.6829438 , 0.67875409, 0.68168016, 0.67508328,\n",
              "        0.68693194, 0.6829438 , 0.67620271, 0.66983731, 0.67275816,\n",
              "        0.67131597, 0.67694704, 0.66978067, 0.66183956, 0.68253619,\n",
              "        0.67355409]),\n",
              " 'split1_test_score': array([0.58364027, 0.58754328, 0.55970036, 0.55025706, 0.58783676,\n",
              "        0.58996367, 0.58518028, 0.59386908, 0.55358869, 0.54412959,\n",
              "        0.59032342, 0.58120078, 0.69573394, 0.6932172 , 0.68299677,\n",
              "        0.69015509, 0.7009665 , 0.69715187, 0.69528337, 0.69019633,\n",
              "        0.68011107, 0.69348047, 0.69519732, 0.7009919 , 0.69668606,\n",
              "        0.69392643, 0.68417371, 0.68430238, 0.69419651, 0.6871795 ,\n",
              "        0.68887324, 0.694244  , 0.68943055, 0.69068836, 0.70135564,\n",
              "        0.68679851]),\n",
              " 'split2_test_score': array([0.59679446, 0.58652783, 0.54665724, 0.54819215, 0.59143698,\n",
              "        0.59709416, 0.58759462, 0.58623897, 0.55280987, 0.55951291,\n",
              "        0.5889343 , 0.58469162, 0.6851104 , 0.68579178, 0.67842248,\n",
              "        0.68470766, 0.69679463, 0.68912261, 0.69038723, 0.694244  ,\n",
              "        0.68624819, 0.6871795 , 0.68105281, 0.68513164, 0.68797827,\n",
              "        0.68551058, 0.67656398, 0.67882089, 0.67017123, 0.68142917,\n",
              "        0.69241197, 0.68107083, 0.67396577, 0.68776748, 0.67957086,\n",
              "        0.68633637]),\n",
              " 'split3_test_score': array([0.58903043, 0.59252419, 0.55393517, 0.55266432, 0.59152116,\n",
              "        0.58462835, 0.58549754, 0.60185883, 0.55266432, 0.55420078,\n",
              "        0.5897479 , 0.59470563, 0.68703488, 0.68520269, 0.68153143,\n",
              "        0.6715713 , 0.68741568, 0.68371723, 0.69328589, 0.69073516,\n",
              "        0.68665424, 0.68013225, 0.69470073, 0.68783145, 0.6882124 ,\n",
              "        0.68052469, 0.66691839, 0.67454256, 0.67865416, 0.6779052 ,\n",
              "        0.68419988, 0.67939103, 0.67343166, 0.66966894, 0.68236519,\n",
              "        0.67899274]),\n",
              " 'split4_test_score': array([0.59466006, 0.59233918, 0.54975638, 0.55083948, 0.58849304,\n",
              "        0.59490499, 0.59490499, 0.58820212, 0.54530614, 0.54481795,\n",
              "        0.5846144 , 0.59335988, 0.68128031, 0.68935619, 0.68535514,\n",
              "        0.6699753 , 0.68388759, 0.68987328, 0.69435974, 0.6880257 ,\n",
              "        0.67451176, 0.68875166, 0.68314358, 0.69683853, 0.68728055,\n",
              "        0.68165275, 0.66688286, 0.66908154, 0.68938811, 0.68128101,\n",
              "        0.68316155, 0.6775138 , 0.68617194, 0.67715654, 0.68163669,\n",
              "        0.67603165]),\n",
              " 'std_fit_time': array([0.32888346, 0.29308809, 0.19776137, 0.24701607, 0.54412914,\n",
              "        0.4745137 , 1.52841422, 0.18312374, 0.26528959, 0.33227313,\n",
              "        0.32152599, 0.44890568, 0.7166357 , 0.36643882, 0.25031702,\n",
              "        0.3931073 , 0.51054257, 0.5580484 , 1.25944754, 1.37584761,\n",
              "        0.47537242, 0.26335182, 0.42585967, 0.4866738 , 0.5047255 ,\n",
              "        0.5341201 , 0.5969081 , 0.401719  , 0.49738587, 0.57840714,\n",
              "        1.86449545, 1.76063827, 0.60609074, 0.48864738, 0.5339636 ,\n",
              "        0.93555951]),\n",
              " 'std_score_time': array([0.06534257, 0.04135208, 0.00570875, 0.02054414, 0.02551534,\n",
              "        0.05118465, 0.05910256, 0.04353866, 0.01760687, 0.01857299,\n",
              "        0.02685105, 0.05344484, 0.05299759, 0.06533551, 0.10546844,\n",
              "        0.05867546, 0.03388804, 0.04358708, 0.07436912, 0.08300842,\n",
              "        0.0715397 , 0.06001807, 0.08020632, 0.07122244, 0.06925017,\n",
              "        0.02592931, 0.07149952, 0.06240868, 0.09597795, 0.03612998,\n",
              "        0.05189631, 0.07021525, 0.08141534, 0.04124926, 0.09809781,\n",
              "        0.0784839 ]),\n",
              " 'std_test_score': array([0.00473484, 0.00290808, 0.00554955, 0.00212326, 0.00292462,\n",
              "        0.00631121, 0.00492717, 0.00711504, 0.00367112, 0.00596318,\n",
              "        0.0029911 , 0.00709185, 0.00574131, 0.00617918, 0.00432174,\n",
              "        0.00768619, 0.00923151, 0.00427604, 0.00354132, 0.00363942,\n",
              "        0.00446284, 0.0046358 , 0.00698295, 0.00726618, 0.00690394,\n",
              "        0.00474113, 0.00747897, 0.00500353, 0.00990317, 0.00473286,\n",
              "        0.00715733, 0.00637346, 0.00775363, 0.01082606, 0.00800115,\n",
              "        0.00536832])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnfZiorW3wLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "67b92a44-25a3-4d20-863e-762ba2012511"
      },
      "source": [
        "gridsearchcv.best_params_"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anexar_vetores__tamanho': 'medio',\n",
              " 'uniao_features__vetores_entidades__vetorizar__use_idf': False,\n",
              " 'uniao_features__vetorizar_tweets__vetorizar__ngram_range': (1, 1),\n",
              " 'uniao_features__vetorizar_tweets__vetorizar__use_idf': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypFQpczQZSo3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6f4f6c7f-6ea8-4d0e-c00b-a880a32e3cde"
      },
      "source": [
        "y_pred = gridsearchcv.predict(X_test)\n",
        "print(\"F1 Score\")\n",
        "print(f1_score(y_test, y_pred, average='weighted'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score\n",
            "0.6851896167237592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfODskRULZwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2a53e443-bd41-4c1a-eba3-6d5d5630d238"
      },
      "source": [
        "pd.Series(y_pred).value_counts()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NOT    2332\n",
              "OFF     310\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CyaC2v4LPGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b1c4b4a-7ada-4a28-ce59-09ec59726163"
      },
      "source": [
        "t = gridsearchcv.predict(testset_a_aumentado)\n",
        "pd.Series(t).value_counts()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NOT    775\n",
              "OFF     85\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx3Y7tj0zDP6",
        "colab_type": "text"
      },
      "source": [
        "##### Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf6P2BzMrcXX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "cd7ab58b-d284-4ff1-915b-1337081937f4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import nltk\n",
        "tokenizer = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True) \n",
        "\n",
        "count_vect = CountVectorizer(tokenizer=tokenizer.tokenize) \n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "pipeline = Pipeline([\n",
        "                    \n",
        "        ('uniao_features', uniao_features),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(\"F1 Score\")\n",
        "print(f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'entities' is not in list",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5c5fb0a74c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     ])\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 Score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0msum\u001b[0m \u001b[0mof\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_parallel_func\u001b[0;34m(self, X, y, fit_params, func)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                                     weight) in enumerate(transformers, 1))\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 raise ValueError(\n\u001b[1;32m    467\u001b[0m                     \u001b[0;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 ) from e\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEqtCs-uGhmE",
        "colab_type": "text"
      },
      "source": [
        "#### Algoritmos utilizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB8DWNizFko7",
        "colab_type": "text"
      },
      "source": [
        "#### Ajuste de hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgoyyk0Kn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsIpSfQrsZiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def genericize_mentions(text):\n",
        "    return re.sub(r'@[\\w_-]+', 'thisisanatmention', text)\n",
        "\n",
        "def get_tweet_length(text):\n",
        "    return len(text)\n",
        "\n",
        "def pipelinize(function, active=True):\n",
        "    def list_comprehend_a_function(list_or_series, active=True):\n",
        "        if active:\n",
        "            return [function(i) for i in list_or_series]\n",
        "        else: # if it's not active, just pass it right back\n",
        "            return list_or_series\n",
        "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})\n",
        "\n",
        "def reshape_a_feature_column(series):\n",
        "    return np.reshape(np.asarray(series), (len(series), 1))\n",
        "\n",
        "def pipelinize_feature(function, active=True):\n",
        "    def list_comprehend_a_function(list_or_series, active=True):\n",
        "        if active:\n",
        "            processed = [function(i) for i in list_or_series]\n",
        "            processed = reshape_a_feature_column(processed)\n",
        "            return processed\n",
        "#         This is incredibly stupid and hacky, but we need it to do a grid search.\n",
        "#         If a feature is deactivated, we're going to just return a column of zeroes.\n",
        "#         Zeroes shouldn't affect the regression, but other values may.\n",
        "#         If you really want brownie points, consider pulling out that feature column later in the pipeline.\n",
        "        else:\n",
        "            return reshape_a_feature_column(np.zeros(len(list_or_series)))\n",
        "\n",
        "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})\n",
        "\n",
        "def display_null_accuracy(y_test):\n",
        "    value_counts = pd.value_counts(y_test)\n",
        "    null_accuracy = max(value_counts) / float(len(y_test))\n",
        "    print( 'null accuracy: %s' % '{:.2%}'.format(null_accuracy))\n",
        "    return null_accuracy\n",
        "\n",
        "def display_accuracy_score(y_test, y_pred_class):\n",
        "    score = accuracy_score(y_test, y_pred_class)\n",
        "    print ('accuracy score: %s' % '{:.2%}'.format(score))\n",
        "    return score\n",
        "\n",
        "def display_accuracy_difference(y_test, y_pred_class):\n",
        "    null_accuracy = display_null_accuracy(y_test)\n",
        "    accuracy_score = display_accuracy_score(y_test, y_pred_class)\n",
        "    difference = accuracy_score - null_accuracy\n",
        "    if difference > 0:\n",
        "        print('model is %s more accurate than null accuracy' % '{:.2%}'.format(difference))\n",
        "    elif difference < 0:\n",
        "        print( 'model is %s less accurate than null accuracy' % '{:.2%}'.format(abs(difference)))\n",
        "    elif difference == 0:\n",
        "        print( 'model is exactly as accurate as null accuracy')\n",
        "    return null_accuracy, accuracy_score\n",
        "\n",
        "def train_test_and_evaluate(pipeline, X_train, y_train, X_test, y_test):\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred_class = pipeline.predict(X_test)\n",
        "    display_accuracy_difference(y_test, y_pred_class)\n",
        "\n",
        "    print(\"F1 Score\")\n",
        "    print(f1_score(y_test, y_pred_class, average='macro'))\n",
        "      \n",
        "    return pipeline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZr3AvppA6e1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f92a3a01-6945-4337-a3cc-7a8e35bd6728"
      },
      "source": [
        "def spacy_tokenizer_lemmatizer(text):\n",
        "    \n",
        "    nlp = English()\n",
        "    tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "    tokens = tokenizer(text)\n",
        "    \n",
        "    lemma_list = []\n",
        "    for token in tokens:\n",
        "        if (token.is_stop and token.is_punct) is False :\n",
        "            lemma_list.append(token.lemma_)\n",
        "    \n",
        "    return(lemma_list)\n",
        "\n",
        "print(tweets.tweet[0])\n",
        "spacy_tokenizer_lemmatizer(tweets.tweet[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@USER She should ask a few native Americans what their take on this is.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@USER', 'ask', 'native', 'Americans']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFitjJ_eLcZ",
        "colab_type": "text"
      },
      "source": [
        "## rascunhos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2jJxhY0l-wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "94e884f8-b9f1-4337-dd80-3931a75b5e7a"
      },
      "source": [
        "for sent in tweets['tokens_sem_stopwords'][0]:\n",
        "  for palavra in sent:\n",
        "    print(palavra)\n",
        "    if len(wn.synsets(palavra))>0:\n",
        "      print(wn.synsets(palavra)[0].hypernyms()[0].name())\n",
        "    \n",
        "\n",
        "def busca_hiperonimos(lista_sentencas_tokenizadas):\n",
        "  dicionario_sinonimos = dict()\n",
        "  dicionario_antonimos = dict()\n",
        "\n",
        "  for sent in lista_sentencas_tokenizadas:\n",
        "    for palavra in sent:\n",
        "      sinonimos = []\n",
        "      antonimos = []\n",
        "      for syn  in wn.synsets(palavra):\n",
        "        for l in syn.lemmas():\n",
        "          if l.name() not in sinonimos:\n",
        "            sinonimos.append(l.name()) \n",
        "          if l.antonyms():\n",
        "              antonimos.append(l.antonyms()[0].name())\n",
        "      if len(sinonimos) > 0:\n",
        "        dicionario_sinonimos[palavra] = sinonimos\n",
        "      if len(antonimos) > 0:\n",
        "        dicionario_antonimos[palavra] = antonimos\n",
        "    \n",
        "  return dicionario_sinonimos, dicionario_antonimos\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@user\n",
            "ask\n",
            "communicate.v.02\n",
            "native\n",
            "person.n.01\n",
            "americans\n",
            "inhabitant.n.01\n",
            "take\n",
            "income.n.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zocVWj8Sm0du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, language='english'):\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  def is_punct(self, token):\n",
        "    return all(\n",
        "    unicodedata.category(char).startswith('P') for char in token)\n",
        "  def is_stopword(self, token):\n",
        "    return token.lower() in self.stopwords\n",
        "\n",
        "  def normalize(self, document):\n",
        "\n",
        "    return [\n",
        "    self.lemmatize(token, tag).lower()\n",
        "    for paragraph in document\n",
        "    for sentence in paragraph\n",
        "    for (token, tag) in sentence\n",
        "    if not self.is_punct(token) and not self.is_stopword(token)\n",
        "    ]\n",
        "\n",
        "\n",
        "def lemmatize(self, token, pos_tag):\n",
        "  tag = {\n",
        "  'N': wn.NOUN,\n",
        "  'V': wn.VERB,\n",
        "  'R': wn.ADV,\n",
        "  'J': wn.ADJ\n",
        "  }.get(pos_tag[0], wn.NOUN)\n",
        "  return self.lemmatizer.lemmatize(token, tag)\n",
        "\n",
        "def fit(self, X, y=None):\n",
        "  return self\n",
        "def transform(self, documents):\n",
        "  for document in documents:\n",
        "    yield self.normalize(document)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLfeL62XdHqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, language='english'):\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "  def is_punct(self, token):\n",
        "    return all(unicodedata.category(char).startswith('P') for char in token)\n",
        " \n",
        "  def is_stopword(self, token):\n",
        "    return token.lower() in self.stopwords\n",
        " \n",
        "  def normalize(self, document):\n",
        "    return [\n",
        "      self.lemmatize(token, tag).lower()\n",
        "      for paragraph in document\n",
        "        for sentence in paragraph\n",
        "          for (token, tag) in sentence\n",
        "            if not self.is_punct(token) and not self.is_stopword(token)\n",
        "    ]\n",
        "  \n",
        "  def lemmatize(self, token, pos_tag):\n",
        "    tag = {\n",
        "    'N': wn.NOUN,\n",
        "    'V': wn.VERB,\n",
        "    'R': wn.ADV,\n",
        "    'J': wn.ADJ\n",
        "    }.get(pos_tag[0], wn.NOUN)\n",
        "    return self.lemmatizer.lemmatize(token, tag)\n",
        "  \n",
        "  \n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def transform(self, documents):\n",
        "    for document in documents:\n",
        "      yield self.normalize(document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWqFAvhSVnH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "def pipelinize(function, active=True):\n",
        "    def list_comprehend_a_function(list_or_series, active=True):\n",
        "        if active:\n",
        "            return [function(i) for i in list_or_series]\n",
        "        else: # if it's not active, just pass it right back\n",
        "            return list_or_series\n",
        "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}