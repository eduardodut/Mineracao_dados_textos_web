{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "projeto01_equipe01_Atividade02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardodut/Mineracao_dados_textos_web/blob/master/projeto01_equipe01_Atividade02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDkhltJeg5ag",
        "colab_type": "text"
      },
      "source": [
        "<b> EQUIPE: </b>\n",
        "  - Eduardo Façanha\n",
        "  - Giovanni Brígido\n",
        "  - Maurício Brito\n",
        "\n",
        "<b> ATIVIDADE 01 </b> - Pré-processamento dos textos (Prazo: 11/05/2020 - 30%)\n",
        "\n",
        "- Tokenização\n",
        "- Lematização\n",
        "- POS Tagging\n",
        "- Normalização (hashtags, menções, emojis e símbolos especiais)\n",
        "- Chunking\n",
        "- NER (entidades nomeadas)\n",
        "- Remoção stop-words\n",
        "\n",
        "<b> ATIVIDADE 02 </b> - Representação Semântica (Prazo: 30/06/2020 - 30%)\n",
        "\n",
        "- Uso de bases de conhecimento externas\n",
        "- Identificação de tópicos\n",
        "- Representação vetorial das palavras e textos\n",
        "\n",
        "<b> ATIVIDADE 03 </b> - Analise da Linguagem Ofensiva - Subtarefas A e B (Prazo: 30/07/2020 - 40%)\n",
        "\n",
        "- Resultado da subtarefa A para um conjunto de teste a ser fornecido\n",
        "- Resultado da subtarefa B para um conjunto de teste a ser fornecido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoO70tTEg4Bl",
        "colab_type": "text"
      },
      "source": [
        "## Atividade 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MRHH-jFQ9NhR"
      },
      "source": [
        "### Carregamento do arquivo de dados e transformação em DataFrame\n",
        "\n",
        "É realizado o download do arquivo e instanciado um DataFrame com os dados. A variável do DataFrame é chamada 'tweets'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPX5dwMB9Fwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "c85bc95e-52af-4e11-bfaa-3d407b43df2a"
      },
      "source": [
        "import pandas as pd\n",
        "#download o arquivo localizado no reposítório do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/olid-training-v1.0.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "tweets = pd.read_csv('/content/olid-training-v1.0.tsv', sep='\\t',encoding= 'utf-8')\n",
        "\n",
        "#conversão da coluna 'id' de inteiro para string\n",
        "tweets['id'] = tweets['id'].astype('str')\n",
        "\n",
        "#visualização dos primeiros registros\n",
        "\n",
        "tweets = tweets[['subtask_c','subtask_b','subtask_a','id','tweet']]\n",
        "tweets.head(20)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1915k  100 1915k    0     0  4789k      0 --:--:-- --:--:-- --:--:-- 4789k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subtask_c</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>UNT</td>\n",
              "      <td>OFF</td>\n",
              "      <td>86426</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IND</td>\n",
              "      <td>TIN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>90194</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>16820</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>UNT</td>\n",
              "      <td>OFF</td>\n",
              "      <td>62688</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>43605</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>OTH</td>\n",
              "      <td>TIN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>97670</td>\n",
              "      <td>@USER Liberals are all Kookoo !!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>UNT</td>\n",
              "      <td>OFF</td>\n",
              "      <td>77444</td>\n",
              "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>GRP</td>\n",
              "      <td>TIN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>52415</td>\n",
              "      <td>@USER was literally just talking about this lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>45157</td>\n",
              "      <td>@USER Buy more icecream!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>IND</td>\n",
              "      <td>TIN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>13384</td>\n",
              "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>82776</td>\n",
              "      <td>@USER @USER @USER It’s not my fault you suppor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>42992</td>\n",
              "      <td>@USER What’s the difference between #Kavanaugh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>IND</td>\n",
              "      <td>TIN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>28414</td>\n",
              "      <td>@USER you are a lying corrupt traitor!!! Nobod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>54920</td>\n",
              "      <td>@USER @USER @USER It should scare every Americ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>56392</td>\n",
              "      <td>@USER @USER @USER @USER @USER @USER @USER @USE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>86735</td>\n",
              "      <td>@USER you are also the king of taste</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>95686</td>\n",
              "      <td>#MAGA @USER  🎶 Sing like no one is listening  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>71446</td>\n",
              "      <td>5/5: @USER The time is right for this House to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NOT</td>\n",
              "      <td>23958</td>\n",
              "      <td>@USER Besides Jax’s mom and maybe Ope he is ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>OTH</td>\n",
              "      <td>TIN</td>\n",
              "      <td>OFF</td>\n",
              "      <td>28195</td>\n",
              "      <td>@USER @USER @USER gun control! That is all the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   subtask_c  ...                                              tweet\n",
              "0        NaN  ...  @USER She should ask a few native Americans wh...\n",
              "1        IND  ...  @USER @USER Go home you’re drunk!!! @USER #MAG...\n",
              "2        NaN  ...  Amazon is investigating Chinese employees who ...\n",
              "3        NaN  ...  @USER Someone should'veTaken\" this piece of sh...\n",
              "4        NaN  ...  @USER @USER Obama wanted liberals &amp; illega...\n",
              "5        OTH  ...                  @USER Liberals are all Kookoo !!!\n",
              "6        NaN  ...                   @USER @USER Oh noes! Tough shit.\n",
              "7        GRP  ...  @USER was literally just talking about this lo...\n",
              "8        NaN  ...                         @USER Buy more icecream!!!\n",
              "9        IND  ...  @USER Canada doesn’t need another CUCK! We alr...\n",
              "10       NaN  ...  @USER @USER @USER It’s not my fault you suppor...\n",
              "11       NaN  ...  @USER What’s the difference between #Kavanaugh...\n",
              "12       IND  ...  @USER you are a lying corrupt traitor!!! Nobod...\n",
              "13       NaN  ...  @USER @USER @USER It should scare every Americ...\n",
              "14       NaN  ...  @USER @USER @USER @USER @USER @USER @USER @USE...\n",
              "15       NaN  ...               @USER you are also the king of taste\n",
              "16       NaN  ...  #MAGA @USER  🎶 Sing like no one is listening  ...\n",
              "17       NaN  ...  5/5: @USER The time is right for this House to...\n",
              "18       NaN  ...  @USER Besides Jax’s mom and maybe Ope he is ha...\n",
              "19       OTH  ...  @USER @USER @USER gun control! That is all the...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dwWEaIDEcwF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82c60729-afa8-474b-c92b-a4b065b7660c"
      },
      "source": [
        "#verificação e remoção de duplicatas\n",
        "if tweets.duplicated(['tweet']).sum()>0:\n",
        "  tweets.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "print('TWEETS DUPLICADOS: ',tweets.duplicated(['tweet']).sum())"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TWEETS DUPLICADOS:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVAbRxcTLQZ0"
      },
      "source": [
        "### Tratamento inicial do texto\n",
        "\n",
        "Converte o texto de cada tweet, separadamente, em minúsculo e remove espaços e tabulações extras. O resultado é guardado no DataFrame tweets em uma nova coluna.\n",
        "\n",
        "Entrada: tweets['tweet']<br/>\n",
        "Saída: tweets['tweet_tratado']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tqS-1g3Kwgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a1a4cbd1-431f-408c-9811-d0cb535c75c4"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "def tratamento_texto(tweet):\n",
        "  \n",
        "  tweet = tweet.lower()\n",
        "  tweet = tweet.strip()\n",
        "  \n",
        "  #remove as menções a usuários de cada tweet\n",
        "  # tweet = re.sub(r'@user', '', tweet, flags=re.MULTILINE)\n",
        "  #remove as palavras url\n",
        "  tweet = re.sub(r'url', '', tweet, flags=re.MULTILINE)\n",
        "  #remove as quebras de linha\n",
        "  tweet = re.sub(r'\\n', '', tweet)\n",
        "  #substitui tabulações por um espaço em branco\n",
        "  tweet = re.sub(r'\\t', ' ', tweet)\n",
        "  #substitui um ou mais espaços em branco por um espaço\n",
        "  tweet= re.sub(r'\\s+', ' ', tweet, flags=re.I)\n",
        "  #&amp;\n",
        "  #remove aspas e apóstofres\n",
        "  tweet = re.sub('[\\'\"‘’“”…]', '', tweet)\n",
        "  #remove aspas e apóstofres\n",
        "  tweet = re.sub('^#$', '', tweet)\n",
        "  tweet = re.sub('@', '', tweet)\n",
        "  return tweet\n",
        "\n",
        "#cria uma nova coluna no dataframe 'tweets' com cada tweet tokenizado\n",
        "tweets['tweet_tratado'] = tweets['tweet'].apply(tratamento_texto)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user user go home youre drunk!!! user #maga #t...</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user someone shouldvetaken this piece of shit ...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user user obama wanted liberals &amp;amp; illegals...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       tweet_tratado  ... subtask_c\n",
              "0  user she should ask a few native americans wha...  ...       NaN\n",
              "1  user user go home youre drunk!!! user #maga #t...  ...       IND\n",
              "2  amazon is investigating chinese employees who ...  ...       NaN\n",
              "3  user someone shouldvetaken this piece of shit ...  ...       NaN\n",
              "4  user user obama wanted liberals &amp; illegals...  ...       NaN\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9VVuTg18hDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3de065a4-44b1-4355-f435-d2c6d458c577"
      },
      "source": [
        "tweets.tweet_tratado[1]"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'user user go home youre drunk!!! user #maga #trump2020 👊🇺🇸👊 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dzoy4baWLizj"
      },
      "source": [
        "<b> Separação em sentenças </b>\n",
        "\n",
        "Separa cada tweet em sentenças.\n",
        "\n",
        "Entrada: tweets['tweet_tratado']<br/>\n",
        "Saída: tweets['tweet_em_sentencas']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qma4M_noK4zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "4d32300f-2cf8-45e2-b9f2-297c13689073"
      },
      "source": [
        "import nltk\n",
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "\n",
        "with redirect_stdout(open(os.devnull, \"w\")):\n",
        "  nltk.download(\"stopwords\") \n",
        "  nltk.download('punkt')\n",
        "\n",
        "def separa_sentencas(tweet):\n",
        "  \n",
        "  lista_sentencas = sent_tokenize(tweet)\n",
        "  # lista_setencas.str.strip()\n",
        "  nova_lista = []\n",
        "  for sent in lista_sentencas:\n",
        "    nova_lista.append(sent.strip())\n",
        "\n",
        "  return nova_lista #retorna lista de sentenças com a função .strip() aplicada\n",
        "tweets['tweet_em_sentencas'] = tweets['tweet_tratado'].apply(separa_sentencas)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[user she should ask a few native americans wh...</td>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[user user go home youre drunk!!!, user #maga ...</td>\n",
              "      <td>user user go home youre drunk!!! user #maga #t...</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[user someone shouldvetaken this piece of shit...</td>\n",
              "      <td>user someone shouldvetaken this piece of shit ...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[user user obama wanted liberals &amp;amp; illegal...</td>\n",
              "      <td>user user obama wanted liberals &amp;amp; illegals...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  tweet_em_sentencas  ... subtask_c\n",
              "0  [user she should ask a few native americans wh...  ...       NaN\n",
              "1  [user user go home youre drunk!!!, user #maga ...  ...       IND\n",
              "2  [amazon is investigating chinese employees who...  ...       NaN\n",
              "3  [user someone shouldvetaken this piece of shit...  ...       NaN\n",
              "4  [user user obama wanted liberals &amp; illegal...  ...       NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZXVEs_1L3dj"
      },
      "source": [
        "### Tokenização \n",
        "\n",
        "Tokenização do tweet.\n",
        "\n",
        "Entrada: tweets['tweet_em_sentencas']<br/>\n",
        "Saída: tweets['tweet_tokenizado']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y7Kz7aOLCzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "04b99089-b5ff-456d-ef33-f9c8b2b118cd"
      },
      "source": [
        "import string as punctuation\n",
        "\n",
        "nltk.download('punkt')\n",
        "def tokeniza_sentenca(lista_sentencas):\n",
        "  # tokenizer = TweetTokenizer()\n",
        "  # #união das sentenças\n",
        "  # sentencas_unidas = \" \".join(w for w in lista_sentencas)\n",
        "  # #tokenização das sentenças unidas\n",
        "  # tokens = tokenizer.tokenize(sentencas_unidas)\n",
        "\n",
        "  tokenizer = TweetTokenizer()\n",
        "  tokens = []\n",
        "  \n",
        "  for sentenca in lista_sentencas:\n",
        "    lista_tokens = tokenizer.tokenize(sentenca)\n",
        "       \n",
        "    sentenca_sem_stopword = []\n",
        "\n",
        "    for token in lista_tokens:\n",
        "      if token not in string.punctuation:\n",
        "        sentenca_sem_stopword.append(token)\n",
        "\n",
        "    tokens.append(sentenca_sem_stopword)\n",
        "\n",
        "  # tweet = re.sub('[\\'\"‘’“”!…]', '', tweet)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# tokeniza_sentenca(tweets['tweet_em_sentencas'][2])\n",
        "\n",
        "tweets['tweet_tokenizado'] = tweets['tweet_em_sentencas'].apply(tokeniza_sentenca)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[user, she, should, ask, a, few, native, amer...</td>\n",
              "      <td>[user she should ask a few native americans wh...</td>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[user, user, go, home, youre, drunk], [user, ...</td>\n",
              "      <td>[user user go home youre drunk!!!, user #maga ...</td>\n",
              "      <td>user user go home youre drunk!!! user #maga #t...</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[user, someone, shouldvetaken, this, piece, o...</td>\n",
              "      <td>[user someone shouldvetaken this piece of shit...</td>\n",
              "      <td>user someone shouldvetaken this piece of shit ...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[user, user, obama, wanted, liberals, illegal...</td>\n",
              "      <td>[user user obama wanted liberals &amp;amp; illegal...</td>\n",
              "      <td>user user obama wanted liberals &amp;amp; illegals...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    tweet_tokenizado  ... subtask_c\n",
              "0  [[user, she, should, ask, a, few, native, amer...  ...       NaN\n",
              "1  [[user, user, go, home, youre, drunk], [user, ...  ...       IND\n",
              "2  [[amazon, is, investigating, chinese, employee...  ...       NaN\n",
              "3  [[user, someone, shouldvetaken, this, piece, o...  ...       NaN\n",
              "4  [[user, user, obama, wanted, liberals, illegal...  ...       NaN\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gc2ojEgN8CAz"
      },
      "source": [
        "### POS Tagger\n",
        "\n",
        "Realiza a part of speech tagging do texto de cada token\n",
        "\n",
        "Entrada: tweets['tweet_tokenizado']<br/>\n",
        "Saída: tweets['tweet_POS_tagged']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO70tP5r8NtQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1b761109-e3d8-4697-d9f0-d68a2819ccf2"
      },
      "source": [
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "\n",
        "with redirect_stdout(open(os.devnull, \"w\")):\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "# a função map aplica a funcao nltk.post_tag para cada lista contida da coluna tweet tokenizado\n",
        " \n",
        "def pos_taggeador(lista_tokens):\n",
        "  setenca_taggeada = []\n",
        "  for lista in lista_tokens:\n",
        "    setenca_taggeada.append(nltk.pos_tag(lista))\n",
        "\n",
        "  return setenca_taggeada\n",
        "\n",
        "                                                        #apply(nltk.pos) se a coluna for composta de lista de tokens\n",
        "tweets['tweet_POS_tagged'] = tweets['tweet_tokenizado'].apply(pos_taggeador)#\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_POS_tagged</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[(user, IN), (she, PRP), (should, MD), (ask, ...</td>\n",
              "      <td>[[user, she, should, ask, a, few, native, amer...</td>\n",
              "      <td>[user she should ask a few native americans wh...</td>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[user, user, go, home, youre, drunk], [user, ...</td>\n",
              "      <td>[user user go home youre drunk!!!, user #maga ...</td>\n",
              "      <td>user user go home youre drunk!!! user #maga #t...</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[(amazon, NN), (is, VBZ), (investigating, VBG...</td>\n",
              "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[user, someone, shouldvetaken, this, piece, o...</td>\n",
              "      <td>[user someone shouldvetaken this piece of shit...</td>\n",
              "      <td>user someone shouldvetaken this piece of shit ...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[user, user, obama, wanted, liberals, illegal...</td>\n",
              "      <td>[user user obama wanted liberals &amp;amp; illegal...</td>\n",
              "      <td>user user obama wanted liberals &amp;amp; illegals...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    tweet_POS_tagged  ... subtask_c\n",
              "0  [[(user, IN), (she, PRP), (should, MD), (ask, ...  ...       NaN\n",
              "1  [[(user, NN), (user, NN), (go, VBP), (home, NN...  ...       IND\n",
              "2  [[(amazon, NN), (is, VBZ), (investigating, VBG...  ...       NaN\n",
              "3  [[(user, NN), (someone, NN), (shouldvetaken, V...  ...       NaN\n",
              "4  [[(user, RB), (user, JJ), (obama, NN), (wanted...  ...       NaN\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGo5JNKZuK-N",
        "colab_type": "text"
      },
      "source": [
        "### Chunking\n",
        "\n",
        "Separação de cada sentença em chunks. \n",
        "\n",
        "Entrada: tweets['tweet_POS_tagged']<br/>\n",
        "Saída: tweets['tweet_chunked']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_0Uhw6uuLR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "\n",
        "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "pattern1 = 'NP: {<DT>?<JJ>*<NN.*>*}'\n",
        "pattern2 = 'NP: {<DT><NN.*><.*>*<NN.*>}'\n",
        "\n",
        "def chunker(lista_tweets_pos_tagged):\n",
        "\n",
        "  lista_saida = []\n",
        "\n",
        "  pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "  pattern1 = 'NP: {<DT>?<JJ>*<NN.*>*}'\n",
        "  pattern2 = 'NP: {<DT><NN.*><.*>*<NN.*>}'\n",
        "\n",
        "\n",
        "  for lista in lista_tweets_pos_tagged:\n",
        "    cp = nltk.RegexpParser(pattern1)\n",
        "    cs = cp.parse(lista)\n",
        "    iob_tagged = tree2conlltags(cs)\n",
        "    \n",
        "    lista_saida.append(iob_tagged)\n",
        "  return lista_saida\n",
        "\n",
        "\n",
        "tweets['tweet_chunked'] = tweets['tweet_POS_tagged'].apply(chunker)\n",
        "\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sSeX_Mt8DnbX"
      },
      "source": [
        "### NER \n",
        "\n",
        "Realiza a reconhecimento de entidades, NER.\n",
        "\n",
        "Entrada: tweets['tweet_POS_tagged']<br/>\n",
        "Saída: tweets['tweet_NER']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U9gYrTCrDnbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "1b32fa5a-bb2e-4d0e-8f3c-e9cf6057fd60"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "from nltk.chunk.regexp import ChunkString, ChunkRule, ChinkRule \n",
        "from nltk.tree import Tree \n",
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "\n",
        "with redirect_stdout(open(os.devnull, \"w\")):\n",
        "    nltk.download('maxent_ne_chunker')\n",
        "    nltk.download('words')\n",
        "\n",
        "def ner(lista_tokens_taggeados):\n",
        "  lista_tokens_ner = []\n",
        "  for lista in lista_tokens_taggeados:\n",
        "    lista_tokens_ner.append(nltk.ne_chunk(lista))\n",
        "\n",
        "  return lista_tokens_ner\n",
        "\n",
        "\n",
        "tweets['tweet_NER'] = tweets['tweet_POS_tagged'].apply(ner)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_NER</th>\n",
              "      <th>tweet_chunked</th>\n",
              "      <th>tweet_POS_tagged</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[(user, IN), (she, PRP), (should, MD), (ask, ...</td>\n",
              "      <td>[[(user, IN, O), (she, PRP, O), (should, MD, O...</td>\n",
              "      <td>[[(user, IN), (she, PRP), (should, MD), (ask, ...</td>\n",
              "      <td>[[user, she, should, ask, a, few, native, amer...</td>\n",
              "      <td>[user she should ask a few native americans wh...</td>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...</td>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[user, user, go, home, youre, drunk], [user, ...</td>\n",
              "      <td>[user user go home youre drunk!!!, user #maga ...</td>\n",
              "      <td>user user go home youre drunk!!! user #maga #t...</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[(amazon, NN), (is, VBZ), (investigating, VBG...</td>\n",
              "      <td>[[(amazon, NN, B-NP), (is, VBZ, O), (investiga...</td>\n",
              "      <td>[[(amazon, NN), (is, VBZ), (investigating, VBG...</td>\n",
              "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[(user, NN, B-NP), (someone, NN, I-NP), (shou...</td>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[user, someone, shouldvetaken, this, piece, o...</td>\n",
              "      <td>[user someone shouldvetaken this piece of shit...</td>\n",
              "      <td>user someone shouldvetaken this piece of shit ...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[(user, RB, O), (user, JJ, B-NP), (obama, NN,...</td>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[user, user, obama, wanted, liberals, illegal...</td>\n",
              "      <td>[user user obama wanted liberals &amp;amp; illegal...</td>\n",
              "      <td>user user obama wanted liberals &amp;amp; illegals...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           tweet_NER  ... subtask_c\n",
              "0  [[(user, IN), (she, PRP), (should, MD), (ask, ...  ...       NaN\n",
              "1  [[(user, NN), (user, NN), (go, VBP), (home, NN...  ...       IND\n",
              "2  [[(amazon, NN), (is, VBZ), (investigating, VBG...  ...       NaN\n",
              "3  [[(user, NN), (someone, NN), (shouldvetaken, V...  ...       NaN\n",
              "4  [[(user, RB), (user, JJ), (obama, NN), (wanted...  ...       NaN\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I36muOKnMD4D"
      },
      "source": [
        "### Remoção de stop words\n",
        "\n",
        "Remove da lista de tokens de cada tweet as stop words da língua inglesa e pontuações.\n",
        "\n",
        "Entradas:<br/>\n",
        "         * tweets['tweet_tokenizado']<br/>\n",
        "         * tweets['tweet_ner']<br/>\n",
        "         * tweets['tweet_chunked']<br/>\n",
        "\n",
        "Saída:<br/>\n",
        "         * tweets['tokens_sem_stopwords']<br/>\n",
        "         * tweets['NER_sem_stopwords'] <br/>\n",
        "         * tweets['chunks_sem_stopwords']<br/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XcCFILRLFFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "60f05230-3e6a-4b61-8a52-6e7307213ef7"
      },
      "source": [
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "# import string library function  \n",
        "from string import punctuation\n",
        "    \n",
        "\n",
        "\n",
        "def remove_stop_words(lista_token_sentenca):\n",
        "  '''Função de remoção de stop word que recebe lista de tokens e devolve\n",
        "  lista de tokens\n",
        "  '''\n",
        "  with redirect_stdout(open(os.devnull, \"w\")):\n",
        "    nltk.download(\"stopwords\") \n",
        "    nltk.download('punkt')\n",
        "  \n",
        "  stopwords = sw.words('english')\n",
        "  stop_words = set(stopwords + list(punctuation ))\n",
        "\n",
        "  lista_saida = []\n",
        "\n",
        "  for lista_tokens in lista_token_sentenca:\n",
        "    tokens = [w for w in lista_tokens if not w in stop_words]\n",
        "    lista_saida.append(tokens)\n",
        "\n",
        "\n",
        "  return lista_saida\n",
        "\n",
        "def remove_stop_words_tuplas(lista_tuplas_sentencas):\n",
        "  '''Função de remoção de stop word que recebe lista de tuplas de token e tag e devolve\n",
        "  lista de tuplas de token e tag\n",
        "  '''\n",
        "  with redirect_stdout(open(os.devnull, \"w\")):\n",
        "    nltk.download(\"stopwords\") \n",
        "    nltk.download('punkt')\n",
        "  \n",
        "  stopwords = sw.words('english')\n",
        "  stop_words = set(stopwords + list(punctuation ))\n",
        "  lista_saida = []\n",
        "  for lista_tuplas in lista_tuplas_sentencas:\n",
        "    tuplas = [w for w in lista_tuplas if not w[0] in stop_words]\n",
        "    lista_saida.append(tuplas)\n",
        "  return lista_saida\n",
        "\n",
        "tweets['tokens_sem_stopwords'] = tweets['tweet_tokenizado'].apply(remove_stop_words)\n",
        "tweets['NER_sem_stopwords'] = tweets['tweet_NER'].apply(remove_stop_words_tuplas)\n",
        "tweets['chunks_sem_stopwords'] = tweets['tweet_chunked'].apply(remove_stop_words_tuplas)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunks_sem_stopwords</th>\n",
              "      <th>NER_sem_stopwords</th>\n",
              "      <th>tokens_sem_stopwords</th>\n",
              "      <th>tweet_NER</th>\n",
              "      <th>tweet_chunked</th>\n",
              "      <th>tweet_POS_tagged</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[(user, IN, O), (ask, VB, O), (native, JJ, I-...</td>\n",
              "      <td>[[(user, IN), (ask, VB), (native, JJ), (americ...</td>\n",
              "      <td>[[user, ask, native, americans, take]]</td>\n",
              "      <td>[[(user, IN), (she, PRP), (should, MD), (ask, ...</td>\n",
              "      <td>[[(user, IN, O), (she, PRP, O), (should, MD, O...</td>\n",
              "      <td>[[(user, IN), (she, PRP), (should, MD), (ask, ...</td>\n",
              "      <td>[[user, she, should, ask, a, few, native, amer...</td>\n",
              "      <td>[user she should ask a few native americans wh...</td>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...</td>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[user, user, go, home, youre, drunk], [user, ...</td>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...</td>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[user, user, go, home, youre, drunk], [user, ...</td>\n",
              "      <td>[user user go home youre drunk!!!, user #maga ...</td>\n",
              "      <td>user user go home youre drunk!!! user #maga #t...</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
              "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
              "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
              "      <td>[[(amazon, NN), (is, VBZ), (investigating, VBG...</td>\n",
              "      <td>[[(amazon, NN, B-NP), (is, VBZ, O), (investiga...</td>\n",
              "      <td>[[(amazon, NN), (is, VBZ), (investigating, VBG...</td>\n",
              "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[(user, NN, B-NP), (someone, NN, I-NP), (shou...</td>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[user, someone, shouldvetaken, piece, shit, v...</td>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[(user, NN, B-NP), (someone, NN, I-NP), (shou...</td>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[user, someone, shouldvetaken, this, piece, o...</td>\n",
              "      <td>[user someone shouldvetaken this piece of shit...</td>\n",
              "      <td>user someone shouldvetaken this piece of shit ...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[(user, RB, O), (user, JJ, B-NP), (obama, NN,...</td>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[user, user, obama, wanted, liberals, illegal...</td>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[(user, RB, O), (user, JJ, B-NP), (obama, NN,...</td>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[user, user, obama, wanted, liberals, illegal...</td>\n",
              "      <td>[user user obama wanted liberals &amp;amp; illegal...</td>\n",
              "      <td>user user obama wanted liberals &amp;amp; illegals...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                chunks_sem_stopwords  ... subtask_c\n",
              "0  [[(user, IN, O), (ask, VB, O), (native, JJ, I-...  ...       NaN\n",
              "1  [[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...  ...       IND\n",
              "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...  ...       NaN\n",
              "3  [[(user, NN, B-NP), (someone, NN, I-NP), (shou...  ...       NaN\n",
              "4  [[(user, RB, O), (user, JJ, B-NP), (obama, NN,...  ...       NaN\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gpvnfgkKLFB5"
      },
      "source": [
        "<b> Fim da atividade 01 </b>\n",
        "\n",
        "Tem-se como principais entregas as colunas tweets['tokens_sem_stopwords'] e tweets['NER_sem_stopwords'] do dataset tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj0KRyOaLU5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d44f4bf0-c8aa-4a7b-8cc6-c0ea2fd4ef80"
      },
      "source": [
        "tweets[['NER_sem_stopwords','chunks_sem_stopwords']].head()"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NER_sem_stopwords</th>\n",
              "      <th>chunks_sem_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[(user, IN), (ask, VB), (native, JJ), (americ...</td>\n",
              "      <td>[[(user, IN, O), (ask, VB, O), (native, JJ, I-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
              "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[(user, NN, B-NP), (someone, NN, I-NP), (shou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[(user, RB, O), (user, JJ, B-NP), (obama, NN,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   NER_sem_stopwords                               chunks_sem_stopwords\n",
              "0  [[(user, IN), (ask, VB), (native, JJ), (americ...  [[(user, IN, O), (ask, VB, O), (native, JJ, I-...\n",
              "1  [[(user, NN), (user, NN), (go, VBP), (home, NN...  [[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...\n",
              "2  [[(amazon, NN), (investigating, VBG), (chinese...  [[(amazon, NN, B-NP), (investigating, VBG, O),...\n",
              "3  [[(user, NN), (someone, NN), (shouldvetaken, V...  [[(user, NN, B-NP), (someone, NN, I-NP), (shou...\n",
              "4  [[(user, RB), (user, JJ), (obama, NN), (wanted...  [[(user, RB, O), (user, JJ, B-NP), (obama, NN,..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InGHhHvvJml2",
        "colab_type": "text"
      },
      "source": [
        "base externa: wordnet lemmatizer?\n",
        "Não deixar palavras de meio de sentença em minúsculo pois podem ser entidades\n",
        "identificar \"typos\"\n",
        "Remover n-grams de alta frequência (não adicionam informação), e de  baixa frequência com erros (para prevenir overfit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOFyjuybzr2G",
        "colab_type": "text"
      },
      "source": [
        "## Atividade 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGvJSlsB4a87",
        "colab_type": "text"
      },
      "source": [
        "### Bag of wods: unigramas e bigramas\n",
        "\n",
        "entrada: tweets sem stopwords\n",
        "saida: uma coluna para os bigramas por tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR4vIL9s1IEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#passado como argumento para evitar outro preprocessamento pelo objeto de contagem\n",
        "def preprocessador_nulo(texto):\n",
        "  return texto\n",
        "#Para não realizar outra tokenização e remover as hashtags e emojis\n",
        "def tokenizador_nulo(texto):\n",
        "  return texto.split(\" \")\n",
        "\n",
        "def ngrams_por_tweet(lista_tokens):\n",
        "\n",
        "  texto = \"\"\n",
        "\n",
        "  for sentenca in lista_tokens:\n",
        "\n",
        "    texto = texto + \" \".join(x for x in sentenca)\n",
        "  \n",
        "  # print(texto)\n",
        "\n",
        "  cv = CountVectorizer(tokenizer = tokenizador_nulo, ngram_range= (1,2))\n",
        "  bow = cv.fit_transform([texto])\n",
        "\n",
        "  dicionario = dict(zip(cv.get_feature_names(),bow.toarray().sum(axis=0)))\n",
        "\n",
        "  return dicionario\n",
        "\n",
        "tweets['ngrams_por_tweet'] =  tweets['tokens_sem_stopwords'].apply(ngrams_por_tweet)"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07lKXHUtzytI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "a7897e4c-61c8-4e9c-dbe6-dc7d30c9c0e1"
      },
      "source": [
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ngrams_por_tweet</th>\n",
              "      <th>chunks_sem_stopwords</th>\n",
              "      <th>NER_sem_stopwords</th>\n",
              "      <th>tokens_sem_stopwords</th>\n",
              "      <th>tweet_NER</th>\n",
              "      <th>tweet_chunked</th>\n",
              "      <th>tweet_POS_tagged</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'americans': 1, 'americans take': 1, 'ask': 1...</td>\n",
              "      <td>[[(user, IN, O), (ask, VB, O), (native, JJ, I-...</td>\n",
              "      <td>[[(user, IN), (ask, VB), (native, JJ), (americ...</td>\n",
              "      <td>[[user, ask, native, americans, take]]</td>\n",
              "      <td>[[(user, IN), (she, PRP), (should, MD), (ask, ...</td>\n",
              "      <td>[[(user, IN, O), (she, PRP, O), (should, MD, O...</td>\n",
              "      <td>[[(user, IN), (she, PRP), (should, MD), (ask, ...</td>\n",
              "      <td>[[user, she, should, ask, a, few, native, amer...</td>\n",
              "      <td>[user she should ask a few native americans wh...</td>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'#maga': 1, '#maga #trump2020': 1, '#trump202...</td>\n",
              "      <td>[[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...</td>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[user, user, go, home, youre, drunk], [user, ...</td>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[(user, NN, B-NP), (user, NN, I-NP), (go, VBP...</td>\n",
              "      <td>[[(user, NN), (user, NN), (go, VBP), (home, NN...</td>\n",
              "      <td>[[user, user, go, home, youre, drunk], [user, ...</td>\n",
              "      <td>[user user go home youre drunk!!!, user #maga ...</td>\n",
              "      <td>user user go home youre drunk!!! user #maga #t...</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'#china': 1, '#china #tcot': 1, '#kag': 1, '#...</td>\n",
              "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
              "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
              "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
              "      <td>[[(amazon, NN), (is, VBZ), (investigating, VBG...</td>\n",
              "      <td>[[(amazon, NN, B-NP), (is, VBZ, O), (investiga...</td>\n",
              "      <td>[[(amazon, NN), (is, VBZ), (investigating, VBG...</td>\n",
              "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'piece': 1, 'piece shit': 1, 'shit': 1, 'shit...</td>\n",
              "      <td>[[(user, NN, B-NP), (someone, NN, I-NP), (shou...</td>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[user, someone, shouldvetaken, piece, shit, v...</td>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[(user, NN, B-NP), (someone, NN, I-NP), (shou...</td>\n",
              "      <td>[[(user, NN), (someone, NN), (shouldvetaken, V...</td>\n",
              "      <td>[[user, someone, shouldvetaken, this, piece, o...</td>\n",
              "      <td>[user someone shouldvetaken this piece of shit...</td>\n",
              "      <td>user someone shouldvetaken this piece of shit ...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'illegals': 1, 'illegals move': 1, 'liberals'...</td>\n",
              "      <td>[[(user, RB, O), (user, JJ, B-NP), (obama, NN,...</td>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[user, user, obama, wanted, liberals, illegal...</td>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[(user, RB, O), (user, JJ, B-NP), (obama, NN,...</td>\n",
              "      <td>[[(user, RB), (user, JJ), (obama, NN), (wanted...</td>\n",
              "      <td>[[user, user, obama, wanted, liberals, illegal...</td>\n",
              "      <td>[user user obama wanted liberals &amp;amp; illegal...</td>\n",
              "      <td>user user obama wanted liberals &amp;amp; illegals...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    ngrams_por_tweet  ... subtask_c\n",
              "0  {'americans': 1, 'americans take': 1, 'ask': 1...  ...       NaN\n",
              "1  {'#maga': 1, '#maga #trump2020': 1, '#trump202...  ...       IND\n",
              "2  {'#china': 1, '#china #tcot': 1, '#kag': 1, '#...  ...       NaN\n",
              "3  {'piece': 1, 'piece shit': 1, 'shit': 1, 'shit...  ...       NaN\n",
              "4  {'illegals': 1, 'illegals move': 1, 'liberals'...  ...       NaN\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVhQ7pDO05YJ",
        "colab_type": "text"
      },
      "source": [
        "### Consulta de bases externas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlrKMqpdufLF",
        "colab_type": "text"
      },
      "source": [
        "#### Busca de sinônimos e antônimos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuEdhnKVAxPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3fc07d8b-c3e8-4450-e10f-002a959e5f6c"
      },
      "source": [
        "#função para descobrir os sinônimos dos tokens da colunas \"tokens_sem_stopwords\"\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "\n",
        "def busca_sinonimos_antonimos(lista_sentencas_tokenizadas):\n",
        "  dicionario_sinonimos = dict()\n",
        "  dicionario_antonimos = dict()\n",
        "\n",
        "  for sent in lista_sentencas_tokenizadas:\n",
        "    for palavra in sent:\n",
        "      sinonimos = []\n",
        "      antonimos = []\n",
        "      for syn  in wn.synsets(palavra):\n",
        "        for l in syn.lemmas():\n",
        "          if l.name() not in sinonimos:\n",
        "            sinonimos.append(l.name()) \n",
        "          if l.antonyms():\n",
        "              antonimos.append(l.antonyms()[0].name())\n",
        "      if len(sinonimos) > 0:\n",
        "        dicionario_sinonimos[palavra] = sinonimos\n",
        "      if len(antonimos) > 0:\n",
        "        dicionario_antonimos[palavra] = antonimos\n",
        "    \n",
        "  return dicionario_sinonimos, dicionario_antonimos\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5OpNZbbjkOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "051fd364-a516-4fed-fecd-5baac4c331ca"
      },
      "source": [
        "tweets['sinonimos_antonimos'] = tweets['tokens_sem_stopwords'].apply(busca_sinonimos_antonimos)\n",
        "tweets['sinonimos_antonimos'] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        ({'ask': ['ask', 'inquire', 'enquire', 'requir...\n",
              "1        ({'go': ['go', 'spell', 'tour', 'turn', 'Adam'...\n",
              "2        ({'amazon': ['amazon', 'virago', 'Amazon', 'Am...\n",
              "3        ({'someone': ['person', 'individual', 'someone...\n",
              "4        ({'wanted': ['desire', 'want', 'need', 'requir...\n",
              "                               ...                        \n",
              "13235    ({'sometimes': ['sometimes'], 'get': ['get', '...\n",
              "13236    ({'shabby': ['moth-eaten', 'ratty', 'shabby', ...\n",
              "13237    ({'report': ['report', 'study', 'written_repor...\n",
              "13238    ({'pussy': ['cunt', 'puss', 'pussy', 'slit', '...\n",
              "13239    ({'vs': ['volt', 'V', 'vanadium', 'atomic_numb...\n",
              "Name: sinonimos_antonimos, Length: 13207, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJkSowVecxR",
        "colab_type": "text"
      },
      "source": [
        "#### Empath"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a2ZmTRjsv0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install empath\n",
        "from empath import Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "\n",
        "tweets['classific_empath'] = tweets[\"tweet_tratado\"].apply(lexicon.analyze)\n"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fAhJYSFxbhsL"
      },
      "source": [
        "### Recuperação dos word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb-_BEzY_q0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2295e604-c2af-42ab-ae71-66000eeba10b"
      },
      "source": [
        "# from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "def word_embedding(lista_sentencas_tokenizadas):\n",
        "  dicionario_saida = dict()\n",
        "  model = Word2Vec(lista_sentencas_tokenizadas, min_count=1,size= 50,workers=3, window =2, sg = 1)\n",
        "  for palavra in model.wv.vocab:\n",
        "    dicionario_saida[palavra] = model[palavra]\n",
        "\n",
        "  return dicionario_saida\n",
        "\n",
        "\n",
        "# word_embedding(tweets['tweet_tokenizado'].apply(lambda l: [item for sublist in l for item in sublist]))#.apply(\" \".join))\n",
        "# tweets['tweet_embeddings'] = tweets['tweet_tokenizado'].apply(lambda l: [item for sublist in l for item in sublist]).apply(word_embedding)\n",
        "tweets['tweet_embeddings'] = tweets['tweet_tokenizado'].apply(word_embedding)"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFwGqA3JIzto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "931e75cc-8f80-4a8e-f55d-fa541ff0a9b9"
      },
      "source": [
        "#exemplo dos vetores de cada tweet\n",
        "print(tweets['tweet_tokenizado'][2])\n",
        "print(tweets['tweet_embeddings'][2])"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['amazon', 'is', 'investigating', 'chinese', 'employees', 'who', 'are', 'selling', 'internal', 'data', 'to', 'third-party', 'sellers', 'looking', 'for', 'an', 'edge', 'in', 'the', 'competitive', 'marketplace'], ['#amazon', '#maga', '#kag', '#china', '#tcot']]\n",
            "{'amazon': array([ 0.00708107, -0.00160974, -0.00302817, -0.00557131,  0.00803064,\n",
            "       -0.00126878,  0.00032594, -0.0044409 , -0.003109  ,  0.00228831,\n",
            "       -0.00240538,  0.00445926,  0.00242291, -0.00491508,  0.00508993,\n",
            "       -0.00803601,  0.00654048, -0.00555123,  0.00293973,  0.0090288 ,\n",
            "       -0.00339887,  0.00253844,  0.00222103,  0.00992467,  0.00517678,\n",
            "        0.0073553 ,  0.00747471, -0.00740749, -0.00537205,  0.00851592,\n",
            "        0.00139181,  0.00637761,  0.00542973, -0.0092469 ,  0.00479682,\n",
            "        0.00642271, -0.00557764, -0.00363793,  0.00676623,  0.00828977,\n",
            "       -0.00604183,  0.00758583,  0.00558285,  0.00663622,  0.00286959,\n",
            "       -0.00754548, -0.00605596, -0.00688615, -0.00524256,  0.00821458],\n",
            "      dtype=float32), 'is': array([ 8.9303234e-05,  3.2977073e-03,  3.2853994e-03, -8.1938021e-03,\n",
            "        4.1227797e-03, -3.9165950e-04,  1.8597363e-03,  8.7183258e-03,\n",
            "        9.2859287e-03,  4.6326993e-03,  4.4480287e-03,  2.1137111e-03,\n",
            "        2.7983307e-03, -6.8373419e-03,  9.2161475e-03,  3.4381961e-03,\n",
            "        5.2750241e-03,  2.8960847e-03,  9.4811684e-03, -2.1214788e-03,\n",
            "        3.2035545e-03,  8.0834124e-03, -7.3925708e-03, -7.5222710e-03,\n",
            "        2.0484882e-03, -8.1787994e-03,  9.3614543e-03,  7.5391652e-03,\n",
            "        4.5430203e-04, -3.1412556e-03,  6.3887611e-04,  6.6126129e-03,\n",
            "       -2.9268980e-03, -7.9246629e-03, -6.6232657e-05, -9.2544332e-03,\n",
            "       -3.2131905e-03,  5.6551290e-03,  1.9070511e-03, -4.4348529e-03,\n",
            "       -2.4489542e-03,  9.9996466e-04, -6.9742077e-03, -2.4347850e-03,\n",
            "        8.1953636e-05, -5.3166873e-03, -4.5314235e-05,  3.0841136e-03,\n",
            "        3.9783381e-03, -8.8015795e-03], dtype=float32), 'investigating': array([ 1.0615704e-03,  1.8477275e-03, -3.3138536e-03,  8.4003266e-03,\n",
            "        8.1723928e-03,  5.2642697e-03, -8.7384842e-03, -8.2994718e-03,\n",
            "        9.4507253e-03,  9.8546259e-03, -3.6196532e-03,  4.6566641e-03,\n",
            "        1.4915577e-03, -3.2847263e-03, -5.7913570e-05,  5.6847604e-03,\n",
            "       -8.9542354e-03,  4.4744150e-03,  7.8526055e-03, -3.1603780e-03,\n",
            "        8.6103246e-04, -8.2769844e-04,  4.8081600e-03,  5.5822316e-03,\n",
            "        7.1922666e-04,  2.3825332e-03, -6.4073601e-03,  4.3969085e-03,\n",
            "        2.1562811e-03,  6.1206557e-03,  6.2645292e-03, -3.9081569e-03,\n",
            "       -9.0435809e-03,  9.1644116e-03,  9.3765855e-03, -5.1780000e-05,\n",
            "       -5.6135561e-03,  4.0140413e-04,  5.8930297e-03,  1.9278817e-03,\n",
            "        8.3558180e-04,  3.3178225e-03,  7.7295848e-03,  1.1937235e-03,\n",
            "       -6.9907977e-04, -4.2603780e-03, -2.6421098e-03, -9.8582385e-03,\n",
            "       -3.7499110e-03, -3.0810139e-03], dtype=float32), 'chinese': array([-3.6286409e-03,  7.7491682e-03, -2.5488515e-03,  8.1832893e-03,\n",
            "       -2.3971994e-03, -4.1388636e-04, -8.9586647e-03, -5.9478910e-04,\n",
            "       -6.0727154e-03,  6.3622510e-04,  9.7285844e-03, -1.5483154e-03,\n",
            "       -1.0270881e-04,  7.3702391e-03, -7.1671950e-03, -6.5488452e-03,\n",
            "       -3.0387179e-03, -6.7158411e-03,  3.0629018e-03,  5.2235252e-03,\n",
            "        3.9407909e-03, -5.4755257e-03, -6.0120574e-03, -1.9093265e-04,\n",
            "        6.0865103e-04,  3.0247327e-03, -1.6840156e-03, -4.9409334e-04,\n",
            "        8.5174078e-03,  4.4296575e-03,  1.8735250e-03, -3.7650820e-03,\n",
            "       -8.7600813e-04,  7.1459045e-03, -4.4226656e-03,  9.2248674e-03,\n",
            "       -6.3216714e-03,  1.5320999e-03,  4.3066018e-03,  9.2530986e-03,\n",
            "       -7.2750389e-03,  9.3108881e-03, -9.5407864e-05,  7.9568205e-03,\n",
            "        2.9500462e-03, -5.8225272e-03,  1.8991097e-03, -2.2109787e-03,\n",
            "       -2.2849918e-03, -9.6351383e-03], dtype=float32), 'employees': array([-0.00308568, -0.00980235, -0.00440924,  0.00881446,  0.00484042,\n",
            "        0.00391033, -0.00651116, -0.00935936,  0.00201999, -0.00707099,\n",
            "        0.00081677, -0.00886063, -0.00664638, -0.00068796, -0.0069812 ,\n",
            "        0.00426838, -0.00986031,  0.00282166, -0.00611894, -0.00775047,\n",
            "        0.0037997 , -0.00599495, -0.0005405 ,  0.00683622,  0.00215503,\n",
            "       -0.00635396,  0.00046624,  0.00579823,  0.0075349 , -0.00583952,\n",
            "        0.00418631, -0.00693475,  0.0034328 ,  0.00560072, -0.00969659,\n",
            "        0.00685189, -0.00237575,  0.0081982 ,  0.00237795, -0.00653972,\n",
            "       -0.00085718, -0.00895241,  0.00030476,  0.00058268, -0.00525727,\n",
            "        0.00676936,  0.00105061,  0.006076  , -0.007165  , -0.00150598],\n",
            "      dtype=float32), 'who': array([-0.00996166,  0.00585944, -0.0073204 , -0.00074763,  0.00230171,\n",
            "        0.00281833, -0.00117605, -0.00784282, -0.00518592,  0.00452197,\n",
            "        0.00455635,  0.0013659 ,  0.00491696,  0.00230982,  0.00908855,\n",
            "       -0.0017858 , -0.00013955,  0.00099777, -0.00418142, -0.0048881 ,\n",
            "        0.00666549, -0.00990993, -0.00770123,  0.00280465,  0.00966532,\n",
            "        0.00634095, -0.00313535, -0.0078421 ,  0.00394523,  0.00096412,\n",
            "        0.00716107,  0.00461328,  0.00629693, -0.00682344, -0.00865882,\n",
            "       -0.00806864, -0.00958591,  0.00783996,  0.0003228 ,  0.00421842,\n",
            "        0.0021628 , -0.00861207, -0.00735607, -0.00481204,  0.00896257,\n",
            "       -0.00572169, -0.00236751,  0.00946205, -0.00529408,  0.00732495],\n",
            "      dtype=float32), 'are': array([-5.8128643e-03, -6.5817065e-03, -9.1155684e-03, -1.6893352e-03,\n",
            "       -4.6037687e-03,  5.9361465e-04,  5.4072468e-03, -6.6225086e-03,\n",
            "       -2.1315047e-03,  3.2814997e-03, -3.8691836e-03, -7.7970685e-03,\n",
            "        7.6104021e-03, -3.4530328e-03,  4.9425019e-03, -6.9721080e-03,\n",
            "        2.8418405e-03,  2.1956868e-03, -8.5494854e-03,  6.8260268e-03,\n",
            "        6.9065564e-03, -6.0685468e-03, -3.0702273e-03,  6.1159097e-03,\n",
            "       -5.3579686e-03,  4.2648087e-03, -4.1514672e-03, -5.9378780e-03,\n",
            "       -1.4290254e-03, -2.8888923e-03,  9.5644697e-05, -4.0598200e-03,\n",
            "        2.7086304e-03,  5.4494920e-04,  6.3355239e-03, -8.3364295e-03,\n",
            "        8.4561622e-04, -7.1723945e-05,  2.1791940e-04,  4.2086756e-03,\n",
            "        8.6208194e-04, -5.1780390e-03, -1.7925275e-03,  2.4368258e-03,\n",
            "        3.4032441e-03,  3.4101345e-03,  6.1173327e-03, -3.8122640e-03,\n",
            "        5.1821079e-03,  5.9261457e-03], dtype=float32), 'selling': array([ 0.00723476, -0.00832112, -0.00285352, -0.00602914, -0.00970097,\n",
            "        0.00320608,  0.00737598, -0.00931111, -0.00212784,  0.00294343,\n",
            "       -0.00119446,  0.00061226,  0.0077544 , -0.00602525,  0.00475968,\n",
            "       -0.00844046, -0.00303517,  0.00258367, -0.00530316, -0.00245621,\n",
            "        0.00785612, -0.00775358, -0.00929626,  0.00800955,  0.00600612,\n",
            "        0.00205467, -0.00743028, -0.00279356,  0.00742173,  0.00183643,\n",
            "       -0.00535923, -0.00568486,  0.00830546,  0.00918449, -0.00990939,\n",
            "        0.00470994,  0.00620306,  0.00045995,  0.00596975, -0.00845783,\n",
            "        0.00753567, -0.00039431,  0.00359871,  0.00790893,  0.00529638,\n",
            "       -0.00725849,  0.00640923,  0.00041886, -0.00579812, -0.0050872 ],\n",
            "      dtype=float32), 'internal': array([-6.6904281e-03,  6.4016017e-03,  3.9350204e-03, -9.5204292e-03,\n",
            "       -2.0578040e-03, -2.0597496e-03, -2.5939327e-04, -7.1204342e-03,\n",
            "        7.0988056e-03, -7.4351579e-03,  7.6570059e-03, -5.9015476e-03,\n",
            "        5.0525251e-04, -2.9426303e-03,  3.7280621e-03,  4.8876912e-03,\n",
            "        3.6465188e-03, -8.1323013e-03,  7.2599836e-03, -4.5505874e-03,\n",
            "        9.8426994e-03, -2.8643009e-04,  7.7520814e-03,  4.6665375e-03,\n",
            "       -1.8867394e-03,  9.2658568e-03,  7.7817799e-03, -4.0412894e-03,\n",
            "        4.3944446e-03, -1.5003263e-03, -1.3956953e-03,  8.1149638e-03,\n",
            "       -4.8590344e-03,  8.2438271e-03,  7.9034455e-03,  7.5819977e-03,\n",
            "        4.2791353e-03,  8.8992796e-04, -1.3294984e-03, -7.5608427e-03,\n",
            "       -2.9027401e-03,  4.7804948e-04,  3.7015635e-03, -9.5119132e-03,\n",
            "       -9.1636200e-03, -2.1257037e-03, -6.5902048e-03, -3.7244188e-03,\n",
            "        4.8732809e-03,  7.9257261e-05], dtype=float32), 'data': array([ 0.00170164, -0.00947584, -0.0059052 , -0.00687972, -0.00795706,\n",
            "       -0.00772644,  0.00164283, -0.00299798,  0.00146178, -0.00590204,\n",
            "       -0.00656477, -0.00319035, -0.0020209 , -0.00096735, -0.00840148,\n",
            "        0.000476  , -0.00375213, -0.00163205,  0.00734432,  0.00436274,\n",
            "       -0.00249841,  0.00994866, -0.0039843 ,  0.00691326,  0.00263644,\n",
            "        0.00248456, -0.00159182, -0.0075561 , -0.00673761,  0.00464536,\n",
            "        0.00448546,  0.00117689,  0.00898043,  0.00166051, -0.00480343,\n",
            "        0.00513709,  0.00367255, -0.00492156,  0.00598711, -0.00751042,\n",
            "        0.00656891, -0.00882852,  0.00969507,  0.0076734 , -0.00029595,\n",
            "        0.00145328, -0.0013857 , -0.00227735, -0.00686664, -0.00218547],\n",
            "      dtype=float32), 'to': array([ 0.00436448, -0.00160835, -0.00527685,  0.00966209, -0.00037389,\n",
            "        0.00113274, -0.00331564,  0.00094745, -0.00495395,  0.00737468,\n",
            "       -0.00327262, -0.00876103, -0.00550431, -0.00993837, -0.00114686,\n",
            "       -0.00712851,  0.00206325, -0.0005838 , -0.00813739, -0.00762631,\n",
            "       -0.00464531, -0.00766915,  0.00444731, -0.00735518,  0.00437668,\n",
            "       -0.00315076,  0.00070097,  0.00382902, -0.00262556,  0.00970107,\n",
            "        0.00311836, -0.00617663, -0.00303803,  0.00649767, -0.00602885,\n",
            "       -0.00120882,  0.00544804, -0.00641059, -0.00746336,  0.00734494,\n",
            "       -0.00767254, -0.00404825, -0.00378898, -0.0088372 ,  0.00328185,\n",
            "        0.00442479,  0.00210991, -0.00635967, -0.00592371, -0.00182211],\n",
            "      dtype=float32), 'third-party': array([-3.7276885e-03,  1.2158722e-03, -4.0853363e-03,  8.5160369e-03,\n",
            "        3.4631293e-03,  3.6698550e-03, -6.5715266e-03,  4.0352615e-03,\n",
            "        9.8216720e-04, -1.8926660e-04, -4.9493462e-03,  8.6187851e-03,\n",
            "        5.8365441e-03,  6.6125770e-03, -4.7261566e-03, -9.3015805e-03,\n",
            "       -4.0086010e-03, -9.6827196e-03, -3.7951300e-05, -5.3506222e-04,\n",
            "       -2.8302965e-03,  7.1117380e-03,  8.2533220e-03,  4.6426994e-03,\n",
            "       -5.7538706e-03,  9.5968433e-03,  9.5373103e-03,  2.6948054e-03,\n",
            "       -6.7493678e-03,  8.5012009e-03, -1.4992752e-03, -7.4291471e-03,\n",
            "        9.9663129e-03,  3.8310518e-03,  8.9272726e-03, -1.4130399e-04,\n",
            "       -6.6934144e-03,  5.1724180e-03,  4.6555395e-03,  2.0158396e-03,\n",
            "       -9.6105253e-03, -6.2515326e-03,  7.4490113e-04, -2.1136103e-03,\n",
            "        2.7996334e-03, -2.4999315e-03, -2.7592108e-03, -2.6038657e-03,\n",
            "       -1.8856942e-03,  3.0282777e-04], dtype=float32), 'sellers': array([ 6.7468598e-03, -5.0387653e-03,  1.9826388e-03, -6.9849920e-03,\n",
            "        5.1907322e-04,  1.0957512e-04, -1.4733090e-03, -7.0287683e-03,\n",
            "        2.0085580e-03,  1.7739771e-03, -6.4744907e-03,  1.1006668e-03,\n",
            "       -2.7803397e-03,  4.9540587e-03, -6.6384934e-03,  8.0645818e-04,\n",
            "        3.8014529e-03,  8.4855044e-03,  8.4440773e-03, -4.3195034e-03,\n",
            "       -4.1120322e-03, -2.3797180e-03,  4.4380482e-03, -4.2766100e-03,\n",
            "        6.6299303e-03, -2.5604074e-03, -7.8396136e-03,  7.6611619e-03,\n",
            "        3.1026620e-03,  6.4579486e-03, -1.2670301e-03,  9.8524950e-03,\n",
            "        2.8515528e-03, -7.1524233e-03, -2.9369770e-03,  2.8125604e-03,\n",
            "        8.3197514e-03, -9.4269579e-03, -3.1583225e-03, -5.9856717e-03,\n",
            "        3.9724138e-04, -7.2108662e-05,  2.3393545e-03,  7.3295750e-04,\n",
            "       -1.6390547e-03,  9.2804777e-03,  4.7399886e-03, -1.4872684e-03,\n",
            "        6.5772962e-03,  2.2417931e-03], dtype=float32), 'looking': array([-2.3804433e-03, -1.3246441e-03, -8.1859557e-03, -8.7965336e-03,\n",
            "        4.1703437e-03,  5.4540038e-03, -4.9079111e-04, -3.9727967e-03,\n",
            "       -7.6263202e-03,  7.1011200e-03,  2.6753475e-04,  2.2566842e-03,\n",
            "        8.7838098e-03,  4.6227022e-05, -9.7004268e-03,  4.2488251e-04,\n",
            "        4.3351627e-03,  4.2523546e-03,  8.2441699e-03,  9.1865249e-03,\n",
            "       -7.6737166e-03,  5.2270130e-03, -6.7627435e-03,  9.4396053e-03,\n",
            "        1.5812434e-03,  4.8101461e-03, -7.5523616e-03,  5.9148925e-03,\n",
            "        1.9810533e-03, -2.1022731e-03, -2.3662027e-03,  6.7974199e-03,\n",
            "       -3.3039181e-03,  2.5293629e-03, -2.3766954e-03,  3.1670139e-03,\n",
            "       -9.3176598e-03,  6.7377752e-03,  6.6026337e-03, -9.5781107e-03,\n",
            "       -3.5997946e-03,  1.0890930e-03,  3.7471859e-03,  1.3232609e-03,\n",
            "       -8.0659436e-03,  9.5568672e-03,  5.0457299e-04,  4.8463778e-03,\n",
            "        5.4703210e-03,  1.0284309e-03], dtype=float32), 'for': array([-0.00502646,  0.00616337,  0.00855201, -0.00320062,  0.00885977,\n",
            "       -0.00943646,  0.00398649,  0.00287443,  0.00111164, -0.00487354,\n",
            "       -0.00726955, -0.00331193, -0.00524112, -0.00212893,  0.00433566,\n",
            "       -0.00958671, -0.00543906,  0.00269987,  0.00939057, -0.00195596,\n",
            "       -0.00404714, -0.00146523, -0.00848719, -0.00560341,  0.0072566 ,\n",
            "        0.00181044, -0.00519094,  0.0046412 ,  0.00353072, -0.00983677,\n",
            "        0.00258651,  0.00038711,  0.0038169 , -0.00961611, -0.00594277,\n",
            "        0.0050388 , -0.00655604,  0.00672587,  0.00698281, -0.00014409,\n",
            "       -0.00295544, -0.00996579,  0.00833798, -0.00662317, -0.00259824,\n",
            "        0.00377102, -0.00113626,  0.00424766,  0.00239395, -0.00799427],\n",
            "      dtype=float32), 'an': array([-0.00202314, -0.00787076,  0.00354689,  0.00251525,  0.00845067,\n",
            "       -0.00066835,  0.00178016, -0.00296041,  0.00601244, -0.00921927,\n",
            "        0.00063608, -0.00436413, -0.00130025, -0.00595584,  0.00125717,\n",
            "       -0.00764568,  0.00379722,  0.00743011, -0.00462196,  0.00814276,\n",
            "       -0.00971648,  0.0052187 , -0.00909969, -0.00571917, -0.00700273,\n",
            "       -0.00025043, -0.00696224, -0.00812542, -0.00239203,  0.00829122,\n",
            "        0.0066605 ,  0.00090033, -0.00530727, -0.00320675,  0.00272893,\n",
            "        0.00402417, -0.00849137,  0.00725993, -0.00386034,  0.00768654,\n",
            "        0.00512634, -0.00637035, -0.00427269,  0.00380692, -0.00194389,\n",
            "        0.00838688,  0.00966466,  0.00933897, -0.00524077, -0.00464568],\n",
            "      dtype=float32), 'edge': array([ 3.3152956e-03, -9.8140752e-03,  6.7244396e-03,  7.8611057e-03,\n",
            "       -5.6680441e-03,  2.8512713e-03,  2.8274525e-03,  9.6629849e-03,\n",
            "        8.6877560e-03, -3.0525783e-03, -7.9384297e-03,  1.5763607e-03,\n",
            "       -5.5926656e-03,  6.1345571e-03,  2.0351608e-03,  8.0925477e-04,\n",
            "       -6.4385668e-03, -8.7185455e-03, -8.7688686e-03, -2.7128775e-03,\n",
            "       -5.7630646e-03, -2.2442888e-03, -9.8999059e-03,  4.8903958e-03,\n",
            "       -2.0471222e-03,  3.3995593e-03,  4.8963409e-03,  8.1102671e-03,\n",
            "       -1.3818008e-04, -9.9227810e-03, -4.5704608e-05, -3.0432981e-03,\n",
            "       -4.2363107e-03,  7.3797048e-05, -7.5048939e-03, -6.8189385e-03,\n",
            "       -2.2514025e-03, -4.0157107e-03,  4.5438628e-03, -1.1480175e-05,\n",
            "        4.9275095e-03,  1.7125329e-03,  3.5326495e-03,  9.0563474e-03,\n",
            "        7.0035495e-03, -5.7313545e-03,  4.8901234e-03, -1.9448088e-03,\n",
            "       -7.1221041e-03, -4.3041832e-03], dtype=float32), 'in': array([ 0.00100213,  0.00885073, -0.00232273, -0.00660318, -0.00519666,\n",
            "       -0.00522826,  0.00340045, -0.00915098, -0.00469759,  0.0014671 ,\n",
            "        0.00893681, -0.00087566, -0.00419929, -0.00233554, -0.00991172,\n",
            "        0.0036128 ,  0.00186385, -0.00121895, -0.00473252, -0.0035497 ,\n",
            "       -0.00408245,  0.00636309, -0.00645493,  0.00250271,  0.00420304,\n",
            "        0.0012317 ,  0.0054502 , -0.00675316, -0.00938844, -0.00395708,\n",
            "        0.0099734 , -0.00216646,  0.00175231, -0.00723857,  0.0020881 ,\n",
            "       -0.00885682,  0.00511304,  0.00340566, -0.00861644, -0.00327451,\n",
            "        0.0038141 ,  0.00508861,  0.0088076 ,  0.00893038, -0.00951783,\n",
            "        0.00753222, -0.00472261, -0.00244772,  0.00021926,  0.0049369 ],\n",
            "      dtype=float32), 'the': array([-1.5059458e-03, -5.4453169e-03, -9.4029000e-03,  3.1750954e-03,\n",
            "       -3.1068381e-03,  6.5774927e-03, -6.5477854e-03,  8.4181698e-03,\n",
            "       -9.3395775e-04, -3.9829402e-03, -3.4290887e-03,  9.1565251e-03,\n",
            "       -1.1246257e-03, -9.5096594e-03,  6.4493077e-05, -9.8638097e-03,\n",
            "        3.6454489e-03, -3.8392402e-03, -2.0748635e-03,  1.0586440e-03,\n",
            "       -3.3592319e-03,  5.8539039e-03, -2.2218719e-03, -5.5983011e-03,\n",
            "       -7.0299585e-03, -6.7118942e-03, -7.7754497e-03, -4.8127710e-03,\n",
            "        9.7181052e-03,  3.2006560e-03, -6.2161437e-03,  4.6389936e-03,\n",
            "       -8.1004379e-03, -9.6593471e-03,  3.5249684e-03,  7.5654648e-03,\n",
            "        6.4974502e-03,  1.7028599e-04,  6.3939639e-03,  2.7642483e-04,\n",
            "       -6.3832738e-03, -7.7805282e-03, -8.8354573e-03,  9.4737699e-03,\n",
            "        7.1469322e-03,  1.9603390e-03,  5.0696614e-03, -8.2577718e-03,\n",
            "        8.6628972e-03,  4.4821519e-03], dtype=float32), 'competitive': array([-0.00723519,  0.00664685,  0.00996554, -0.0031003 , -0.0013174 ,\n",
            "       -0.00640318, -0.00210537, -0.00729163, -0.0030747 , -0.00743159,\n",
            "       -0.00839255,  0.00846822,  0.00391684, -0.00595591,  0.00729622,\n",
            "       -0.0063791 , -0.00864882,  0.00866383, -0.00465274, -0.00815425,\n",
            "        0.00859948,  0.00214703,  0.00402863, -0.00157316, -0.00761349,\n",
            "       -0.0080919 ,  0.00638737, -0.00347955,  0.00250901,  0.00334006,\n",
            "        0.00182834,  0.00402813,  0.00278233,  0.00120436, -0.0078867 ,\n",
            "       -0.00841703,  0.00035746,  0.00080693,  0.0049958 ,  0.00425749,\n",
            "        0.00462391, -0.00999099,  0.00982952, -0.00668278, -0.00117691,\n",
            "       -0.00287833, -0.00026668, -0.00409902, -0.00619701,  0.00536619],\n",
            "      dtype=float32), 'marketplace': array([-4.5930319e-03, -6.7783198e-03, -2.8259007e-03, -4.0175989e-03,\n",
            "        6.2029189e-03, -6.2909937e-03,  2.0212822e-03, -1.5722453e-03,\n",
            "        7.5698146e-03,  5.6947307e-03,  7.5171394e-03,  1.0839104e-05,\n",
            "       -9.2699593e-03,  2.7364254e-04,  9.6995365e-03,  2.1246912e-04,\n",
            "        4.5292152e-04,  3.2473111e-03,  2.0881721e-03, -1.8626518e-03,\n",
            "       -1.0136933e-03,  5.1786336e-03, -6.5819314e-03, -4.0648817e-03,\n",
            "        8.4380349e-03,  2.0337172e-03,  4.4892725e-04, -8.6223017e-03,\n",
            "        9.2834616e-03, -8.8609019e-03, -3.9459956e-03, -8.0885114e-03,\n",
            "        2.6225685e-03, -9.4850771e-03,  4.6425224e-03,  3.1965147e-03,\n",
            "        9.0363203e-03, -7.9726037e-03, -2.1493346e-03, -4.4315304e-03,\n",
            "       -8.9755254e-03,  9.6034538e-03, -6.2558376e-03, -1.8163535e-03,\n",
            "       -2.0466845e-03,  1.8434114e-03,  1.7578832e-03,  6.5824585e-03,\n",
            "       -5.5827582e-03, -6.4480812e-03], dtype=float32), '#amazon': array([-0.00807455, -0.00398044, -0.00638054,  0.00183282, -0.002173  ,\n",
            "       -0.00827475,  0.00709283,  0.00897556, -0.0006195 ,  0.00756885,\n",
            "       -0.00572101,  0.005848  , -0.00303053,  0.009093  , -0.00555966,\n",
            "        0.00998952, -0.00677239, -0.0062044 , -0.00070646,  0.00674357,\n",
            "       -0.00649981,  0.00774653, -0.00794401, -0.00855177,  0.00810874,\n",
            "        0.00614783,  0.00265572, -0.00358955, -0.0051109 , -0.00100642,\n",
            "        0.00377838,  0.00187775, -0.00033218, -0.00045005,  0.00456414,\n",
            "        0.00350628, -0.00062977,  0.00425425,  0.0025227 , -0.00327245,\n",
            "       -0.00222287,  0.0065637 ,  0.00648051, -0.00849693, -0.00488211,\n",
            "        0.00172988,  0.00910306, -0.00049859, -0.00522154, -0.00802987],\n",
            "      dtype=float32), '#maga': array([-0.00024741, -0.00932324,  0.00963106,  0.00374483, -0.00840152,\n",
            "        0.00351248, -0.00038382,  0.00524481, -0.00643484,  0.00562447,\n",
            "        0.00069439,  0.0085937 , -0.00744559,  0.00617892,  0.00594402,\n",
            "       -0.00314778,  0.00055732, -0.0067597 ,  0.00902681,  0.00561381,\n",
            "       -0.0007192 , -0.00064593,  0.0080939 , -0.00696249, -0.00184007,\n",
            "        0.009048  , -0.00404259,  0.0002731 , -0.00805077, -0.0092464 ,\n",
            "        0.00830508, -0.00315763,  0.00940294, -0.00025072, -0.00488634,\n",
            "        0.00913436, -0.00460071,  0.00991212,  0.00517945,  0.00769136,\n",
            "        0.00261315,  0.00078697, -0.00716693, -0.00341835,  0.00488413,\n",
            "        0.00694101, -0.00911109,  0.00592086,  0.00224937, -0.00797835],\n",
            "      dtype=float32), '#kag': array([ 0.00696612,  0.00429773,  0.00400037,  0.0079278 , -0.00925911,\n",
            "       -0.00146032,  0.00926435, -0.00995578,  0.00354949,  0.00038723,\n",
            "       -0.00924579, -0.00035364,  0.00729689, -0.00342198,  0.00242951,\n",
            "       -0.00932189,  0.00055114, -0.00064874,  0.00841727,  0.00993214,\n",
            "        0.00262649, -0.00227613,  0.00303401, -0.00991549,  0.00805334,\n",
            "        0.00161745, -0.00603514,  0.0017811 ,  0.00967721, -0.00682046,\n",
            "       -0.00739571, -0.00051873,  0.00366367,  0.00245145,  0.00770522,\n",
            "        0.00796299,  0.00952659, -0.00778902,  0.00329379, -0.00523403,\n",
            "       -0.00328819, -0.00755466, -0.00749515,  0.00954181,  0.00149565,\n",
            "       -0.00563055,  0.00441124,  0.0013072 , -0.00912138,  0.00320458],\n",
            "      dtype=float32), '#china': array([-8.44759750e-04,  4.92441747e-03, -6.36953441e-03,  2.51748017e-03,\n",
            "        7.95009919e-03,  8.35000165e-03,  4.93677007e-03, -2.11703801e-03,\n",
            "       -9.76285525e-03, -6.05991809e-03, -9.42409877e-03,  9.91384313e-03,\n",
            "       -4.93804505e-03,  4.43701539e-03, -6.99704187e-03,  1.45674020e-03,\n",
            "        1.16797655e-05, -3.52434441e-03, -7.24701304e-03,  6.81381207e-03,\n",
            "       -5.63188735e-03, -3.29169328e-04,  6.59230025e-03,  9.82846133e-03,\n",
            "        3.44524044e-03, -9.56534129e-03,  8.15484300e-03,  4.75864997e-03,\n",
            "        6.67714654e-03, -3.07362969e-03,  4.41772025e-03,  8.02355073e-03,\n",
            "        3.91534017e-03, -9.74156894e-03, -8.29959568e-03, -5.46648074e-03,\n",
            "       -1.11442897e-03,  6.61557820e-03,  9.02647339e-03, -2.43427581e-03,\n",
            "       -6.51097810e-03, -4.73180180e-03,  5.86661277e-03, -2.33698846e-03,\n",
            "       -3.26541788e-03, -7.21814064e-03,  7.28542730e-03,  2.97615584e-03,\n",
            "       -2.11002491e-03, -9.82229365e-04], dtype=float32), '#tcot': array([ 0.00425641,  0.00106743,  0.00241794, -0.00797824,  0.00654966,\n",
            "        0.00111003, -0.005996  ,  0.00764211,  0.00210866,  0.00789511,\n",
            "        0.0093363 , -0.00285367, -0.00913231,  0.00642938,  0.00306188,\n",
            "        0.00323845,  0.00210156,  0.00606055, -0.00152883, -0.0099468 ,\n",
            "       -0.00408066, -0.00719978,  0.00027441, -0.00017899, -0.00443474,\n",
            "        0.00983017,  0.00600494, -0.0099467 , -0.00448821, -0.00650762,\n",
            "        0.00116662,  0.00912261, -0.00276402, -0.00144637, -0.00902153,\n",
            "       -0.00860408, -0.0056868 , -0.00164676, -0.00740587, -0.00673731,\n",
            "       -0.00212196, -0.00998219, -0.00850962,  0.00841338,  0.00732293,\n",
            "       -0.0051713 , -0.00044824,  0.00798169,  0.00865417, -0.00458351],\n",
            "      dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9ZV2uv0dOgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf49a5cf-1939-4987-a525-158ecb3d1b40"
      },
      "source": [
        "tweets.tweet_tratado[1]"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'user user go home youre drunk!!! user #maga #trump2020 👊🇺🇸👊 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxHxS2i_gAae",
        "colab_type": "text"
      },
      "source": [
        "### Vetorização por tfidf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W__kJgUFUXwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "55f11873-3de9-4da0-d96c-47e118ef6ea3"
      },
      "source": [
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "tweets['tweet_tokenizado'].apply(lambda l: [item for sublist in l for item in sublist]).apply(\" \".join)"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        user she should ask a few native americans wha...\n",
              "1        user user go home youre drunk user #maga #trum...\n",
              "2        amazon is investigating chinese employees who ...\n",
              "3        user someone shouldvetaken this piece of shit ...\n",
              "4        user user obama wanted liberals illegals to mo...\n",
              "                               ...                        \n",
              "13235    user sometimes i get strong vibes from people ...\n",
              "13236    benidorm ✅ creamfields ✅ maga ✅ not too shabby...\n",
              "13237    user and why report this garbage we dont give ...\n",
              "13238                                           user pussy\n",
              "13239    #spanishrevenge vs #justice #humanrights and #...\n",
              "Name: tweet_tokenizado, Length: 13207, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-AxoOCYaNyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tokenizador = TweetTokenizer()\n",
        "#é utilizado o mesmo tokenizador para o processo de vetorização\n",
        "cv = TfidfVectorizer(tokenizer = tokenizador.tokenize, ngram_range= (1,1))\n",
        "\n",
        "vetorizacao_unigram = cv.fit_transform(tweets['tweet_tokenizado'].apply(lambda l: [item for sublist in l for item in sublist]).apply(\" \".join))\n"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyqXDJEBol0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe_vetorizacao_unigram = pd.DataFrame(vetorizacao_unigram.toarray(), columns= cv.get_feature_names())"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uGdcRcTZTDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "13c3f54d-236f-44df-b61f-802a72a9111a"
      },
      "source": [
        "dataframe_vetorizacao_unigram.head()"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>#100thmonkey</th>\n",
              "      <th>#102</th>\n",
              "      <th>#10millionsubscribers</th>\n",
              "      <th>#12</th>\n",
              "      <th>#180</th>\n",
              "      <th>#18n18</th>\n",
              "      <th>#1950sbornwomen</th>\n",
              "      <th>#1950swomen</th>\n",
              "      <th>#1a</th>\n",
              "      <th>#1ab</th>\n",
              "      <th>#1linewed</th>\n",
              "      <th>#1standlast</th>\n",
              "      <th>#1worldonlines</th>\n",
              "      <th>#2019loancharge</th>\n",
              "      <th>#2020</th>\n",
              "      <th>#2020maga</th>\n",
              "      <th>#2a</th>\n",
              "      <th>#2adefenders</th>\n",
              "      <th>#2ashallnotbeinfringed</th>\n",
              "      <th>#2birdsofafeather</th>\n",
              "      <th>#4-reds</th>\n",
              "      <th>#405</th>\n",
              "      <th>#51</th>\n",
              "      <th>#60minutes</th>\n",
              "      <th>#80s</th>\n",
              "      <th>#8217</th>\n",
              "      <th>#88</th>\n",
              "      <th>#a2</th>\n",
              "      <th>#a8</th>\n",
              "      <th>#aba</th>\n",
              "      <th>#abcnews</th>\n",
              "      <th>#abetterway</th>\n",
              "      <th>#ableg</th>\n",
              "      <th>#abortion</th>\n",
              "      <th>#ac360</th>\n",
              "      <th>#accountability</th>\n",
              "      <th>#activist</th>\n",
              "      <th>#adamandeve</th>\n",
              "      <th>#adelaide</th>\n",
              "      <th>#adiya</th>\n",
              "      <th>...</th>\n",
              "      <th>🚶</th>\n",
              "      <th>🛑</th>\n",
              "      <th>🛵</th>\n",
              "      <th>🛸</th>\n",
              "      <th>🤐</th>\n",
              "      <th>🤑</th>\n",
              "      <th>🤒</th>\n",
              "      <th>🤔</th>\n",
              "      <th>🤖</th>\n",
              "      <th>🤗</th>\n",
              "      <th>🤙</th>\n",
              "      <th>🤞</th>\n",
              "      <th>🤟</th>\n",
              "      <th>🤠</th>\n",
              "      <th>🤡</th>\n",
              "      <th>🤢</th>\n",
              "      <th>🤣</th>\n",
              "      <th>🤤</th>\n",
              "      <th>🤥</th>\n",
              "      <th>🤦</th>\n",
              "      <th>🤧</th>\n",
              "      <th>🤨</th>\n",
              "      <th>🤩</th>\n",
              "      <th>🤪</th>\n",
              "      <th>🤫</th>\n",
              "      <th>🤬</th>\n",
              "      <th>🤭</th>\n",
              "      <th>🤮</th>\n",
              "      <th>🤯</th>\n",
              "      <th>🤷</th>\n",
              "      <th>🥀</th>\n",
              "      <th>🥂</th>\n",
              "      <th>🦁</th>\n",
              "      <th>🦅</th>\n",
              "      <th>🦇</th>\n",
              "      <th>🦊</th>\n",
              "      <th>🧐</th>\n",
              "      <th>🧟</th>\n",
              "      <th>🧠</th>\n",
              "      <th>🧡</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 20846 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   #100thmonkey  #102  #10millionsubscribers  #12  ...    🧐    🧟    🧠    🧡\n",
              "0           0.0   0.0                    0.0  0.0  ...  0.0  0.0  0.0  0.0\n",
              "1           0.0   0.0                    0.0  0.0  ...  0.0  0.0  0.0  0.0\n",
              "2           0.0   0.0                    0.0  0.0  ...  0.0  0.0  0.0  0.0\n",
              "3           0.0   0.0                    0.0  0.0  ...  0.0  0.0  0.0  0.0\n",
              "4           0.0   0.0                    0.0  0.0  ...  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 20846 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgquWZw3S-C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6orKLEOSyeN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4352a048-e15e-41c2-8257-ddc73f28ed56"
      },
      "source": [
        ""
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        [user, she, should, ask, a, few, native, ameri...\n",
            "1        [user, user, go, home, youre, drunk, user, #ma...\n",
            "2        [amazon, is, investigating, chinese, employees...\n",
            "3        [user, someone, shouldvetaken, this, piece, of...\n",
            "4        [user, user, obama, wanted, liberals, illegals...\n",
            "                               ...                        \n",
            "13235    [user, sometimes, i, get, strong, vibes, from,...\n",
            "13236    [benidorm, ✅, creamfields, ✅, maga, ✅, not, to...\n",
            "13237    [user, and, why, report, this, garbage, we, do...\n",
            "13238                                        [user, pussy]\n",
            "13239    [#spanishrevenge, vs, #justice, #humanrights, ...\n",
            "Name: tweet_tokenizado, Length: 13207, dtype: object\n",
            "['user', 'liberals', 'are', 'all', 'kookoo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFvvYxs0tW07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tokenizador = TweetTokenizer()\n",
        "\n",
        "cv2 = TfidfVectorizer(tokenizer = tokenizador.tokenize, ngram_range= (2,2))\n",
        "\n",
        "vetorizacao_bigram = cv2.fit_transform(tweets['tweet_tokenizado'].apply(lambda l: [item for sublist in l for item in sublist]).apply(\" \".join))"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ7wjax5tfrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A visualização em dataframe não é possível por limitação de memória ram\n",
        "\n",
        "#dataframe_vetorizacao_bigram = pd.DataFrame(vetorizacao_bigram.toarray(), columns= cv2.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUwJS-sYtfis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4a9ceb24-51c3-4583-c56c-d5c6f4822eb5"
      },
      "source": [
        "vetorizacao_bigram"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<13207x130705 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 265934 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFitjJ_eLcZ",
        "colab_type": "text"
      },
      "source": [
        "## rascunhos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2jJxhY0l-wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "94e884f8-b9f1-4337-dd80-3931a75b5e7a"
      },
      "source": [
        "for sent in tweets['tokens_sem_stopwords'][0]:\n",
        "  for palavra in sent:\n",
        "    print(palavra)\n",
        "    if len(wn.synsets(palavra))>0:\n",
        "      print(wn.synsets(palavra)[0].hypernyms()[0].name())\n",
        "    \n",
        "\n",
        "def busca_hiperonimos(lista_sentencas_tokenizadas):\n",
        "  dicionario_sinonimos = dict()\n",
        "  dicionario_antonimos = dict()\n",
        "\n",
        "  for sent in lista_sentencas_tokenizadas:\n",
        "    for palavra in sent:\n",
        "      sinonimos = []\n",
        "      antonimos = []\n",
        "      for syn  in wn.synsets(palavra):\n",
        "        for l in syn.lemmas():\n",
        "          if l.name() not in sinonimos:\n",
        "            sinonimos.append(l.name()) \n",
        "          if l.antonyms():\n",
        "              antonimos.append(l.antonyms()[0].name())\n",
        "      if len(sinonimos) > 0:\n",
        "        dicionario_sinonimos[palavra] = sinonimos\n",
        "      if len(antonimos) > 0:\n",
        "        dicionario_antonimos[palavra] = antonimos\n",
        "    \n",
        "  return dicionario_sinonimos, dicionario_antonimos\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@user\n",
            "ask\n",
            "communicate.v.02\n",
            "native\n",
            "person.n.01\n",
            "americans\n",
            "inhabitant.n.01\n",
            "take\n",
            "income.n.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zocVWj8Sm0du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, language='english'):\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  def is_punct(self, token):\n",
        "    return all(\n",
        "    unicodedata.category(char).startswith('P') for char in token)\n",
        "  def is_stopword(self, token):\n",
        "    return token.lower() in self.stopwords\n",
        "\n",
        "  def normalize(self, document):\n",
        "\n",
        "    return [\n",
        "    self.lemmatize(token, tag).lower()\n",
        "    for paragraph in document\n",
        "    for sentence in paragraph\n",
        "    for (token, tag) in sentence\n",
        "    if not self.is_punct(token) and not self.is_stopword(token)\n",
        "    ]\n",
        "\n",
        "\n",
        "def lemmatize(self, token, pos_tag):\n",
        "  tag = {\n",
        "  'N': wn.NOUN,\n",
        "  'V': wn.VERB,\n",
        "  'R': wn.ADV,\n",
        "  'J': wn.ADJ\n",
        "  }.get(pos_tag[0], wn.NOUN)\n",
        "  return self.lemmatizer.lemmatize(token, tag)\n",
        "\n",
        "def fit(self, X, y=None):\n",
        "  return self\n",
        "def transform(self, documents):\n",
        "  for document in documents:\n",
        "    yield self.normalize(document)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}