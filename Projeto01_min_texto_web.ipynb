{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Projeto01_min_texto_web.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardodut/Mineracao_dados_textos_web/blob/master/Projeto01_min_texto_web.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDkhltJeg5ag",
        "colab_type": "text"
      },
      "source": [
        "<b> EQUIPE: </b>\n",
        "  - Eduardo Fa√ßanha\n",
        "  - Giovanni Br√≠gido\n",
        "  - Maur√≠cio Brito\n",
        "\n",
        "<b> ATIVIDADE 01 </b> - Pr√©-processamento dos textos (Prazo: 11/05/2020 - 30%)\n",
        "\n",
        "- Tokeniza√ß√£o\n",
        "- Lematiza√ß√£o\n",
        "- POS Tagging\n",
        "- Normaliza√ß√£o (hashtags, men√ß√µes, emojis e s√≠mbolos especiais)\n",
        "- NER (entidades nomeadas)\n",
        "- Remo√ß√£o stop-words\n",
        "\n",
        "<b> ATIVIDADE 02 </b> - Representa√ß√£o Sem√¢ntica (Prazo: 22/06/2020 - 30%)\n",
        "\n",
        "- Uso de bases de conhecimento externas\n",
        "- Identifica√ß√£o de t√≥picos\n",
        "- Representa√ß√£o vetorial das palavras e textos\n",
        "\n",
        "<b> ATIVIDADE 03 </b> - Analise da Linguagem Ofensiva - Subtarefas A e B (Prazo: 27/07/2020 - 40%)\n",
        "\n",
        "- Resultado da subtarefa A para um conjunto de teste a ser fornecido\n",
        "- Resultado da subtarefa B para um conjunto de teste a ser fornecido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRQvNpQ79NTF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYmShHnx9IUF",
        "colab_type": "text"
      },
      "source": [
        "<b> ATIVIDADE 01 (parcial)</b>\n",
        "  - Leitura do arquivo txt (tratar os tweets separadamente)\n",
        "  - Tratamento de hashtags, men√ß√µes, emojis e s√≠mbolos especiais, valores, datas\n",
        "  - Separa√ß√£o das senten√ßas e Tokeniza√ß√£o\n",
        "  - Remo√ß√£o stop-words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MRHH-jFQ9NhR"
      },
      "source": [
        "<b> Carregamento do arquivo de dados e transforma√ß√£o em DataFrame </b>\n",
        "\n",
        "√â realizado o download do arquivo e instanciado um DataFrame com os dados. A vari√°vel do DataFrame √© chamada 'tweets'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPX5dwMB9Fwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "6292248c-d8ea-4f51-c46a-b9f147d3f8f6"
      },
      "source": [
        "import pandas as pd\n",
        "#download o arquivo localizado no repos√≠t√≥rio do projeto\n",
        "!curl --remote-name \\\n",
        "    -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "    --location https://raw.githubusercontent.com/eduardodut/Mineracao_dados_textos_web/master/datasets/olid-training-v1.0.tsv\n",
        "\n",
        "#leitura para objeto dataframe\n",
        "tweets = pd.read_csv('/content/olid-training-v1.0.tsv', sep='\\t',encoding= 'utf-8')\n",
        "\n",
        "#convers√£o da coluna 'id' de inteiro para string\n",
        "tweets['id'] = tweets['id'].astype('str')\n",
        "\n",
        "#visualiza√ß√£o dos primeiros registros\n",
        "tweets.head()"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1915k  100 1915k    0     0  3297k      0 --:--:-- --:--:-- --:--:-- 3303k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16820</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62688</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43605</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet  ... subtask_b subtask_c\n",
              "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
              "1  90194  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  ...       TIN       IND\n",
              "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
              "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
              "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNEs_dMiA3vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop das colunas que come√ßam com 'sub'\n",
        "tweets.drop(tweets.columns[tweets.columns.str.startswith('sub')], axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dwWEaIDEcwF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81465ebe-5d7e-4c83-d2bd-b7c69f534a38"
      },
      "source": [
        "#verifica√ß√£o e remo√ß√£o de duplicatas\n",
        "if tweets.duplicated(['tweet']).sum()>0:\n",
        "  tweets.drop_duplicates(subset='tweet', keep='first', inplace=True)\n",
        "\n",
        "print('VALORES DUPLICADOS: ',tweets.duplicated(['tweet']).sum())"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VALORES DUPLICADOS:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naYbiDjILOK5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVAbRxcTLQZ0"
      },
      "source": [
        "<b> Tratamento inicial do texto </b>\n",
        "\n",
        "Converte o texto de cada tweet, separadamente, em min√∫sculo e remove espa√ßos e tabula√ß√µes extras. O resultado e guardado no DataFrame tweets em uma nova coluna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tqS-1g3Kwgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bfe38559-3084-4d38-e277-93a2a9391570"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "def tratamento_texto(tweet):\n",
        "  \n",
        "  tweet = tweet.lower()\n",
        "  tweet = tweet.strip()\n",
        "  \n",
        "  #remove as men√ß√µes a usu√°rios de cada tweet\n",
        "  tweet = re.sub(r'@user', '', tweet, flags=re.MULTILINE)\n",
        "  \n",
        "  #remove as quebras de linha\n",
        "  tweet = re.sub('\\n', '', tweet)\n",
        "  #substitui tabula√ß√µes por um espa√ßo em branco\n",
        "  tweet = re.sub('\\t', ' ', tweet)\n",
        "  #substitui um ou mais espa√ßos em branco por um espa√ßo\n",
        "  tweet= re.sub(r'\\s+', ' ', tweet, flags=re.I)\n",
        "  \n",
        "  #remove aspas e ap√≥stofres\n",
        "  tweet = re.sub('[‚Äò‚Äô‚Äú‚Äù‚Ä¶]', '', tweet)\n",
        "  return tweet\n",
        "\n",
        "#cria uma nova coluna no dataframe 'tweets' com cada tweet tokenizado\n",
        "tweets['tweet_tratado'] = tweets['tweet'].apply(tratamento_texto)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens_sem_stopwords</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ask, native, americans, take]</td>\n",
              "      <td>[she, should, ask, a, few, native, americans, ...</td>\n",
              "      <td>[she should ask a few native americans what th...</td>\n",
              "      <td>she should ask a few native americans what th...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[go, home, ‚Äô, drunk, #maga, #trump2020, üëä, üá∫, ...</td>\n",
              "      <td>[go, home, you, ‚Äô, re, drunk, !, !, !, #maga, ...</td>\n",
              "      <td>[go home you‚Äôre drunk!!!, #maga #trump2020 üëäüá∫üá∏...</td>\n",
              "      <td>go home youre drunk!!! #maga #trump2020 üëäüá∫üá∏üëä url</td>\n",
              "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[amazon, investigating, chinese, employees, se...</td>\n",
              "      <td>[amazon, is, investigating, chinese, employees...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[someone, should'vetaken, piece, shit, volcano...</td>\n",
              "      <td>[someone, should'vetaken, \", this, piece, of, ...</td>\n",
              "      <td>[someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[obama, wanted, liberals, illegals, move, red,...</td>\n",
              "      <td>[obama, wanted, liberals, &amp;, illegals, to, mov...</td>\n",
              "      <td>[obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                tokens_sem_stopwords  ...     id\n",
              "0                     [ask, native, americans, take]  ...  86426\n",
              "1  [go, home, ‚Äô, drunk, #maga, #trump2020, üëä, üá∫, ...  ...  90194\n",
              "2  [amazon, investigating, chinese, employees, se...  ...  16820\n",
              "3  [someone, should'vetaken, piece, shit, volcano...  ...  62688\n",
              "4  [obama, wanted, liberals, illegals, move, red,...  ...  43605\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dzoy4baWLizj"
      },
      "source": [
        "<b> Separa√ß√£o em senten√ßas </b>\n",
        "\n",
        "Separa cada tweet em senten√ßas e os coloca no DataFrame, em uma nova coluna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qma4M_noK4zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "84554185-a406-4322-a22c-55fa505463a7"
      },
      "source": [
        "def separa_sentencas(tweet):\n",
        "  \n",
        "  lista_sentencas = sent_tokenize(tweet)\n",
        "  nova_lista = []\n",
        "  for sent in lista_sentencas:\n",
        "    nova_lista.append(sent.strip())\n",
        "\n",
        "  return nova_lista\n",
        "tweets['tweet_em_sentencas']  = tweets['tweet_tratado'].apply(separa_sentencas)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens_sem_stopwords</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ask, native, americans, take]</td>\n",
              "      <td>[she, should, ask, a, few, native, americans, ...</td>\n",
              "      <td>[she should ask a few native americans what th...</td>\n",
              "      <td>she should ask a few native americans what th...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[go, home, ‚Äô, drunk, #maga, #trump2020, üëä, üá∫, ...</td>\n",
              "      <td>[go, home, you, ‚Äô, re, drunk, !, !, !, #maga, ...</td>\n",
              "      <td>[go home youre drunk!!!, #maga #trump2020 üëäüá∫üá∏üëä...</td>\n",
              "      <td>go home youre drunk!!! #maga #trump2020 üëäüá∫üá∏üëä url</td>\n",
              "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[amazon, investigating, chinese, employees, se...</td>\n",
              "      <td>[amazon, is, investigating, chinese, employees...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[someone, should'vetaken, piece, shit, volcano...</td>\n",
              "      <td>[someone, should'vetaken, \", this, piece, of, ...</td>\n",
              "      <td>[someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[obama, wanted, liberals, illegals, move, red,...</td>\n",
              "      <td>[obama, wanted, liberals, &amp;, illegals, to, mov...</td>\n",
              "      <td>[obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                tokens_sem_stopwords  ...     id\n",
              "0                     [ask, native, americans, take]  ...  86426\n",
              "1  [go, home, ‚Äô, drunk, #maga, #trump2020, üëä, üá∫, ...  ...  90194\n",
              "2  [amazon, investigating, chinese, employees, se...  ...  16820\n",
              "3  [someone, should'vetaken, piece, shit, volcano...  ...  62688\n",
              "4  [obama, wanted, liberals, illegals, move, red,...  ...  43605\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZXVEs_1L3dj"
      },
      "source": [
        "<b> Tokeniza√ß√£o </b>\n",
        "\n",
        "Re√∫ne as senten√ßas em uma string √∫nica e realiza a tokeniza√ß√£o do tweet. O resultado pode ser observado em uma nova coluna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y7Kz7aOLCzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "806c6aeb-1623-41e6-f105-e72529294aa7"
      },
      "source": [
        "def tokeniza_sentenca(lista_sentencas):\n",
        "\n",
        "  sentencas_unidas = \" \".join(w for w in lista_sentencas)\n",
        "  tokens = tokenizer.tokenize(sentencas_unidas)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "tweets['tweet_tokenizado'] = tweets['tweet_em_sentencas'].apply(tokeniza_sentenca)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens_sem_stopwords</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ask, native, americans, take]</td>\n",
              "      <td>[she, should, ask, a, few, native, americans, ...</td>\n",
              "      <td>[she should ask a few native americans what th...</td>\n",
              "      <td>she should ask a few native americans what th...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[go, home, ‚Äô, drunk, #maga, #trump2020, üëä, üá∫, ...</td>\n",
              "      <td>[go, home, youre, drunk, !, !, !, #maga, #trum...</td>\n",
              "      <td>[go home youre drunk!!!, #maga #trump2020 üëäüá∫üá∏üëä...</td>\n",
              "      <td>go home youre drunk!!! #maga #trump2020 üëäüá∫üá∏üëä url</td>\n",
              "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[amazon, investigating, chinese, employees, se...</td>\n",
              "      <td>[amazon, is, investigating, chinese, employees...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[someone, should'vetaken, piece, shit, volcano...</td>\n",
              "      <td>[someone, should'vetaken, \", this, piece, of, ...</td>\n",
              "      <td>[someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[obama, wanted, liberals, illegals, move, red,...</td>\n",
              "      <td>[obama, wanted, liberals, &amp;, illegals, to, mov...</td>\n",
              "      <td>[obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                tokens_sem_stopwords  ...     id\n",
              "0                     [ask, native, americans, take]  ...  86426\n",
              "1  [go, home, ‚Äô, drunk, #maga, #trump2020, üëä, üá∫, ...  ...  90194\n",
              "2  [amazon, investigating, chinese, employees, se...  ...  16820\n",
              "3  [someone, should'vetaken, piece, shit, volcano...  ...  62688\n",
              "4  [obama, wanted, liberals, illegals, move, red,...  ...  43605\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I36muOKnMD4D"
      },
      "source": [
        "<b> Remo√ß√£o de stop words </b>\n",
        "\n",
        "Remove da lista de tokens de cada tweet as stop words da l√≠ngua inglesa e pontua√ß√µes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XcCFILRLFFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "189b3bd8-886e-40d9-df31-a4e504931815"
      },
      "source": [
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "def remove_stop_words(lista_tokens):\n",
        "  \n",
        "  with redirect_stdout(open(os.devnull, \"w\")):\n",
        "    nltk.download(\"stopwords\") \n",
        "    nltk.download('punkt')\n",
        "  \n",
        "  stopwords = sw.words('english')\n",
        "  stop_words = set(stopwords + list(punctuation))\n",
        "\n",
        "  tokens = [w for w in lista_tokens if not w in stop_words]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "tweets['tokens_sem_stopwords'] = tweets['tweet_tokenizado'].apply(remove_stop_words)\n",
        "tweets[tweets.columns[::-1]].head()"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens_sem_stopwords</th>\n",
              "      <th>tweet_tokenizado</th>\n",
              "      <th>tweet_em_sentencas</th>\n",
              "      <th>tweet_tratado</th>\n",
              "      <th>tweet</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ask, native, americans, take]</td>\n",
              "      <td>[she, should, ask, a, few, native, americans, ...</td>\n",
              "      <td>[she should ask a few native americans what th...</td>\n",
              "      <td>she should ask a few native americans what th...</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>86426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[go, home, youre, drunk, #maga, #trump2020, üëä,...</td>\n",
              "      <td>[go, home, youre, drunk, !, !, !, #maga, #trum...</td>\n",
              "      <td>[go home youre drunk!!!, #maga #trump2020 üëäüá∫üá∏üëä...</td>\n",
              "      <td>go home youre drunk!!! #maga #trump2020 üëäüá∫üá∏üëä url</td>\n",
              "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
              "      <td>90194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[amazon, investigating, chinese, employees, se...</td>\n",
              "      <td>[amazon, is, investigating, chinese, employees...</td>\n",
              "      <td>[amazon is investigating chinese employees who...</td>\n",
              "      <td>amazon is investigating chinese employees who ...</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>16820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[someone, should'vetaken, piece, shit, volcano...</td>\n",
              "      <td>[someone, should'vetaken, \", this, piece, of, ...</td>\n",
              "      <td>[someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>someone should'vetaken\" this piece of shit to...</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>62688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[obama, wanted, liberals, illegals, move, red,...</td>\n",
              "      <td>[obama, wanted, liberals, &amp;, illegals, to, mov...</td>\n",
              "      <td>[obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>obama wanted liberals &amp;amp; illegals to move ...</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>43605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                tokens_sem_stopwords  ...     id\n",
              "0                     [ask, native, americans, take]  ...  86426\n",
              "1  [go, home, youre, drunk, #maga, #trump2020, üëä,...  ...  90194\n",
              "2  [amazon, investigating, chinese, employees, se...  ...  16820\n",
              "3  [someone, should'vetaken, piece, shit, volcano...  ...  62688\n",
              "4  [obama, wanted, liberals, illegals, move, red,...  ...  43605\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    }
  ]
}